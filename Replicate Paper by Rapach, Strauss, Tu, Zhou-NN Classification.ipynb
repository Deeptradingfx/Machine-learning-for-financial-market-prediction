{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate [Dynamic Return Dependencies Across Industries: A Machine Learning Approach](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3120110&download=yes) by David Rapach, Jack Strauss, Jun Tu and Guofu Zhou.\n",
    "\n",
    "1) Use Keras NN classification instead of linear regression\n",
    "\n",
    "2) Add additional variables, 3 and 12-month MA, interest rate change, yield curve, Mkt-RF. The hope is with  cross-validation and regularization we can do that without overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as datareader\n",
    "import time \n",
    "import datetime\n",
    "import copy\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Hide messy TensorFlow warnings\n",
    "warnings.filterwarnings(\"ignore\") #Hide messy numpy warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, lasso_path, lars_path, LassoLarsIC\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1764)\n",
    "\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.regularizers import l1\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "import ffn\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly as py\n",
    "# print (py.__version__) # requires version >= 1.9.0\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "random.seed(1764)\n",
    "np.random.seed(1764)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(697, 133)\n",
      "['Food', 'Beer', 'Smoke', 'Games', 'Books', 'Hshld', 'Clths', 'Hlth', 'Chems', 'Txtls', 'Cnstr', 'Steel', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Mines', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'BusEq', 'Paper', 'Trans', 'Whlsl', 'Rtail', 'Meals', 'Fin', 'Other', '3month', '10year', 'curve', 'month', 'Mkt-RF', 'Food.3m', 'Beer.3m', 'Smoke.3m', 'Games.3m', 'Books.3m', 'Hshld.3m', 'Clths.3m', 'Hlth.3m', 'Chems.3m', 'Txtls.3m', 'Cnstr.3m', 'Steel.3m', 'FabPr.3m', 'ElcEq.3m', 'Autos.3m', 'Carry.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Whlsl.3m', 'Rtail.3m', 'Meals.3m', 'Fin.3m', 'Other.3m', '3month.3m', '10year.3m', 'curve.3m', 'Mkt-RF.3m', 'Food.12m', 'Beer.12m', 'Smoke.12m', 'Games.12m', 'Books.12m', 'Hshld.12m', 'Clths.12m', 'Hlth.12m', 'Chems.12m', 'Txtls.12m', 'Cnstr.12m', 'Steel.12m', 'FabPr.12m', 'ElcEq.12m', 'Autos.12m', 'Carry.12m', 'Mines.12m', 'Coal.12m', 'Oil.12m', 'Util.12m', 'Telcm.12m', 'Servs.12m', 'BusEq.12m', 'Paper.12m', 'Trans.12m', 'Whlsl.12m', 'Rtail.12m', 'Meals.12m', 'Fin.12m', 'Other.12m', '3month.12m', '10year.12m', 'curve.12m', 'Mkt-RF.12m', 'Food.lead', 'Beer.lead', 'Smoke.lead', 'Games.lead', 'Books.lead', 'Hshld.lead', 'Clths.lead', 'Hlth.lead', 'Chems.lead', 'Txtls.lead', 'Cnstr.lead', 'Steel.lead', 'FabPr.lead', 'ElcEq.lead', 'Autos.lead', 'Carry.lead', 'Mines.lead', 'Coal.lead', 'Oil.lead', 'Util.lead', 'Telcm.lead', 'Servs.lead', 'BusEq.lead', 'Paper.lead', 'Trans.lead', 'Whlsl.lead', 'Rtail.lead', 'Meals.lead', 'Fin.lead', 'Other.lead']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3month</th>\n",
       "      <th>10year</th>\n",
       "      <th>curve</th>\n",
       "      <th>month</th>\n",
       "      <th>Mkt-RF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195912</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196001</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196002</th>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196003</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196004</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196005</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196006</th>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>-2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196008</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196009</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196010</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196011</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196012</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196101</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196102</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196103</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196104</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196105</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196106</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196108</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196201</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196202</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196203</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196204</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196205</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-8.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201507</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201508</th>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201509</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201510</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201511</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201512</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201606</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201607</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201608</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201609</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201610</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201611</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201612</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201701</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201702</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201703</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201705</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201706</th>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201707</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201708</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201709</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201711</th>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        3month  10year  curve     month  Mkt-RF\n",
       "yyyymm                                         \n",
       "195912    0.34    0.16   0.20  1.000000    2.45\n",
       "196001   -0.14    0.03   0.37  0.083333   -6.98\n",
       "196002   -0.39   -0.23   0.53  0.166667    1.17\n",
       "196003   -0.65   -0.24   0.94  0.250000   -1.63\n",
       "196004   -0.08    0.03   1.05  0.333333   -1.71\n",
       "196005    0.06    0.07   1.06  0.416667    3.12\n",
       "196006   -0.83   -0.20   1.69  0.500000    2.08\n",
       "196007   -0.16   -0.25   1.60  0.583333   -2.37\n",
       "196008    0.00   -0.10   1.50  0.666667    3.01\n",
       "196009    0.18    0.00   1.32  0.750000   -5.99\n",
       "196010   -0.18    0.09   1.59  0.833333   -0.71\n",
       "196011    0.07    0.04   1.56  0.916667    4.69\n",
       "196012   -0.12   -0.09   1.59  1.000000    4.71\n",
       "196101   -0.01    0.00   1.60  0.083333    6.20\n",
       "196102    0.18   -0.06   1.36  0.166667    3.57\n",
       "196103   -0.03   -0.04   1.35  0.250000    2.89\n",
       "196104   -0.10    0.04   1.49  0.333333    0.29\n",
       "196105    0.00   -0.07   1.42  0.416667    2.40\n",
       "196106    0.04    0.17   1.55  0.500000   -3.08\n",
       "196107   -0.09    0.04   1.68  0.583333    2.83\n",
       "196108    0.15    0.12   1.65  0.666667    2.57\n",
       "196109   -0.11   -0.06   1.70  0.750000   -2.15\n",
       "196110    0.02   -0.06   1.62  0.833333    2.57\n",
       "196111    0.18    0.02   1.46  0.916667    4.45\n",
       "196112    0.12    0.12   1.46  1.000000   -0.18\n",
       "196201    0.12    0.02   1.36  0.083333   -3.87\n",
       "196202    0.01   -0.04   1.31  0.166667    1.81\n",
       "196203   -0.01   -0.11   1.21  0.250000   -0.68\n",
       "196204    0.01   -0.09   1.11  0.333333   -6.59\n",
       "196205   -0.04    0.03   1.18  0.416667   -8.65\n",
       "...        ...     ...    ...       ...     ...\n",
       "201507    0.01   -0.04   2.29  0.583333    1.54\n",
       "201508    0.04   -0.15   2.10  0.666667   -6.04\n",
       "201509   -0.05    0.00   2.15  0.750000   -3.08\n",
       "201510    0.00   -0.10   2.05  0.833333    7.75\n",
       "201511    0.10    0.19   2.14  0.916667    0.56\n",
       "201512    0.11   -0.02   2.01  1.000000   -2.17\n",
       "201601    0.03   -0.15   1.83  0.083333   -5.77\n",
       "201602    0.05   -0.31   1.47  0.166667   -0.07\n",
       "201603   -0.02    0.11   1.60  0.250000    6.96\n",
       "201604   -0.06   -0.08   1.58  0.333333    0.92\n",
       "201605    0.04    0.00   1.54  0.416667    1.78\n",
       "201606    0.00   -0.17   1.37  0.500000   -0.05\n",
       "201607    0.03   -0.14   1.20  0.583333    3.95\n",
       "201608    0.00    0.06   1.26  0.666667    0.50\n",
       "201609   -0.01    0.07   1.34  0.750000    0.25\n",
       "201610    0.04    0.13   1.43  0.833333   -2.02\n",
       "201611    0.12    0.38   1.69  0.916667    4.86\n",
       "201612    0.06    0.35   1.98  1.000000    1.82\n",
       "201701    0.00   -0.06   1.92  0.083333    1.94\n",
       "201702    0.01   -0.01   1.90  0.166667    3.57\n",
       "201703    0.22    0.06   1.74  0.250000    0.17\n",
       "201704    0.06   -0.18   1.50  0.333333    1.09\n",
       "201705    0.09    0.00   1.41  0.416667    1.06\n",
       "201706    0.09   -0.11   1.21  0.500000    0.78\n",
       "201707    0.09    0.13   1.25  0.583333    1.87\n",
       "201708   -0.06   -0.11   1.20  0.666667    0.16\n",
       "201709    0.02   -0.01   1.17  0.750000    2.51\n",
       "201710    0.04    0.16   1.29  0.833333    2.25\n",
       "201711    0.16   -0.01   1.12  0.916667    3.12\n",
       "201712    0.09    0.05   1.08  1.000000    1.06\n",
       "\n",
       "[697 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(\"30_Industry_Portfolios.csv\")\n",
    "data = data.set_index('yyyymm')\n",
    "industries = list(data.columns)\n",
    "# map industry names to col nums\n",
    "ind_reverse_dict = dict([(industries[i], i) for i in range(len(industries))])\n",
    "\n",
    "rfdata = pd.read_csv(\"F-F_Research_Data_Factors.csv\")\n",
    "rfdata = rfdata.set_index('yyyymm')\n",
    "data['rf'] = rfdata['RF']\n",
    "\n",
    "# subtract risk-free rate\n",
    "# create a response variable led by 1 period to predict\n",
    "for ind in industries:\n",
    "    data[ind] = data[ind] - data['rf']\n",
    "\n",
    "    \n",
    "# add rates data from FRED\n",
    "start_date = datetime.datetime(1926, 9, 1)\n",
    "end_date = datetime.datetime(2017, 12, 1)\n",
    "TB3MS = datareader.DataReader(\"TB3MS\", \"fred\", start_date, end_date)\n",
    "TB3MS['yyyymm'] = TB3MS.index.strftime('%Y%m')\n",
    "TB3MS['yyyymm'] = [int(datestr) for datestr in TB3MS['yyyymm']]\n",
    "TB3MS=TB3MS.set_index(['yyyymm'])\n",
    "data['3month']=TB3MS['TB3MS']\n",
    "\n",
    "GS10 =  datareader.DataReader(\"GS10\", \"fred\", start_date, end_date)\n",
    "GS10['yyyymm'] = GS10.index.strftime('%Y%m')\n",
    "GS10['yyyymm'] = [int(datestr) for datestr in GS10['yyyymm']]\n",
    "GS10=GS10.set_index(['yyyymm'])\n",
    "data['10year']=GS10['GS10']\n",
    "\n",
    "data['curve'] = data['10year'] - data['3month']\n",
    "data['10year'] = data['10year'].diff() # first difference 10-year yield\n",
    "data['3month'] = data['3month'].diff() # first difference 3-month\n",
    "data['month'] = (data.index  % 100)/12.0 # for possible seasonality\n",
    "data['Mkt-RF'] = rfdata['Mkt-RF']\n",
    "\n",
    "for ind in industries + ['3month', '10year', 'curve', 'Mkt-RF',]:\n",
    "    data[ind+\".3m\"] = pd.rolling_mean(data[ind],3)\n",
    "    \n",
    "#for ind in industries + ['3month', '10year', 'curve', 'Mkt-RF',]:\n",
    "#    data[ind+\".6m\"] = pd.rolling_mean(data[ind],6)\n",
    "\n",
    "for ind in industries + ['3month', '10year', 'curve', 'Mkt-RF',]:\n",
    "    data[ind+\".12m\"] = pd.rolling_mean(data[ind],12)\n",
    "\n",
    "for ind in industries:\n",
    "    data[ind+\".lead\"] = data[ind].shift(-1)\n",
    "\n",
    "data = data.loc[data.index[data.index > 195911]]\n",
    "data = data.drop(columns=['rf'])    \n",
    "data = data.dropna(axis=0, how='any')\n",
    "\n",
    "nresponses = len(industries)\n",
    "npredictors = data.shape[1]-nresponses\n",
    "\n",
    "predictors = list(data.columns[:npredictors])\n",
    "predictor_reverse_dict = dict([(predictors[i], i) for i in range(len(predictors))])\n",
    "\n",
    "responses = list(data.columns[-nresponses:])\n",
    "response_reverse_dict = dict([(responses[i], i) for i in range(len(responses))])\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n",
    "data[['3month', '10year', 'curve', 'month', 'Mkt-RF',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Beer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Games</th>\n",
       "      <th>Books</th>\n",
       "      <th>Hshld</th>\n",
       "      <th>Clths</th>\n",
       "      <th>Hlth</th>\n",
       "      <th>Chems</th>\n",
       "      <th>Txtls</th>\n",
       "      <th>...</th>\n",
       "      <th>Telcm.lead</th>\n",
       "      <th>Servs.lead</th>\n",
       "      <th>BusEq.lead</th>\n",
       "      <th>Paper.lead</th>\n",
       "      <th>Trans.lead</th>\n",
       "      <th>Whlsl.lead</th>\n",
       "      <th>Rtail.lead</th>\n",
       "      <th>Meals.lead</th>\n",
       "      <th>Fin.lead</th>\n",
       "      <th>Other.lead</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195912</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>1.64</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>-4.31</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>-6.09</td>\n",
       "      <td>-10.08</td>\n",
       "      <td>-4.68</td>\n",
       "      <td>-3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196001</th>\n",
       "      <td>-4.49</td>\n",
       "      <td>-5.71</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-5.47</td>\n",
       "      <td>-7.84</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>-6.68</td>\n",
       "      <td>-10.03</td>\n",
       "      <td>-4.77</td>\n",
       "      <td>...</td>\n",
       "      <td>8.07</td>\n",
       "      <td>9.13</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196002</th>\n",
       "      <td>3.35</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>2.27</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.39</td>\n",
       "      <td>9.31</td>\n",
       "      <td>1.44</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-3.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196003</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>-6.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>8.86</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196004</th>\n",
       "      <td>1.17</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>1.35</td>\n",
       "      <td>6.46</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.49</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>11.90</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196005</th>\n",
       "      <td>8.20</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>2.44</td>\n",
       "      <td>7.28</td>\n",
       "      <td>11.67</td>\n",
       "      <td>7.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>13.50</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-8.07</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.72</td>\n",
       "      <td>6.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196006</th>\n",
       "      <td>5.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.38</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.84</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>-6.16</td>\n",
       "      <td>-2.99</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>-2.11</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>4.60</td>\n",
       "      <td>-4.72</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>-6.80</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>...</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.69</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.51</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196008</th>\n",
       "      <td>4.57</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.20</td>\n",
       "      <td>7.16</td>\n",
       "      <td>3.63</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.07</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-7.61</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-8.44</td>\n",
       "      <td>-8.57</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196009</th>\n",
       "      <td>-3.88</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-6.20</td>\n",
       "      <td>-9.18</td>\n",
       "      <td>-4.23</td>\n",
       "      <td>-8.87</td>\n",
       "      <td>-6.70</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.62</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-4.54</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196010</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.48</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.06</td>\n",
       "      <td>9.49</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.35</td>\n",
       "      <td>9.72</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4.40</td>\n",
       "      <td>7.71</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196011</th>\n",
       "      <td>9.46</td>\n",
       "      <td>6.57</td>\n",
       "      <td>5.44</td>\n",
       "      <td>13.91</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.04</td>\n",
       "      <td>...</td>\n",
       "      <td>12.29</td>\n",
       "      <td>8.18</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.57</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.56</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196012</th>\n",
       "      <td>4.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.54</td>\n",
       "      <td>7.77</td>\n",
       "      <td>7.41</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.28</td>\n",
       "      <td>6.06</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>7.70</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.08</td>\n",
       "      <td>4.56</td>\n",
       "      <td>8.35</td>\n",
       "      <td>7.93</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.08</td>\n",
       "      <td>7.12</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196101</th>\n",
       "      <td>4.70</td>\n",
       "      <td>5.23</td>\n",
       "      <td>8.77</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.47</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.94</td>\n",
       "      <td>5.86</td>\n",
       "      <td>6.46</td>\n",
       "      <td>11.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.54</td>\n",
       "      <td>6.83</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.31</td>\n",
       "      <td>4.82</td>\n",
       "      <td>8.23</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196102</th>\n",
       "      <td>4.21</td>\n",
       "      <td>8.16</td>\n",
       "      <td>5.41</td>\n",
       "      <td>22.33</td>\n",
       "      <td>2.15</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>2.13</td>\n",
       "      <td>6.81</td>\n",
       "      <td>...</td>\n",
       "      <td>7.23</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4.45</td>\n",
       "      <td>5.76</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.34</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196103</th>\n",
       "      <td>4.64</td>\n",
       "      <td>2.55</td>\n",
       "      <td>5.60</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.77</td>\n",
       "      <td>6.34</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196104</th>\n",
       "      <td>-1.39</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.39</td>\n",
       "      <td>4.74</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196105</th>\n",
       "      <td>4.20</td>\n",
       "      <td>5.38</td>\n",
       "      <td>3.39</td>\n",
       "      <td>-3.91</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-3.31</td>\n",
       "      <td>-4.46</td>\n",
       "      <td>-4.57</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>-2.88</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196106</th>\n",
       "      <td>-2.17</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-5.87</td>\n",
       "      <td>-3.85</td>\n",
       "      <td>3.43</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.35</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>2.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.95</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196108</th>\n",
       "      <td>4.92</td>\n",
       "      <td>3.20</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10.45</td>\n",
       "      <td>5.21</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.24</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.30</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>3.73</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>7.05</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>8.28</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.11</td>\n",
       "      <td>...</td>\n",
       "      <td>8.34</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.65</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>5.28</td>\n",
       "      <td>4.47</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.51</td>\n",
       "      <td>5.30</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.49</td>\n",
       "      <td>7.37</td>\n",
       "      <td>...</td>\n",
       "      <td>3.14</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-4.44</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>-3.69</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-6.12</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>-3.78</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.32</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>-6.91</td>\n",
       "      <td>-5.22</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-9.56</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196201</th>\n",
       "      <td>-6.67</td>\n",
       "      <td>-3.45</td>\n",
       "      <td>-4.28</td>\n",
       "      <td>-13.23</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>-5.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>3.59</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196202</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>2.01</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-4.07</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196203</th>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-6.67</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.93</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>-12.01</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-6.27</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>-9.09</td>\n",
       "      <td>-7.69</td>\n",
       "      <td>-2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196204</th>\n",
       "      <td>-4.59</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-12.99</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>-8.74</td>\n",
       "      <td>-7.03</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>-11.23</td>\n",
       "      <td>-6.23</td>\n",
       "      <td>-7.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>-10.97</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>-11.03</td>\n",
       "      <td>-5.17</td>\n",
       "      <td>-11.34</td>\n",
       "      <td>-9.09</td>\n",
       "      <td>-7.46</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>-11.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196205</th>\n",
       "      <td>-11.25</td>\n",
       "      <td>-9.05</td>\n",
       "      <td>-14.14</td>\n",
       "      <td>-11.39</td>\n",
       "      <td>-14.87</td>\n",
       "      <td>-10.19</td>\n",
       "      <td>-10.01</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>-7.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>-13.11</td>\n",
       "      <td>-12.59</td>\n",
       "      <td>-9.62</td>\n",
       "      <td>-7.81</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-10.43</td>\n",
       "      <td>-12.90</td>\n",
       "      <td>-11.01</td>\n",
       "      <td>-14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201507</th>\n",
       "      <td>4.03</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.59</td>\n",
       "      <td>6.09</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.42</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-6.50</td>\n",
       "      <td>-5.69</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-4.13</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>-6.48</td>\n",
       "      <td>-6.54</td>\n",
       "      <td>-5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201508</th>\n",
       "      <td>-4.37</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>-8.61</td>\n",
       "      <td>-6.94</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>-8.37</td>\n",
       "      <td>-7.15</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201509</th>\n",
       "      <td>-1.19</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-9.94</td>\n",
       "      <td>-5.32</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-7.28</td>\n",
       "      <td>-8.38</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>8.84</td>\n",
       "      <td>11.26</td>\n",
       "      <td>8.16</td>\n",
       "      <td>10.19</td>\n",
       "      <td>6.48</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.90</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201510</th>\n",
       "      <td>5.81</td>\n",
       "      <td>8.06</td>\n",
       "      <td>10.90</td>\n",
       "      <td>14.61</td>\n",
       "      <td>12.21</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.74</td>\n",
       "      <td>16.62</td>\n",
       "      <td>7.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201511</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.03</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-4.64</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>-5.02</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201512</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>1.86</td>\n",
       "      <td>-4.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-5.09</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>-5.27</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>-8.68</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-9.63</td>\n",
       "      <td>-3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>4.28</td>\n",
       "      <td>-8.15</td>\n",
       "      <td>-5.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-9.43</td>\n",
       "      <td>-11.10</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.89</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>0.95</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4.25</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.76</td>\n",
       "      <td>8.86</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.18</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.36</td>\n",
       "      <td>6.65</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>4.69</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.04</td>\n",
       "      <td>8.61</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.90</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>-5.46</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.19</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.42</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-4.96</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-2.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-5.30</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201606</th>\n",
       "      <td>4.75</td>\n",
       "      <td>5.31</td>\n",
       "      <td>6.87</td>\n",
       "      <td>-4.43</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>...</td>\n",
       "      <td>2.27</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.20</td>\n",
       "      <td>2.52</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.04</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201607</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-2.79</td>\n",
       "      <td>6.15</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.62</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.29</td>\n",
       "      <td>8.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.56</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.09</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>4.88</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201608</th>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4.33</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>2.86</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>-3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201609</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>4.62</td>\n",
       "      <td>-3.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-6.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>-4.87</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.24</td>\n",
       "      <td>-5.49</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201610</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>4.59</td>\n",
       "      <td>5.59</td>\n",
       "      <td>-10.28</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-7.45</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.39</td>\n",
       "      <td>4.21</td>\n",
       "      <td>12.75</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.99</td>\n",
       "      <td>8.47</td>\n",
       "      <td>12.84</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201611</th>\n",
       "      <td>-4.41</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-5.12</td>\n",
       "      <td>3.87</td>\n",
       "      <td>8.15</td>\n",
       "      <td>-4.18</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.55</td>\n",
       "      <td>1.58</td>\n",
       "      <td>...</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201612</th>\n",
       "      <td>4.43</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>...</td>\n",
       "      <td>3.36</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201701</th>\n",
       "      <td>0.95</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>5.61</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.23</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.35</td>\n",
       "      <td>4.58</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201702</th>\n",
       "      <td>1.71</td>\n",
       "      <td>6.13</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>4.83</td>\n",
       "      <td>3.81</td>\n",
       "      <td>7.03</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201703</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.03</td>\n",
       "      <td>5.79</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>3.21</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>0.76</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>2.74</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6.93</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201705</th>\n",
       "      <td>1.63</td>\n",
       "      <td>4.28</td>\n",
       "      <td>6.12</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201706</th>\n",
       "      <td>-2.65</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9.38</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1.93</td>\n",
       "      <td>4.39</td>\n",
       "      <td>...</td>\n",
       "      <td>5.25</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-2.42</td>\n",
       "      <td>-4.49</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201707</th>\n",
       "      <td>1.52</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-6.06</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-4.55</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201708</th>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.44</td>\n",
       "      <td>6.20</td>\n",
       "      <td>4.46</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201709</th>\n",
       "      <td>0.43</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.54</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.40</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.75</td>\n",
       "      <td>5.93</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4.28</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.22</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>0.71</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-3.09</td>\n",
       "      <td>3.47</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1.23</td>\n",
       "      <td>...</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.41</td>\n",
       "      <td>4.21</td>\n",
       "      <td>6.66</td>\n",
       "      <td>3.37</td>\n",
       "      <td>9.38</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201711</th>\n",
       "      <td>4.15</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.30</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.94</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.21</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.31</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.77</td>\n",
       "      <td>...</td>\n",
       "      <td>3.08</td>\n",
       "      <td>9.25</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4.84</td>\n",
       "      <td>11.37</td>\n",
       "      <td>3.12</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Food  Beer  Smoke  Games  Books  Hshld  Clths   Hlth  Chems  Txtls  \\\n",
       "yyyymm                                                                        \n",
       "195912   2.01  0.35  -3.02   1.64   7.29   0.67   1.87  -1.97   3.08   0.74   \n",
       "196001  -4.49 -5.71  -2.05   1.21  -5.47  -7.84  -8.53  -6.68 -10.03  -4.77   \n",
       "196002   3.35 -2.14   2.27   4.23   2.39   9.31   1.44  -0.02  -0.74   0.32   \n",
       "196003  -1.67 -2.94  -0.18  -0.65   2.18  -0.56  -2.59   1.26  -2.75  -6.79   \n",
       "196004   1.17 -2.16   1.35   6.46  -1.17  -1.27   0.21   1.49  -5.53  -1.10   \n",
       "196005   8.20 -0.52   2.44   7.28  11.67   7.74   1.74  13.50   3.40   2.10   \n",
       "196006   5.39  0.47   4.73   2.24   0.02   6.38  -1.59  -0.40   0.45   4.04   \n",
       "196007  -2.11 -0.79   4.60  -4.72   0.23  -0.60  -1.10  -3.99  -6.80  -3.14   \n",
       "196008   4.57  3.24   5.20   7.16   3.63   5.09   3.34   2.29   1.17  -0.84   \n",
       "196009  -3.88 -5.00  -2.09  -2.33  -6.20  -9.18  -4.23  -8.87  -6.70  -5.25   \n",
       "196010   1.02  0.54   3.87   0.11   2.38   6.48  -3.50  -3.71  -1.59  -3.06   \n",
       "196011   9.46  6.57   5.44  13.91  10.11   9.13   3.15   3.91   4.25   2.04   \n",
       "196012   4.51 -0.31   3.54   7.77   7.41   1.76   3.28   6.06   2.85   0.52   \n",
       "196101   4.70  5.23   8.77   0.56   9.47   4.36   5.94   5.86   6.46  11.21   \n",
       "196102   4.21  8.16   5.41  22.33   2.15   5.90   7.84   5.05   2.13   6.81   \n",
       "196103   4.64  2.55   5.60   7.18   4.77   6.34   3.08   3.60   0.92   5.92   \n",
       "196104  -1.39  1.40  -0.23  -2.21  -6.37   2.66   2.60  -0.47  -1.47  -5.31   \n",
       "196105   4.20  5.38   3.39  -3.91   2.71  -0.02   6.80   2.10   5.50   5.47   \n",
       "196106  -2.17 -3.12   3.97  -5.87  -3.85   3.43  -5.50  -3.58  -1.32  -3.36   \n",
       "196107   2.72  0.88   5.95  -1.21  -2.55   1.97   2.03   3.27   2.95   1.53   \n",
       "196108   4.92  3.20   7.74   0.89   0.89  10.45   5.21   3.70   2.35   5.77   \n",
       "196109  -0.62 -1.48  -0.07   1.24   0.75  -3.05  -1.14  -1.48  -4.45  -4.25   \n",
       "196110   3.73 -0.84   7.05  -5.26   0.99  -0.67   8.28   3.33   0.05   3.11   \n",
       "196111   5.28  4.47   8.03   0.25   3.75   4.51   5.30   3.12   2.49   7.37   \n",
       "196112  -3.69  1.41  -6.12   1.97  -3.66  -3.78   0.32  -2.21  -0.16  -1.17   \n",
       "196201  -6.67 -3.45  -4.28 -13.23  -3.44  -7.37  -5.89  -4.86  -4.76   0.57   \n",
       "196202  -0.25  0.28   0.68  -2.02  -0.52  -0.90   2.01   3.56   3.30   1.93   \n",
       "196203   0.98 -0.34  -6.67  -5.34   0.41   4.31  -1.18   0.34  -2.72  -0.74   \n",
       "196204  -4.59 -3.59 -12.99 -11.04  -8.74  -7.03  -8.01 -11.23  -6.23  -7.53   \n",
       "196205 -11.25 -9.05 -14.14 -11.39 -14.87 -10.19 -10.01 -11.14  -8.25  -7.50   \n",
       "...       ...   ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "201507   4.03  3.51   9.59   6.09  -2.90   0.71   5.96   3.66  -4.90  -0.72   \n",
       "201508  -4.37 -3.12  -4.06  -7.35  -8.61  -6.94  -3.86  -8.37  -7.15  -3.11   \n",
       "201509  -1.19  2.58   2.37  -9.94  -5.32  -0.53   1.18  -7.28  -8.38  -5.92   \n",
       "201510   5.81  8.06  10.90  14.61  12.21   5.81   0.98   7.74  16.62   7.96   \n",
       "201511   0.11 -0.71  -3.00  -0.41  -1.17  -1.10  -1.08   0.71   1.68  -2.59   \n",
       "201512   1.96  0.30   1.59  -1.70  -6.18   1.86  -4.38   0.39  -4.82  -2.65   \n",
       "201601  -1.67 -0.23   4.28  -8.15  -5.28   0.16   1.52  -9.43 -11.10  -5.33   \n",
       "201602   0.95 -2.34   0.93   4.25  -0.96   0.34   0.81  -1.09   6.79   0.63   \n",
       "201603   4.69  5.61   5.04   8.61   6.88   4.65   2.30   2.90   8.33   3.58   \n",
       "201604   0.63  0.31  -0.25  -6.30   1.82  -0.42  -2.27   3.55   3.77   1.55   \n",
       "201605   2.06 -0.91   0.83   5.42  -0.53  -0.09  -4.96   2.46  -1.42  -1.70   \n",
       "201606   4.75  5.31   6.87  -4.43  -0.34   3.16   1.63   0.11  -1.14  -4.67   \n",
       "201607  -0.51  1.82  -2.79   6.15   7.38   2.58   1.62   6.00   4.29   8.39   \n",
       "201608  -0.52 -0.90  -1.22   0.94   0.29   1.24   1.37  -3.24   2.48   1.03   \n",
       "201609  -2.92  1.63  -2.78   4.62  -3.95   0.00  -6.92   0.35  -1.76  -4.87   \n",
       "201610  -0.33 -1.65   4.59   5.59 -10.28  -2.96  -5.76  -7.45  -1.95  -4.17   \n",
       "201611  -4.41 -5.76  -5.12   3.87   8.15  -4.18   1.80   1.37   7.55   1.58   \n",
       "201612   4.43  3.00   5.39  -3.36   1.98   1.43  -0.44   0.82   0.32  -1.27   \n",
       "201701   0.95 -1.02   5.61   4.84   1.60   2.72  -0.01   2.17   3.79   6.90   \n",
       "201702   1.71  6.13   7.93   0.65  -0.39   4.83   3.81   7.03   3.10  -2.12   \n",
       "201703   0.52  0.89   1.03   5.79  -0.92  -0.36   0.32  -0.19   2.17   1.94   \n",
       "201704   0.76  1.89  -0.14   2.74  -0.83  -0.36  -1.24   1.06   1.28   3.24   \n",
       "201705   1.63  4.28   6.12   2.80  -1.71   1.77  -2.17  -0.50  -0.63  -0.14   \n",
       "201706  -2.65 -1.20  -1.00  -0.20   2.69  -0.19   9.38   5.49   1.93   4.39   \n",
       "201707   1.52  1.18  -6.06   4.98   2.34   1.79   1.25   0.45   1.03   1.22   \n",
       "201708  -2.77  0.94  -0.96  -1.78  -6.37  -0.04  -5.79   1.69   1.52   2.62   \n",
       "201709   0.43 -3.00  -2.13   3.66   3.54  -0.21   1.01   1.40   6.58   0.45   \n",
       "201710   0.71  1.24  -2.90   2.02  -1.90  -3.09   3.47  -2.35   5.52   1.23   \n",
       "201711   4.15  4.33   1.34   3.30  10.00   4.55   7.94   2.28   2.17   3.80   \n",
       "201712  -0.10  4.31   4.87   0.80   1.06   2.31   4.65  -0.32   0.49  -1.77   \n",
       "\n",
       "           ...      Telcm.lead  Servs.lead  BusEq.lead  Paper.lead  \\\n",
       "yyyymm     ...                                                       \n",
       "195912     ...            0.62       -6.18       -7.93       -9.41   \n",
       "196001     ...            8.07        9.13        5.09        3.00   \n",
       "196002     ...           -0.21       -0.31        3.34       -2.43   \n",
       "196003     ...           -1.24        7.14        1.77        0.41   \n",
       "196004     ...            3.05       -1.75       11.90        2.85   \n",
       "196005     ...           -0.58       -8.07        2.39        3.50   \n",
       "196006     ...           -0.03        2.84       -2.02       -4.10   \n",
       "196007     ...            6.94        5.69        2.71        1.18   \n",
       "196008     ...           -6.07       -3.53       -7.61       -7.37   \n",
       "196009     ...           -0.08        4.62       -3.40       -1.85   \n",
       "196010     ...            4.06        9.49        8.19        5.31   \n",
       "196011     ...           12.29        8.18        4.29        5.57   \n",
       "196012     ...            7.70        4.29        5.08        4.56   \n",
       "196101     ...            0.61        0.20        4.54        6.83   \n",
       "196102     ...            7.23       -0.20        2.31       -0.69   \n",
       "196103     ...            0.63       -0.12        2.19       -0.37   \n",
       "196104     ...           -1.22       -0.70        1.57        1.39   \n",
       "196105     ...           -4.19        0.13       -3.31       -4.46   \n",
       "196106     ...            6.25       -8.25        0.56       -0.50   \n",
       "196107     ...            0.12        8.99        5.11        5.37   \n",
       "196108     ...           -2.94       -6.04        1.01       -2.74   \n",
       "196109     ...            0.00        2.24        6.53        1.74   \n",
       "196110     ...            8.34        8.27        0.99        2.05   \n",
       "196111     ...            3.14       -0.68       -0.55       -2.65   \n",
       "196112     ...           -6.32       -4.88       -6.91       -5.22   \n",
       "196201     ...            3.89       -1.94       -0.07        3.87   \n",
       "196202     ...           -3.14       -1.41       -0.76        1.13   \n",
       "196203     ...           -4.93       -3.11      -12.01       -7.93   \n",
       "196204     ...           -7.35      -10.97      -13.19      -11.03   \n",
       "196205     ...           -8.72      -13.11      -12.59       -9.62   \n",
       "...        ...             ...         ...         ...         ...   \n",
       "201507     ...           -8.42       -5.29       -6.50       -5.69   \n",
       "201508     ...           -2.63       -1.47       -1.72       -2.66   \n",
       "201509     ...            8.84       11.26        8.16       10.19   \n",
       "201510     ...           -1.92        1.99        0.12       -0.02   \n",
       "201511     ...           -3.03       -1.19       -4.64       -3.75   \n",
       "201512     ...           -0.36       -5.09       -7.95       -5.27   \n",
       "201601     ...            1.15       -2.45        1.45        2.99   \n",
       "201602     ...            6.00        7.76        8.86        8.18   \n",
       "201603     ...            0.59       -2.55       -5.46        0.80   \n",
       "201604     ...            0.30        4.67        5.64        1.79   \n",
       "201605     ...            3.10       -2.12       -1.63        2.06   \n",
       "201606     ...            2.27        7.33        8.20        2.52   \n",
       "201607     ...           -3.56        1.19        2.38        2.46   \n",
       "201608     ...            0.52        0.82        4.33       -0.64   \n",
       "201609     ...           -2.85       -0.55       -2.24       -5.49   \n",
       "201610     ...            6.25       -0.01        2.39        4.21   \n",
       "201611     ...            4.65       -0.19        2.07        1.86   \n",
       "201612     ...            3.36        5.45        3.20        2.28   \n",
       "201701     ...           -0.11        3.23        6.99        3.21   \n",
       "201702     ...            0.81        1.81        2.45        0.26   \n",
       "201703     ...           -0.16        3.83        0.99        2.06   \n",
       "201704     ...           -2.00        3.60        4.57        2.60   \n",
       "201705     ...           -2.33       -0.82       -3.44        1.87   \n",
       "201706     ...            5.25        3.98        3.05       -2.42   \n",
       "201707     ...           -2.58        1.36        4.98        0.72   \n",
       "201708     ...           -1.68        0.60        0.80        2.44   \n",
       "201709     ...           -5.75        5.93        7.12        4.28   \n",
       "201710     ...            3.91        0.05        2.41        4.21   \n",
       "201711     ...            4.41        0.39       -0.89       -0.95   \n",
       "201712     ...            3.08        9.25        5.02        4.28   \n",
       "\n",
       "        Trans.lead  Whlsl.lead  Rtail.lead  Meals.lead  Fin.lead  Other.lead  \n",
       "yyyymm                                                                        \n",
       "195912       -4.31       -5.33       -6.09      -10.08     -4.68       -3.98  \n",
       "196001       -0.94        1.42        4.00        1.81     -0.98        6.32  \n",
       "196002       -4.99       -1.37       -0.13       -3.88      0.05       -2.43  \n",
       "196003       -2.13        0.45       -0.53        8.86     -0.64        0.55  \n",
       "196004        0.90        1.65        3.11        0.80     -0.45        1.02  \n",
       "196005        2.17        5.96        3.41        1.03      3.72        6.41  \n",
       "196006       -3.11       -6.16       -2.99       -1.25      0.09       -5.95  \n",
       "196007        1.98        4.51        2.85        2.05      3.47        3.48  \n",
       "196008       -7.07       -8.44       -8.57       -1.90     -5.78       -4.21  \n",
       "196009       -1.02       -4.22        0.31       -4.54     -0.40        0.38  \n",
       "196010        5.35        9.72        6.50        4.40      7.71        4.01  \n",
       "196011        2.27        2.06        2.05        2.08      5.56        3.80  \n",
       "196012        8.35        7.93        2.28        4.08      7.12        8.23  \n",
       "196101        4.22        3.31        4.82        8.23      7.00        6.00  \n",
       "196102        0.86        4.45        5.76        4.06      4.34        7.08  \n",
       "196103       -1.62        3.08        0.22        4.23      1.38       -3.67  \n",
       "196104        4.74       -0.04        4.31       -1.90      4.00        3.32  \n",
       "196105       -4.57       -4.90        0.80       -5.63     -2.88        0.37  \n",
       "196106       -0.32       -0.01        2.45        2.69      3.35        5.37  \n",
       "196107        3.52        3.09        3.03        0.46      8.65        1.64  \n",
       "196108       -1.16       -4.22        0.66       -6.21     -0.40        3.14  \n",
       "196109        2.16        4.30        9.35        0.71      2.02        0.39  \n",
       "196110        0.47        5.65        4.90        1.08      7.22        1.69  \n",
       "196111       -0.24        0.46       -0.63       -2.21     -4.44       -0.77  \n",
       "196112        2.52       -0.79       -9.56       -3.90     -4.99       -3.62  \n",
       "196201        0.32       -0.09        1.58       -0.59      3.59        4.20  \n",
       "196202       -1.68       -2.30        0.90       -4.07     -2.13       -1.83  \n",
       "196203       -6.27       -5.78       -4.61       -9.09     -7.69       -2.12  \n",
       "196204       -5.17      -11.34       -9.09       -7.46    -10.02      -11.83  \n",
       "196205       -7.81      -11.11      -10.43      -12.90    -11.01      -14.25  \n",
       "...            ...         ...         ...         ...       ...         ...  \n",
       "201507       -6.37       -4.13       -5.44       -6.48     -6.54       -5.20  \n",
       "201508       -0.71       -6.04       -1.75        0.44     -3.14       -1.87  \n",
       "201509        6.48        5.07        4.56        5.05      5.90        6.98  \n",
       "201510       -1.10        2.67        0.61       -1.01      2.16        0.05  \n",
       "201511       -5.02       -1.88        0.82       -0.95     -2.92        0.25  \n",
       "201512       -8.53       -8.68       -4.45       -0.94     -9.63       -3.20  \n",
       "201601        6.89        3.85       -0.36        1.03     -2.85        2.71  \n",
       "201602        6.86        6.18        5.99        5.36      6.65        6.68  \n",
       "201603       -1.08        0.49       -0.38       -2.38      3.96        0.67  \n",
       "201604       -2.18        1.78        1.19       -1.48      2.15       -2.02  \n",
       "201605       -2.53        1.81        0.71        1.16     -5.30        3.61  \n",
       "201606        5.39        3.65        3.78        2.19      4.04       -0.21  \n",
       "201607        1.09       -1.03       -1.69       -0.24      4.88        2.24  \n",
       "201608        2.86       -2.56       -0.18       -2.25     -1.45       -3.48  \n",
       "201609       -0.64       -8.18       -3.59       -1.96      1.40       -0.53  \n",
       "201610       12.75        9.29        2.99        8.47     12.84        8.29  \n",
       "201611        0.84        2.34       -0.98        0.58      3.80        2.57  \n",
       "201612        1.70        1.69        0.93        0.71      0.56       -0.87  \n",
       "201701        2.56        2.73        2.79        2.35      4.58        3.71  \n",
       "201702       -2.80       -1.29        0.80        2.73     -2.26       -1.83  \n",
       "201703        2.14       -2.96        3.21        4.30      0.18       -1.06  \n",
       "201704        2.68        2.56        0.45        6.93     -1.03       -0.74  \n",
       "201705        3.38        0.15       -2.45       -2.54      5.70        1.31  \n",
       "201706       -4.49       -0.99        0.93       -2.63      1.86        1.44  \n",
       "201707        0.60       -4.55       -1.81        1.26     -1.31        1.53  \n",
       "201708        6.20        4.46        2.71        0.79      5.07        0.99  \n",
       "201709       -1.99       -1.18        3.22        3.64      3.22       -1.20  \n",
       "201710        6.66        3.37        9.38        5.45      3.76        1.60  \n",
       "201711        2.73        4.21        2.45        1.18      0.88        1.14  \n",
       "201712        2.56        4.84       11.37        3.12      6.00        5.41  \n",
       "\n",
       "[697 rows x 133 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = data.loc[data.index[data.index < 201701]]\n",
    "data = data.loc[data.index[data.index > 195911]]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Beer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Games</th>\n",
       "      <th>Books</th>\n",
       "      <th>Hshld</th>\n",
       "      <th>Clths</th>\n",
       "      <th>Hlth</th>\n",
       "      <th>Chems</th>\n",
       "      <th>Txtls</th>\n",
       "      <th>...</th>\n",
       "      <th>Telcm.lead</th>\n",
       "      <th>Servs.lead</th>\n",
       "      <th>BusEq.lead</th>\n",
       "      <th>Paper.lead</th>\n",
       "      <th>Trans.lead</th>\n",
       "      <th>Whlsl.lead</th>\n",
       "      <th>Rtail.lead</th>\n",
       "      <th>Meals.lead</th>\n",
       "      <th>Fin.lead</th>\n",
       "      <th>Other.lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.688666</td>\n",
       "      <td>0.727030</td>\n",
       "      <td>0.985079</td>\n",
       "      <td>0.732095</td>\n",
       "      <td>0.532253</td>\n",
       "      <td>0.564333</td>\n",
       "      <td>0.690387</td>\n",
       "      <td>0.665825</td>\n",
       "      <td>0.552367</td>\n",
       "      <td>0.687145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515968</td>\n",
       "      <td>0.729928</td>\n",
       "      <td>0.622970</td>\n",
       "      <td>0.534806</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.631076</td>\n",
       "      <td>0.698235</td>\n",
       "      <td>0.728766</td>\n",
       "      <td>0.637547</td>\n",
       "      <td>0.396628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.308660</td>\n",
       "      <td>5.058992</td>\n",
       "      <td>6.032324</td>\n",
       "      <td>7.128170</td>\n",
       "      <td>5.780362</td>\n",
       "      <td>4.728000</td>\n",
       "      <td>6.355251</td>\n",
       "      <td>4.897557</td>\n",
       "      <td>5.482363</td>\n",
       "      <td>6.970961</td>\n",
       "      <td>...</td>\n",
       "      <td>4.607931</td>\n",
       "      <td>6.486956</td>\n",
       "      <td>6.698787</td>\n",
       "      <td>5.021876</td>\n",
       "      <td>5.707154</td>\n",
       "      <td>5.571040</td>\n",
       "      <td>5.334178</td>\n",
       "      <td>6.065564</td>\n",
       "      <td>5.381389</td>\n",
       "      <td>5.771655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.150000</td>\n",
       "      <td>-20.190000</td>\n",
       "      <td>-25.320000</td>\n",
       "      <td>-33.400000</td>\n",
       "      <td>-26.560000</td>\n",
       "      <td>-22.240000</td>\n",
       "      <td>-31.500000</td>\n",
       "      <td>-21.060000</td>\n",
       "      <td>-28.600000</td>\n",
       "      <td>-33.110000</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.440000</td>\n",
       "      <td>-28.670000</td>\n",
       "      <td>-32.070000</td>\n",
       "      <td>-27.740000</td>\n",
       "      <td>-28.500000</td>\n",
       "      <td>-29.250000</td>\n",
       "      <td>-29.740000</td>\n",
       "      <td>-31.890000</td>\n",
       "      <td>-22.530000</td>\n",
       "      <td>-28.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.630000</td>\n",
       "      <td>-2.080000</td>\n",
       "      <td>-2.740000</td>\n",
       "      <td>-3.390000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>-2.030000</td>\n",
       "      <td>-2.800000</td>\n",
       "      <td>-2.230000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-3.170000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110000</td>\n",
       "      <td>-3.050000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-2.780000</td>\n",
       "      <td>-2.560000</td>\n",
       "      <td>-2.380000</td>\n",
       "      <td>-2.840000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.070000</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>4.660000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>3.640000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>4.310000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>4.480000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>4.260000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>3.460000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.890000</td>\n",
       "      <td>25.510000</td>\n",
       "      <td>32.380000</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>33.130000</td>\n",
       "      <td>18.220000</td>\n",
       "      <td>31.790000</td>\n",
       "      <td>29.010000</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>59.030000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.220000</td>\n",
       "      <td>23.380000</td>\n",
       "      <td>24.660000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>17.530000</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>27.380000</td>\n",
       "      <td>20.590000</td>\n",
       "      <td>19.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Food        Beer       Smoke       Games       Books       Hshld  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000  697.000000  697.000000   \n",
       "mean     0.688666    0.727030    0.985079    0.732095    0.532253    0.564333   \n",
       "std      4.308660    5.058992    6.032324    7.128170    5.780362    4.728000   \n",
       "min    -18.150000  -20.190000  -25.320000  -33.400000  -26.560000  -22.240000   \n",
       "25%     -1.630000   -2.080000   -2.740000   -3.390000   -2.600000   -2.030000   \n",
       "50%      0.740000    0.750000    1.270000    0.940000    0.510000    0.750000   \n",
       "75%      3.070000    3.690000    4.660000    5.260000    3.640000    3.540000   \n",
       "max     19.890000   25.510000   32.380000   34.520000   33.130000   18.220000   \n",
       "\n",
       "            Clths        Hlth       Chems       Txtls     ...      Telcm.lead  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000     ...      697.000000   \n",
       "mean     0.690387    0.665825    0.552367    0.687145     ...        0.515968   \n",
       "std      6.355251    4.897557    5.482363    6.970961     ...        4.607931   \n",
       "min    -31.500000  -21.060000  -28.600000  -33.110000     ...      -16.440000   \n",
       "25%     -2.800000   -2.230000   -2.750000   -3.170000     ...       -2.110000   \n",
       "50%      0.700000    0.760000    0.720000    0.640000     ...        0.590000   \n",
       "75%      4.310000    3.550000    3.760000    4.480000     ...        3.360000   \n",
       "max     31.790000   29.010000   21.680000   59.030000     ...       21.220000   \n",
       "\n",
       "       Servs.lead  BusEq.lead  Paper.lead  Trans.lead  Whlsl.lead  Rtail.lead  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000  697.000000  697.000000   \n",
       "mean     0.729928    0.622970    0.534806    0.601090    0.631076    0.698235   \n",
       "std      6.486956    6.698787    5.021876    5.707154    5.571040    5.334178   \n",
       "min    -28.670000  -32.070000  -27.740000  -28.500000  -29.250000  -29.740000   \n",
       "25%     -3.050000   -3.220000   -2.400000   -2.780000   -2.560000   -2.380000   \n",
       "50%      1.010000    0.670000    0.710000    0.900000    0.940000    0.540000   \n",
       "75%      4.260000    4.630000    3.460000    4.040000    3.880000    3.980000   \n",
       "max     23.380000   24.660000   21.000000   18.500000   17.530000   26.490000   \n",
       "\n",
       "       Meals.lead    Fin.lead  Other.lead  \n",
       "count  697.000000  697.000000  697.000000  \n",
       "mean     0.728766    0.637547    0.396628  \n",
       "std      6.065564    5.381389    5.771655  \n",
       "min    -31.890000  -22.530000  -28.090000  \n",
       "25%     -2.840000   -2.400000   -2.930000  \n",
       "50%      1.080000    0.870000    0.540000  \n",
       "75%      4.300000    4.000000    4.200000  \n",
       "max     27.380000   20.590000   19.960000  \n",
       "\n",
       "[8 rows x 133 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = data.describe()\n",
    "desc\n",
    "# min, max line up with Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 103)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract X and Y as raw arrays\n",
    "X = data.values[:-1,:npredictors]\n",
    "Y = data.values[:-1,-nresponses:]\n",
    "nrows = X.shape[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.49  -5.71  -2.05   1.21  -5.47  -7.84  -8.53  -6.68 -10.03  -4.77\n",
      "  -6.67  -9.38  -4.42 -12.3  -11.71  -5.03  -3.81  -7.91  -7.82  -2.4\n",
      "   0.62  -6.18  -7.93  -9.41  -4.31  -5.33  -6.09 -10.08  -4.68  -3.98]\n",
      "[13 14 27  8 23 11  6 22 17  5 18  7 10 21 26  1  4 25 15  9 28  0 12 24\n",
      " 29 16 19  2 20  3]\n",
      "[-12.3  -11.71 -10.08 -10.03  -9.41  -9.38  -8.53  -7.93  -7.91  -7.84\n",
      "  -7.82  -6.68  -6.67  -6.18  -6.09  -5.71  -5.47  -5.33  -5.03  -4.77\n",
      "  -4.68  -4.49  -4.42  -4.31  -3.98  -3.81  -2.4   -2.05   0.62   1.21]\n",
      "[0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 0. 2. 0. 2. 2. 0. 1. 0. 0. 1. 1. 0. 0. 2.\n",
      " 0. 0. 0. 2. 0. 1.]\n",
      "[-2.05, 1.21, -3.81, -2.4, 0.6199999999999999, -3.98]\n",
      "[]\n",
      "(696, 30)\n",
      "(696, 103)\n"
     ]
    }
   ],
   "source": [
    "# convert Ys to 3 classes\n",
    "# long = 1\n",
    "# short = 2\n",
    "# neither = 0\n",
    "ISLONG=1\n",
    "ISSHORT=2\n",
    "ISFLAT=0\n",
    "\n",
    "Y_sortindex = np.argsort(Y)\n",
    "print(Y[0])\n",
    "# sorted position\n",
    "print(Y_sortindex[0]) \n",
    "# sorted array\n",
    "print(Y[0,Y_sortindex[0]])\n",
    "# initialize class to 0\n",
    "Y_class=np.zeros_like(Y)\n",
    "for row in range(Y_class.shape[0]):\n",
    "    # if index in last 6, long\n",
    "    longlist = Y_sortindex[row,-6:]\n",
    "    Y_class[row, longlist]=ISLONG\n",
    "    # if index is in first 6, short\n",
    "    shortlist = Y_sortindex[row,:6]\n",
    "    Y_class[row, shortlist]=ISSHORT\n",
    "    \n",
    "print(Y_class[0])\n",
    "print([Y[0,i] for i in range(30) if Y_class[0,i]==1])\n",
    "print([Y[0,i] for i in range(30) if Y_class[0,i]==-1])\n",
    "print(Y_class.shape)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.150000000000002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(Y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "# try keras classifier\n",
    "# wrap their model in a class \n",
    "# use multioutput to speed  up \n",
    "# fit takes a list of response ys, predict returns a list of y_predict arrays\n",
    "INPUT_DIM = X.shape[1]\n",
    "print(INPUT_DIM)\n",
    "NCLASSES=3\n",
    "OUTPUT_DIM = len(responses) # 30\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS=200\n",
    "\n",
    "class KerasBacktestModel(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 n_hidden_layers = 2,\n",
    "                 hidden_layer_size = 32,\n",
    "                 reg_penalty = 0.0001,\n",
    "                 dropout = 0.333,\n",
    "                 verbose=True):\n",
    "        \"\"\"initialize keras model\"\"\"\n",
    "        \n",
    "        main_input = Input(shape=(INPUT_DIM,),\n",
    "                           dtype='float32', \n",
    "                           name='main_input')\n",
    "        lastlayer=main_input\n",
    "        \n",
    "        for i in range(n_hidden_layers):\n",
    "            if verbose:\n",
    "                print(\"layer %d size %d, reg_penalty %.8f, dropout %.3f\" % (i + 1, \n",
    "                                                                            hidden_layer_size, \n",
    "                                                                            reg_penalty, \n",
    "                                                                            dropout))\n",
    "            lastlayer = Dense(units = hidden_layer_size, \n",
    "                              activation = 'relu',\n",
    "                              kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                              kernel_regularizer=keras.regularizers.l1(reg_penalty),\n",
    "                              name = \"Dense%02d\" % i)(lastlayer)\n",
    "            \n",
    "            if dropout:\n",
    "                lastlayer = Dropout(dropout, name = \"Dropout%02d\" % i)(lastlayer)\n",
    "                \n",
    "        outputs = []\n",
    "        for i in range(OUTPUT_DIM):\n",
    "            # OUTPUT_DIM outputs\n",
    "            outputs.append(Dense(NCLASSES, \n",
    "                                 activation='softmax',\n",
    "                                 name = \"Output%02d\" % (i+1))(lastlayer))\n",
    "            \n",
    "        self.model = Model(inputs=[main_input], outputs=outputs)\n",
    "        if verbose:\n",
    "            print(self.model.summary())\n",
    "            \n",
    "        self.model.compile(loss=\"categorical_crossentropy\", \n",
    "                           optimizer=\"rmsprop\", \n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def fit(self, X, Y, epochs=EPOCHS):\n",
    "        \"\"\"fit keras model for epochs\"\"\"\n",
    "        # convert Y to list of ys\n",
    "        nrows, npreds = Y.shape\n",
    "        Y_list = [keras.utils.to_categorical(Y[:,i], num_classes=NCLASSES) for i in range(OUTPUT_DIM)]\n",
    "\n",
    "        fit = self.model.fit(X,\n",
    "                             Y_list,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             epochs=epochs,\n",
    "                             verbose=False)\n",
    "        #evaluate returns a list of overall loss, loss by column and then accuracy by column\n",
    "        evaluate = self.model.evaluate(X, Y_list, batch_size=BATCH_SIZE, verbose=1)\n",
    "        self.accuracy = np.mean(np.array(evaluate[-npreds:]))\n",
    "\n",
    "        return fit\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"predict classes using X\"\"\"\n",
    "        # convert list of ys to Y array\n",
    "        nrows, npreds = X.shape\n",
    "        y_list = self.model.predict(X)\n",
    "        longprobs = np.zeros([nrows, OUTPUT_DIM])\n",
    "        shortprobs = np.zeros([nrows, OUTPUT_DIM])\n",
    "        flatprobs = np.zeros([nrows, OUTPUT_DIM])\n",
    "\n",
    "        evaluate_array = self.model.evaluate(X, y_list, batch_size=BATCH_SIZE, verbose=1)\n",
    "        self.accuracy = np.mean(np.array(evaluate_array[-npreds:]))\n",
    "        \n",
    "        for response in range(OUTPUT_DIM):\n",
    "            for row in range(nrows):\n",
    "                longprobs[row, response] = y_list[response][row, ISLONG]\n",
    "                shortprobs[row, response] = y_list[response][row, ISSHORT]\n",
    "                flatprobs[row, response] = y_list[response][row, ISFLAT]\n",
    "                \n",
    "        return longprobs, shortprobs, flatprobs\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        \"\"\"predict classes using X\"\"\"\n",
    "        # convert list of ys to Y array\n",
    "        nrows, npreds = Y.shape\n",
    "        Y_list = [keras.utils.to_categorical(Y[:,i], num_classes=NCLASSES) for i in range(OUTPUT_DIM)]\n",
    "        evaluate_array = self.model.evaluate(X, Y_list, batch_size=BATCH_SIZE, verbose=False)\n",
    "        self.accuracy = np.mean(np.array(evaluate_array[-npreds:]))\n",
    "        \n",
    "        return self.accuracy\n",
    "    \n",
    "    \n",
    "    def save(self, modelname):\n",
    "        self.model.save(\"%s.h5\" % modelname)\n",
    "        self.model.save_weights(\"%s_weights.h5\" % modelname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(n_hidden_layers, layer_size, reg_penalty, dropout, verbose=False):\n",
    "    def create_func():\n",
    "        return KerasBacktestModel(n_hidden_layers = n_hidden_layers,\n",
    "                                  hidden_layer_size = layer_size,\n",
    "                                  reg_penalty = reg_penalty,\n",
    "                                  dropout = dropout,\n",
    "                                  verbose=verbose)\n",
    "    return create_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:55:43 Start fit\n",
      "layer 1 size 4, reg_penalty 0.01000000, dropout 0.250\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 103)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dense00 (Dense)                 (None, 4)            416         main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dropout00 (Dropout)             (None, 4)            0           Dense00[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output01 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output02 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output03 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output04 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output05 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output06 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output07 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output08 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output09 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output10 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output11 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output12 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output13 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output14 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output15 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output16 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output17 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output18 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output19 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output20 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output21 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output22 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output23 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output24 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output25 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output26 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output27 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output28 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output29 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output30 (Dense)                (None, 3)            15          Dropout00[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 866\n",
      "Trainable params: 866\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(693, 103)\n",
      "(693, 30)\n",
      "693/693 [==============================] - 11s 16ms/step\n",
      "3/3 [==============================] - 0s 22ms/step\n",
      "15:02:05 End fit\n",
      "[[0.18001409 0.24424779 0.34105751 0.25287753 0.16770165 0.16895089\n",
      "  0.21651281 0.21446735 0.13923946 0.23559579 0.1273074  0.20047195\n",
      "  0.16785075 0.18142681 0.2254999  0.23546049 0.28559941 0.30624968\n",
      "  0.25093836 0.25631556 0.20255314 0.18296643 0.1980516  0.10519207\n",
      "  0.18119942 0.11146001 0.1685829  0.19551207 0.15776929 0.14863326]\n",
      " [0.05142649 0.16640641 0.25498432 0.30886507 0.16632526 0.0739701\n",
      "  0.18635334 0.16727851 0.10970846 0.19623445 0.17656356 0.25215474\n",
      "  0.20859075 0.23695871 0.22184196 0.19755521 0.3400023  0.28038153\n",
      "  0.22288367 0.0869388  0.20065685 0.31664324 0.22341554 0.09088121\n",
      "  0.21834566 0.22687934 0.17369086 0.21192308 0.09814936 0.16583462]\n",
      " [0.18001409 0.24424779 0.34105751 0.25287753 0.16770165 0.16895089\n",
      "  0.21651281 0.21446735 0.13923946 0.23559579 0.1273074  0.20047195\n",
      "  0.16785075 0.18142681 0.2254999  0.23546049 0.28559941 0.30624968\n",
      "  0.25093836 0.25631556 0.20255314 0.18296643 0.1980516  0.10519207\n",
      "  0.18119942 0.11146001 0.1685829  0.19551207 0.15776929 0.14863326]]\n",
      "\n",
      "[[0.15540628 0.21889327 0.26267117 0.23906837 0.1718283  0.16168225\n",
      "  0.19324075 0.17083845 0.13929923 0.21239217 0.13816185 0.30806392\n",
      "  0.16991667 0.18157876 0.24938975 0.15876599 0.32973269 0.37386099\n",
      "  0.23928989 0.23705697 0.21765737 0.19650528 0.21598451 0.12263148\n",
      "  0.17124382 0.10934461 0.17362876 0.19012707 0.16836996 0.17418061]\n",
      " [0.18996197 0.24047343 0.3328869  0.21268019 0.06963166 0.28673273\n",
      "  0.19701917 0.25942546 0.18686908 0.12541924 0.15719409 0.11858483\n",
      "  0.0804465  0.19205354 0.19504088 0.23472781 0.1916628  0.36126027\n",
      "  0.24660107 0.41320357 0.32904467 0.1230059  0.20690186 0.13014235\n",
      "  0.11889819 0.05426155 0.153979   0.18772283 0.05200095 0.14241095]\n",
      " [0.15540628 0.21889327 0.26267117 0.23906837 0.1718283  0.16168225\n",
      "  0.19324075 0.17083845 0.13929923 0.21239217 0.13816185 0.30806392\n",
      "  0.16991667 0.18157876 0.24938975 0.15876599 0.32973269 0.37386099\n",
      "  0.23928989 0.23705697 0.21765737 0.19650528 0.21598451 0.12263148\n",
      "  0.17124382 0.10934461 0.17362876 0.19012707 0.16836996 0.17418061]]\n",
      "\n",
      "[[0.66457957 0.53685892 0.39627129 0.50805414 0.66047007 0.6693669\n",
      "  0.5902465  0.61469418 0.72146136 0.55201203 0.73453069 0.49146417\n",
      "  0.66223258 0.63699442 0.52511036 0.60577357 0.38466796 0.31988934\n",
      "  0.5097717  0.50662756 0.57978952 0.62052834 0.5859639  0.77217644\n",
      "  0.64755672 0.77919531 0.6577884  0.61436081 0.67386067 0.67718613]\n",
      " [0.75861156 0.5931201  0.41212875 0.47845474 0.76404309 0.63929719\n",
      "  0.61662751 0.57329607 0.70342249 0.67834628 0.66624242 0.62926048\n",
      "  0.71096271 0.5709877  0.58311713 0.5677169  0.46833491 0.35835817\n",
      "  0.53051525 0.49985769 0.47029856 0.56035089 0.5696826  0.77897644\n",
      "  0.66275615 0.71885914 0.67233008 0.60035408 0.8498497  0.69175446]\n",
      " [0.66457957 0.53685892 0.39627129 0.50805414 0.66047007 0.6693669\n",
      "  0.5902465  0.61469418 0.72146136 0.55201203 0.73453069 0.49146417\n",
      "  0.66223258 0.63699442 0.52511036 0.60577357 0.38466796 0.31988934\n",
      "  0.5097717  0.50662756 0.57978952 0.62052834 0.5859639  0.77217644\n",
      "  0.64755672 0.77919531 0.6577884  0.61436081 0.67386067 0.67718613]]\n",
      "\n",
      "1.395228955589357\n"
     ]
    }
   ],
   "source": [
    "def fit_predict_keras(X, Y, model, epochs=EPOCHS, npredict=1, verbose=False):\n",
    "    \"\"\"simpler fit_predict, no coef_dict, fits all at once, specifies epochs \n",
    "    for backtest, train model using Y_list v. X using n-npredict rows\n",
    "    generate npredict prediction Y_list using last npredict rows of X\n",
    "    if npredict=1, fit using n-1 rows, return prediction using X for final month\n",
    "    if npredict=26, fit using n-26 rows, return prediction using X for final 26 months\"\"\"\n",
    "    \n",
    "    nrows = X.shape[0]\n",
    "    if verbose:\n",
    "        print(\"Fit on %d rows 0 to %d\" % (nrows-npredict, nrows-npredict-1))\n",
    "        print(\"Predict on %d rows %d to %d\" % (npredict, nrows-npredict, nrows-1))\n",
    "        \n",
    "    # keep last rows to predict against\n",
    "    X_predict = X[-npredict:]\n",
    "    X_predict = X_predict.reshape(npredict,X.shape[1])\n",
    "    # fit on remaining rows\n",
    "    X_fit = X[:-npredict]\n",
    "    Y_fit = Y[:-npredict]\n",
    "    print(X_fit.shape)\n",
    "    print(Y_fit.shape)\n",
    "    fit = model.fit(\n",
    "        X_fit,\n",
    "        Y_fit,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    return model.predict(X_predict)\n",
    "\n",
    "print(\"%s Start fit\" % (time.strftime(\"%H:%M:%S\")))\n",
    "keras_create_model=create_keras_model(1, 4, 0.01, 0.25, verbose=True)\n",
    "keras_model=keras_create_model()\n",
    "longprobs, shortprobs, flatprobs = fit_predict_keras(X, Y_class, keras_model, npredict=3, epochs=200)\n",
    "print(\"%s End fit\" % (time.strftime(\"%H:%M:%S\")))\n",
    "\n",
    "print(longprobs)\n",
    "print(\"\")\n",
    "print(shortprobs)\n",
    "print(\"\")\n",
    "print(flatprobs)\n",
    "print(\"\")\n",
    "print(keras_model.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6063218391261338"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.evaluate(X,Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a bunch of experiments\n",
    "\n",
    "EPOCHS=500\n",
    "def walkforward_xval_keras (X, Y, create_model, n_splits=5, epochs=EPOCHS):\n",
    "    ### no coef_dict, fit all at once\n",
    "\n",
    "    # generate k-folds\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    kf.get_n_splits(X)\n",
    "    last_indexes = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # use test_index as last index to train\n",
    "        last_index = test_index[-1] + 1\n",
    "        last_indexes.append(last_index)\n",
    "    print(\"%s Generate splits %s\" % (time.strftime(\"%H:%M:%S\"), str([i for i in last_indexes])))\n",
    "\n",
    "    print(\"%s Starting training\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    model = create_model()\n",
    "    \n",
    "    avg_bests = []\n",
    "    for i in range(1, n_splits-1):\n",
    "\n",
    "        models = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        count = 0        \n",
    "        # skip kfold 0 so you start with train 2x size of eval set\n",
    "        last_train_index = last_indexes[i]\n",
    "        last_xval_index = last_indexes[i+1]\n",
    "\n",
    "        # set up train, xval\n",
    "        # train from beginning to last_train_index        \n",
    "        print(\"%s Training indexes 0 to %d\" % (time.strftime(\"%H:%M:%S\"), last_train_index-1))\n",
    "        X_fit = X[:last_train_index]\n",
    "        Y_fit = Y[:last_train_index]\n",
    "        \n",
    "        # xval from last_train_index to last_xval_index\n",
    "        print(\"%s Cross-validating indexes %d to %d\" % (time.strftime(\"%H:%M:%S\"), last_train_index, last_xval_index -1 ))\n",
    "        X_xval = X[last_train_index:last_xval_index]\n",
    "        Y_xval = Y[last_train_index:last_xval_index]\n",
    "\n",
    "        fit = model.fit(X_fit, Y_fit, epochs=epochs)\n",
    "        xval_score = model.evaluate(X_xval,Y_xval)        \n",
    "\n",
    "        print (\"%s Xval MSE %f\" % (time.strftime(\"%H:%M:%S\"), xval_score))\n",
    "        avg_bests.append(xval_score)\n",
    "    \n",
    "    # mean over folds\n",
    "    avg_loss = np.mean(np.array(avg_bests))\n",
    "    print (\"%s Avg Xval loss %f\" % (time.strftime(\"%H:%M:%S\"), avg_loss))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    return (avg_loss, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:28:30 Generate splits [140, 279, 418, 557, 696]\n",
      "18:28:30 Starting training\n",
      "18:28:36 Training indexes 0 to 278\n",
      "18:28:36 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 30s 106ms/step\n",
      "18:30:51 Xval MSE 0.576499\n",
      "18:30:51 Training indexes 0 to 417\n",
      "18:30:51 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "18:30:57 Xval MSE 0.588489\n",
      "18:30:57 Training indexes 0 to 556\n",
      "18:30:57 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "18:31:05 Xval MSE 0.599041\n",
      "18:31:05 Avg Xval loss 0.588010\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5880095929884129, <__main__.KerasBacktestModel at 0x7fd3a369bb38>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "walkforward_xval_keras(X, Y_class, \n",
    "                       create_keras_model(n_hidden_layers=3,\n",
    "                                          layer_size=2,\n",
    "                                          reg_penalty=0.001,\n",
    "                                          dropout=0.25),\n",
    "                      epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment with walk-forward cross-validation\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "def run_experiment(X, Y, \n",
    "                   n_hidden_layers, \n",
    "                   layer_size, \n",
    "                   reg_penalty,\n",
    "                   dropout,\n",
    "                   minmaxscale=False, \n",
    "                   standardscale=False,\n",
    "                   epochs=EPOCHS):\n",
    "    \n",
    "    Xscale = X.copy()\n",
    "    Yscale = Y.copy()\n",
    "    \n",
    "    if minmaxscale:\n",
    "        # minmaxscale each row (min->0, max->1) - transpose, scale, transpose back because scales by columns\n",
    "        Xscale = MinMaxScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        Yscale = MinMaxScaler().fit_transform(Yscale.transpose()).transpose()\n",
    "        print(\"using MinMaxScaler\")\n",
    "    elif standardscale:\n",
    "        # standardize each row (mean->0, SD->1)- transpose, scale, transpose back because scales by columns\n",
    "        Xscale = StandardScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        Yscale = StandardScaler().fit_transform(Yscale.transpose()).transpose()\n",
    "        print(\"using StandardScaler\")\n",
    "\n",
    "    return walkforward_xval_keras(Xscale, Yscale,\n",
    "                                  create_keras_model(n_hidden_layers=n_hidden_layers,\n",
    "                                                     layer_size=layer_size,\n",
    "                                                     reg_penalty=reg_penalty,\n",
    "                                                     dropout=dropout),\n",
    "                                  epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:31:05 Generate splits [140, 279, 418, 557, 696]\n",
      "18:31:05 Starting training\n",
      "18:31:10 Training indexes 0 to 278\n",
      "18:31:10 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 34s 121ms/step\n",
      "18:33:57 Xval MSE 0.594724\n",
      "18:33:57 Training indexes 0 to 417\n",
      "18:33:57 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "18:34:38 Xval MSE 0.598321\n",
      "18:34:38 Training indexes 0 to 556\n",
      "18:34:38 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "18:35:27 Xval MSE 0.601199\n",
      "18:35:27 Avg Xval loss 0.598082\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5980815354153979, <__main__.KerasBacktestModel at 0x7fd36de630b8>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(X, Y_class, 3, 4, .001, .25, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:35:27 Running 27 experiments\n",
      "18:35:27 Running experiment 1 of 27\n",
      "18:35:27 Generate splits [140, 279, 418, 557, 696]\n",
      "18:35:27 Starting training\n",
      "18:35:32 Training indexes 0 to 278\n",
      "18:35:32 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 34s 120ms/step\n",
      "18:40:08 Xval MSE 0.599520\n",
      "18:40:08 Training indexes 0 to 417\n",
      "18:40:08 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "18:43:29 Xval MSE 0.580576\n",
      "18:43:29 Training indexes 0 to 556\n",
      "18:43:29 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "18:47:44 Xval MSE 0.600000\n",
      "18:47:44 Avg Xval loss 0.593365\n",
      "--------------------------------------------------------------------------------\n",
      "18:47:44 Saving FFNN_0.593365_1_2_0.000000_0.250.h5\n",
      "18:50:56 Running experiment 2 of 27\n",
      "18:50:56 Generate splits [140, 279, 418, 557, 696]\n",
      "18:50:56 Starting training\n",
      "18:51:01 Training indexes 0 to 278\n",
      "18:51:01 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 37s 132ms/step\n",
      "18:55:37 Xval MSE 0.599760\n",
      "18:55:37 Training indexes 0 to 417\n",
      "18:55:37 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "18:58:49 Xval MSE 0.582494\n",
      "18:58:49 Training indexes 0 to 556\n",
      "18:58:49 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "19:02:57 Xval MSE 0.590168\n",
      "19:02:57 Avg Xval loss 0.590807\n",
      "--------------------------------------------------------------------------------\n",
      "19:02:57 Saving FFNN_0.590807_1_2_0.001000_0.250.h5\n",
      "19:06:25 Running experiment 3 of 27\n",
      "19:06:25 Generate splits [140, 279, 418, 557, 696]\n",
      "19:06:25 Starting training\n",
      "19:06:29 Training indexes 0 to 278\n",
      "19:06:29 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 35s 127ms/step\n",
      "19:11:08 Xval MSE 0.593046\n",
      "19:11:08 Training indexes 0 to 417\n",
      "19:11:08 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "19:14:30 Xval MSE 0.593046\n",
      "19:14:30 Training indexes 0 to 556\n",
      "19:14:30 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "19:18:48 Xval MSE 0.599760\n",
      "19:18:48 Avg Xval loss 0.595284\n",
      "--------------------------------------------------------------------------------\n",
      "19:18:48 Saving FFNN_0.595284_1_2_0.100000_0.250.h5\n",
      "19:22:19 Running experiment 4 of 27\n",
      "19:22:19 Generate splits [140, 279, 418, 557, 696]\n",
      "19:22:19 Starting training\n",
      "19:22:34 Training indexes 0 to 278\n",
      "19:22:34 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 35s 126ms/step\n",
      "19:27:12 Xval MSE 0.594005\n",
      "19:27:12 Training indexes 0 to 417\n",
      "19:27:12 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "19:30:37 Xval MSE 0.551559\n",
      "19:30:37 Training indexes 0 to 556\n",
      "19:30:37 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "19:34:54 Xval MSE 0.583693\n",
      "19:34:54 Avg Xval loss 0.576419\n",
      "--------------------------------------------------------------------------------\n",
      "19:34:54 Saving FFNN_0.576419_1_4_0.000000_0.250.h5\n",
      "19:38:33 Running experiment 5 of 27\n",
      "19:38:33 Generate splits [140, 279, 418, 557, 696]\n",
      "19:38:33 Starting training\n",
      "19:38:38 Training indexes 0 to 278\n",
      "19:38:38 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 39s 139ms/step\n",
      "19:43:25 Xval MSE 0.588969\n",
      "19:43:25 Training indexes 0 to 417\n",
      "19:43:25 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "19:46:47 Xval MSE 0.574580\n",
      "19:46:47 Training indexes 0 to 556\n",
      "19:46:47 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "19:51:13 Xval MSE 0.588969\n",
      "19:51:13 Avg Xval loss 0.584173\n",
      "--------------------------------------------------------------------------------\n",
      "19:51:13 Saving FFNN_0.584173_1_4_0.001000_0.250.h5\n",
      "19:55:04 Running experiment 6 of 27\n",
      "19:55:04 Generate splits [140, 279, 418, 557, 696]\n",
      "19:55:04 Starting training\n",
      "19:55:09 Training indexes 0 to 278\n",
      "19:55:09 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 39s 139ms/step\n",
      "20:00:02 Xval MSE 0.596643\n",
      "20:00:02 Training indexes 0 to 417\n",
      "20:00:02 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "20:03:23 Xval MSE 0.591127\n",
      "20:03:23 Training indexes 0 to 556\n",
      "20:03:23 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "20:07:58 Xval MSE 0.604556\n",
      "20:07:58 Avg Xval loss 0.597442\n",
      "--------------------------------------------------------------------------------\n",
      "20:07:58 Saving FFNN_0.597442_1_4_0.100000_0.250.h5\n",
      "20:11:56 Running experiment 7 of 27\n",
      "20:11:56 Generate splits [140, 279, 418, 557, 696]\n",
      "20:11:56 Starting training\n",
      "20:12:00 Training indexes 0 to 278\n",
      "20:12:00 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 41s 148ms/step\n",
      "20:16:52 Xval MSE 0.583213\n",
      "20:16:52 Training indexes 0 to 417\n",
      "20:16:52 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "20:20:15 Xval MSE 0.553717\n",
      "20:20:15 Training indexes 0 to 556\n",
      "20:20:15 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "20:24:33 Xval MSE 0.581775\n",
      "20:24:33 Avg Xval loss 0.572902\n",
      "--------------------------------------------------------------------------------\n",
      "20:24:33 Saving FFNN_0.572902_1_8_0.000000_0.250.h5\n",
      "20:28:33 Running experiment 8 of 27\n",
      "20:28:33 Generate splits [140, 279, 418, 557, 696]\n",
      "20:28:33 Starting training\n",
      "20:28:38 Training indexes 0 to 278\n",
      "20:28:38 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 37s 133ms/step\n",
      "20:33:36 Xval MSE 0.576739\n",
      "20:33:36 Training indexes 0 to 417\n",
      "20:33:36 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "20:37:01 Xval MSE 0.548441\n",
      "20:37:01 Training indexes 0 to 556\n",
      "20:37:01 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "20:41:21 Xval MSE 0.574580\n",
      "20:41:21 Avg Xval loss 0.566587\n",
      "--------------------------------------------------------------------------------\n",
      "20:41:21 Saving FFNN_0.566587_1_8_0.001000_0.250.h5\n",
      "20:45:26 Running experiment 9 of 27\n",
      "20:45:26 Generate splits [140, 279, 418, 557, 696]\n",
      "20:45:26 Starting training\n",
      "20:45:30 Training indexes 0 to 278\n",
      "20:45:30 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 44s 157ms/step\n",
      "20:50:37 Xval MSE 0.592806\n",
      "20:50:37 Training indexes 0 to 417\n",
      "20:50:37 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "20:54:10 Xval MSE 0.575060\n",
      "20:54:10 Training indexes 0 to 556\n",
      "20:54:10 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "20:58:36 Xval MSE 0.593285\n",
      "20:58:36 Avg Xval loss 0.587050\n",
      "--------------------------------------------------------------------------------\n",
      "20:58:36 Saving FFNN_0.587050_1_8_0.100000_0.250.h5\n",
      "21:02:44 Running experiment 10 of 27\n",
      "21:02:44 Generate splits [140, 279, 418, 557, 696]\n",
      "21:02:44 Starting training\n",
      "21:02:49 Training indexes 0 to 278\n",
      "21:02:49 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 43s 155ms/step\n",
      "21:08:04 Xval MSE 0.600000\n",
      "21:08:04 Training indexes 0 to 417\n",
      "21:08:04 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "21:11:43 Xval MSE 0.598082\n",
      "21:11:43 Training indexes 0 to 556\n",
      "21:11:43 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "21:16:25 Xval MSE 0.595683\n",
      "21:16:25 Avg Xval loss 0.597922\n",
      "--------------------------------------------------------------------------------\n",
      "21:16:25 Saving FFNN_0.597922_2_2_0.000000_0.250.h5\n",
      "21:20:51 Running experiment 11 of 27\n",
      "21:20:51 Generate splits [140, 279, 418, 557, 696]\n",
      "21:20:51 Starting training\n",
      "21:21:08 Training indexes 0 to 278\n",
      "21:21:08 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 47s 170ms/step\n",
      "21:26:24 Xval MSE 0.595683\n",
      "21:26:24 Training indexes 0 to 417\n",
      "21:26:24 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "21:30:03 Xval MSE 0.592806\n",
      "21:30:03 Training indexes 0 to 556\n",
      "21:30:03 Cross-validating indexes 557 to 695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/557 [==============================] - 1s 2ms/step\n",
      "21:34:29 Xval MSE 0.601679\n",
      "21:34:29 Avg Xval loss 0.596723\n",
      "--------------------------------------------------------------------------------\n",
      "21:34:29 Saving FFNN_0.596723_2_2_0.001000_0.250.h5\n",
      "21:39:02 Running experiment 12 of 27\n",
      "21:39:02 Generate splits [140, 279, 418, 557, 696]\n",
      "21:39:02 Starting training\n",
      "21:39:07 Training indexes 0 to 278\n",
      "21:39:07 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 46s 164ms/step\n",
      "21:44:27 Xval MSE 0.600000\n",
      "21:44:27 Training indexes 0 to 417\n",
      "21:44:27 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "21:47:54 Xval MSE 0.599760\n",
      "21:47:54 Training indexes 0 to 556\n",
      "21:47:54 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 2s 3ms/step\n",
      "21:52:25 Xval MSE 0.599520\n",
      "21:52:25 Avg Xval loss 0.599760\n",
      "--------------------------------------------------------------------------------\n",
      "21:52:25 Saving FFNN_0.599760_2_2_0.100000_0.250.h5\n",
      "21:57:10 Running experiment 13 of 27\n",
      "21:57:10 Generate splits [140, 279, 418, 557, 696]\n",
      "21:57:10 Starting training\n",
      "21:57:15 Training indexes 0 to 278\n",
      "21:57:15 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 47s 169ms/step\n",
      "22:02:46 Xval MSE 0.596163\n",
      "22:02:46 Training indexes 0 to 417\n",
      "22:02:46 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "22:06:19 Xval MSE 0.587770\n",
      "22:06:19 Training indexes 0 to 556\n",
      "22:06:19 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "22:10:53 Xval MSE 0.597602\n",
      "22:10:53 Avg Xval loss 0.593845\n",
      "--------------------------------------------------------------------------------\n",
      "22:10:53 Saving FFNN_0.593845_2_4_0.000000_0.250.h5\n",
      "22:15:51 Running experiment 14 of 27\n",
      "22:15:51 Generate splits [140, 279, 418, 557, 696]\n",
      "22:15:51 Starting training\n",
      "22:15:56 Training indexes 0 to 278\n",
      "22:15:56 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 50s 179ms/step\n",
      "22:21:30 Xval MSE 0.591127\n",
      "22:21:30 Training indexes 0 to 417\n",
      "22:21:30 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "22:25:00 Xval MSE 0.576259\n",
      "22:25:00 Training indexes 0 to 556\n",
      "22:25:00 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "22:29:44 Xval MSE 0.593285\n",
      "22:29:44 Avg Xval loss 0.586890\n",
      "--------------------------------------------------------------------------------\n",
      "22:29:44 Saving FFNN_0.586890_2_4_0.001000_0.250.h5\n",
      "22:34:49 Running experiment 15 of 27\n",
      "22:34:49 Generate splits [140, 279, 418, 557, 696]\n",
      "22:34:49 Starting training\n",
      "22:34:54 Training indexes 0 to 278\n",
      "22:34:54 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 51s 182ms/step\n",
      "22:40:35 Xval MSE 0.600000\n",
      "22:40:35 Training indexes 0 to 417\n",
      "22:40:35 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "22:44:09 Xval MSE 0.600000\n",
      "22:44:09 Training indexes 0 to 556\n",
      "22:44:09 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "22:49:04 Xval MSE 0.600000\n",
      "22:49:04 Avg Xval loss 0.600000\n",
      "--------------------------------------------------------------------------------\n",
      "22:49:04 Saving FFNN_0.600000_2_4_0.100000_0.250.h5\n",
      "22:54:08 Running experiment 16 of 27\n",
      "22:54:08 Generate splits [140, 279, 418, 557, 696]\n",
      "22:54:08 Starting training\n",
      "22:54:13 Training indexes 0 to 278\n",
      "22:54:13 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 53s 188ms/step\n",
      "22:59:59 Xval MSE 0.594484\n",
      "22:59:59 Training indexes 0 to 417\n",
      "22:59:59 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "23:03:40 Xval MSE 0.590168\n",
      "23:03:40 Training indexes 0 to 556\n",
      "23:03:40 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 3ms/step\n",
      "23:08:15 Xval MSE 0.602158\n",
      "23:08:15 Avg Xval loss 0.595604\n",
      "--------------------------------------------------------------------------------\n",
      "23:08:15 Saving FFNN_0.595604_2_8_0.000000_0.250.h5\n",
      "23:13:22 Running experiment 17 of 27\n",
      "23:13:22 Generate splits [140, 279, 418, 557, 696]\n",
      "23:13:22 Starting training\n",
      "23:13:27 Training indexes 0 to 278\n",
      "23:13:27 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 55s 198ms/step\n",
      "23:19:18 Xval MSE 0.597842\n",
      "23:19:18 Training indexes 0 to 417\n",
      "23:19:18 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "23:22:56 Xval MSE 0.593525\n",
      "23:22:56 Training indexes 0 to 556\n",
      "23:22:56 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 3ms/step\n",
      "23:27:38 Xval MSE 0.600240\n",
      "23:27:38 Avg Xval loss 0.597202\n",
      "--------------------------------------------------------------------------------\n",
      "23:27:38 Saving FFNN_0.597202_2_8_0.001000_0.250.h5\n",
      "23:32:59 Running experiment 18 of 27\n",
      "23:32:59 Generate splits [140, 279, 418, 557, 696]\n",
      "23:32:59 Starting training\n",
      "23:33:05 Training indexes 0 to 278\n",
      "23:33:05 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 54s 195ms/step\n",
      "23:38:50 Xval MSE 0.600000\n",
      "23:38:50 Training indexes 0 to 417\n",
      "23:38:50 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "23:42:26 Xval MSE 0.598561\n",
      "23:42:26 Training indexes 0 to 556\n",
      "23:42:26 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 2s 3ms/step\n",
      "23:47:08 Xval MSE 0.601439\n",
      "23:47:08 Avg Xval loss 0.600000\n",
      "--------------------------------------------------------------------------------\n",
      "23:47:08 Saving FFNN_0.600000_2_8_0.100000_0.250.h5\n",
      "23:52:31 Running experiment 19 of 27\n",
      "23:52:31 Generate splits [140, 279, 418, 557, 696]\n",
      "23:52:31 Starting training\n",
      "23:52:36 Training indexes 0 to 278\n",
      "23:52:36 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 57s 204ms/step\n",
      "23:58:51 Xval MSE 0.601199\n",
      "23:58:51 Training indexes 0 to 417\n",
      "23:58:51 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "00:02:24 Xval MSE 0.597842\n",
      "00:02:24 Training indexes 0 to 556\n",
      "00:02:24 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "00:07:13 Xval MSE 0.601918\n",
      "00:07:13 Avg Xval loss 0.600320\n",
      "--------------------------------------------------------------------------------\n",
      "00:07:13 Saving FFNN_0.600320_3_2_0.000000_0.250.h5\n",
      "00:12:58 Running experiment 20 of 27\n",
      "00:12:58 Generate splits [140, 279, 418, 557, 696]\n",
      "00:12:58 Starting training\n",
      "00:13:03 Training indexes 0 to 278\n",
      "00:13:03 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 57s 206ms/step\n",
      "00:19:12 Xval MSE 0.600000\n",
      "00:19:12 Training indexes 0 to 417\n",
      "00:19:12 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "00:22:56 Xval MSE 0.600000\n",
      "00:22:56 Training indexes 0 to 556\n",
      "00:22:56 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "00:27:45 Xval MSE 0.600000\n",
      "00:27:45 Avg Xval loss 0.600000\n",
      "--------------------------------------------------------------------------------\n",
      "00:27:45 Saving FFNN_0.600000_3_2_0.001000_0.250.h5\n",
      "00:33:33 Running experiment 21 of 27\n",
      "00:33:33 Generate splits [140, 279, 418, 557, 696]\n",
      "00:33:33 Starting training\n",
      "00:33:38 Training indexes 0 to 278\n",
      "00:33:38 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 62s 223ms/step\n",
      "00:39:45 Xval MSE 0.600000\n",
      "00:39:45 Training indexes 0 to 417\n",
      "00:39:45 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "00:43:27 Xval MSE 0.600000\n",
      "00:43:27 Training indexes 0 to 556\n",
      "00:43:27 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "00:48:14 Xval MSE 0.600000\n",
      "00:48:14 Avg Xval loss 0.600000\n",
      "--------------------------------------------------------------------------------\n",
      "00:48:14 Saving FFNN_0.600000_3_2_0.100000_0.250.h5\n",
      "00:54:14 Running experiment 22 of 27\n",
      "00:54:14 Generate splits [140, 279, 418, 557, 696]\n",
      "00:54:14 Starting training\n",
      "00:54:19 Training indexes 0 to 278\n",
      "00:54:19 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 65s 234ms/step\n",
      "01:00:45 Xval MSE 0.600240\n",
      "01:00:45 Training indexes 0 to 417\n",
      "01:00:45 Cross-validating indexes 418 to 556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418/418 [==============================] - 1s 2ms/step\n",
      "01:04:46 Xval MSE 0.596163\n",
      "01:04:46 Training indexes 0 to 556\n",
      "01:04:46 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 3ms/step\n",
      "01:10:01 Xval MSE 0.603837\n",
      "01:10:01 Avg Xval loss 0.600080\n",
      "--------------------------------------------------------------------------------\n",
      "01:10:01 Saving FFNN_0.600080_3_4_0.000000_0.250.h5\n",
      "01:16:17 Running experiment 23 of 27\n",
      "01:16:17 Generate splits [140, 279, 418, 557, 696]\n",
      "01:16:17 Starting training\n",
      "01:16:22 Training indexes 0 to 278\n",
      "01:16:22 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 63s 224ms/step\n",
      "01:22:50 Xval MSE 0.598801\n",
      "01:22:50 Training indexes 0 to 417\n",
      "01:22:50 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "01:27:00 Xval MSE 0.596163\n",
      "01:27:00 Training indexes 0 to 556\n",
      "01:27:00 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 2s 3ms/step\n",
      "01:32:24 Xval MSE 0.602398\n",
      "01:32:24 Avg Xval loss 0.599121\n",
      "--------------------------------------------------------------------------------\n",
      "01:32:24 Saving FFNN_0.599121_3_4_0.001000_0.250.h5\n",
      "01:38:46 Running experiment 24 of 27\n",
      "01:38:46 Generate splits [140, 279, 418, 557, 696]\n",
      "01:38:46 Starting training\n",
      "01:38:51 Training indexes 0 to 278\n",
      "01:38:51 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 64s 230ms/step\n",
      "01:45:27 Xval MSE 0.600000\n",
      "01:45:27 Training indexes 0 to 417\n",
      "01:45:27 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "01:49:28 Xval MSE 0.600000\n",
      "01:49:28 Training indexes 0 to 556\n",
      "01:49:28 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "01:54:30 Xval MSE 0.600000\n",
      "01:54:30 Avg Xval loss 0.600000\n",
      "--------------------------------------------------------------------------------\n",
      "01:54:30 Saving FFNN_0.600000_3_4_0.100000_0.250.h5\n",
      "02:00:56 Running experiment 25 of 27\n",
      "02:00:56 Generate splits [140, 279, 418, 557, 696]\n",
      "02:00:56 Starting training\n",
      "02:01:01 Training indexes 0 to 278\n",
      "02:01:01 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 67s 240ms/step\n",
      "02:07:36 Xval MSE 0.598801\n",
      "02:07:36 Training indexes 0 to 417\n",
      "02:07:36 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "02:11:57 Xval MSE 0.584412\n",
      "02:11:57 Training indexes 0 to 556\n",
      "02:11:57 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 2s 3ms/step\n",
      "02:17:23 Xval MSE 0.599520\n",
      "02:17:23 Avg Xval loss 0.594245\n",
      "--------------------------------------------------------------------------------\n",
      "02:17:23 Saving FFNN_0.594245_3_8_0.000000_0.250.h5\n",
      "02:23:58 Running experiment 26 of 27\n",
      "02:23:58 Generate splits [140, 279, 418, 557, 696]\n",
      "02:23:58 Starting training\n",
      "02:24:03 Training indexes 0 to 278\n",
      "02:24:03 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 68s 244ms/step\n",
      "02:31:09 Xval MSE 0.597362\n",
      "02:31:09 Training indexes 0 to 417\n",
      "02:31:09 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "02:35:30 Xval MSE 0.594005\n",
      "02:35:30 Training indexes 0 to 556\n",
      "02:35:30 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 2s 3ms/step\n",
      "02:40:58 Xval MSE 0.608873\n",
      "02:40:58 Avg Xval loss 0.600080\n",
      "--------------------------------------------------------------------------------\n",
      "02:40:58 Saving FFNN_0.600080_3_8_0.001000_0.250.h5\n",
      "02:47:34 Running experiment 27 of 27\n",
      "02:47:34 Generate splits [140, 279, 418, 557, 696]\n",
      "02:47:34 Starting training\n",
      "02:47:39 Training indexes 0 to 278\n",
      "02:47:39 Cross-validating indexes 279 to 417\n",
      "279/279 [==============================] - 67s 242ms/step\n",
      "02:54:25 Xval MSE 0.600000\n",
      "02:54:25 Training indexes 0 to 417\n",
      "02:54:25 Cross-validating indexes 418 to 556\n",
      "418/418 [==============================] - 1s 3ms/step\n",
      "02:58:44 Xval MSE 0.600000\n",
      "02:58:44 Training indexes 0 to 556\n",
      "02:58:44 Cross-validating indexes 557 to 695\n",
      "557/557 [==============================] - 2s 3ms/step\n",
      "03:04:01 Xval MSE 0.600000\n",
      "03:04:01 Avg Xval loss 0.600000\n",
      "--------------------------------------------------------------------------------\n",
      "03:04:01 Saving FFNN_0.600000_3_8_0.100000_0.250.h5\n"
     ]
    }
   ],
   "source": [
    "# run a lot of experiments in big xval loop to pick best hyperparameters\n",
    "\n",
    "MODELPREFIX = \"FFNN\"\n",
    "EPOCHS=200\n",
    "\n",
    "n_hiddens = [1, 2, 3]\n",
    "layer_sizes = [2, 4, 8]\n",
    "reg_penalties = [0.0, 0.001, 0.1]\n",
    "dropouts = [0.25]\n",
    "\n",
    "hyperparameter_combos = list(product(n_hiddens, layer_sizes, reg_penalties, dropouts))\n",
    "\n",
    "print(\"%s Running %d experiments\" % (time.strftime(\"%H:%M:%S\"), len(hyperparameter_combos)))\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "for counter, param_list in enumerate(hyperparameter_combos):\n",
    "    n_hidden_layers, layer_size, reg_penalty, dropout = param_list\n",
    "    print(\"%s Running experiment %d of %d\" % (time.strftime(\"%H:%M:%S\"), counter+1, len(hyperparameter_combos)))\n",
    "    key = (n_hidden_layers, layer_size, reg_penalty, dropout)\n",
    "    print(\"%s n_hidden_layers %d, layer_size %d, reg_penalty %.6f, dropout %.3f\" % (time.strftime(\"%H:%M:%S\"),  n_hidden_layers, layer_size, reg_penalty, dropout))\n",
    "    score, model = run_experiment(X, Y_class,\n",
    "                                  n_hidden_layers = n_hidden_layers,\n",
    "                                  layer_size = layer_size,\n",
    "                                  reg_penalty = reg_penalty,\n",
    "                                  dropout = dropout,\n",
    "                                  epochs=EPOCHS\n",
    "                                 )\n",
    "    experiments[key] = score \n",
    "    modelname = \"%s_%.6f_%d_%d_%.6f_%.3f\" % (MODELPREFIX, score, n_hidden_layers, layer_size, reg_penalty, dropout)\n",
    "    print(\"%s Saving %s.h5\" % (time.strftime(\"%H:%M:%S\"), modelname))\n",
    "    model.save(modelname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_hidden_layers</th>\n",
       "      <th>layer_size</th>\n",
       "      <th>reg_penalty</th>\n",
       "      <th>dropout</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.599760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.599121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.597922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.597442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.597202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.596723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.595604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.595284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.594245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.593845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.593365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.590807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.587050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.586890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.584173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.576419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.572902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.566587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_hidden_layers  layer_size  reg_penalty  dropout  accuracy\n",
       "18                3           2        0.000     0.25  0.600320\n",
       "21                3           4        0.000     0.25  0.600080\n",
       "25                3           8        0.001     0.25  0.600080\n",
       "14                2           4        0.100     0.25  0.600000\n",
       "20                3           2        0.100     0.25  0.600000\n",
       "19                3           2        0.001     0.25  0.600000\n",
       "26                3           8        0.100     0.25  0.600000\n",
       "23                3           4        0.100     0.25  0.600000\n",
       "17                2           8        0.100     0.25  0.600000\n",
       "11                2           2        0.100     0.25  0.599760\n",
       "22                3           4        0.001     0.25  0.599121\n",
       "9                 2           2        0.000     0.25  0.597922\n",
       "5                 1           4        0.100     0.25  0.597442\n",
       "16                2           8        0.001     0.25  0.597202\n",
       "10                2           2        0.001     0.25  0.596723\n",
       "15                2           8        0.000     0.25  0.595604\n",
       "2                 1           2        0.100     0.25  0.595284\n",
       "24                3           8        0.000     0.25  0.594245\n",
       "12                2           4        0.000     0.25  0.593845\n",
       "0                 1           2        0.000     0.25  0.593365\n",
       "1                 1           2        0.001     0.25  0.590807\n",
       "8                 1           8        0.100     0.25  0.587050\n",
       "13                2           4        0.001     0.25  0.586890\n",
       "4                 1           4        0.001     0.25  0.584173\n",
       "3                 1           4        0.000     0.25  0.576419\n",
       "6                 1           8        0.000     0.25  0.572902\n",
       "7                 1           8        0.001     0.25  0.566587"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list and chart experiments\n",
    "flatlist = [list(l[0]) + [l[1]] for l in experiments.items()]\n",
    "\n",
    "lossframe = pd.DataFrame(flatlist, columns=[\"n_hidden_layers\", \"layer_size\", \"reg_penalty\", \"dropout\",\n",
    "                                            \"accuracy\"])\n",
    "lossframe.sort_values(['accuracy'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_hidden_layers</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.584892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.596438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 accuracy\n",
       "n_hidden_layers          \n",
       "1                0.584892\n",
       "2                0.596438\n",
       "3                0.599316"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can pick lowest loss , but first we look at patterns by hyperparameter\n",
    "pd.DataFrame(lossframe.groupby(['n_hidden_layers'])['accuracy'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_size</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.597131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.593108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.590408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy\n",
       "layer_size          \n",
       "2           0.597131\n",
       "4           0.593108\n",
       "8           0.590408"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lossframe.groupby(['layer_size'])['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_penalty</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000</th>\n",
       "      <td>0.591633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.591287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.597726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy\n",
       "reg_penalty          \n",
       "0.000        0.591633\n",
       "0.001        0.591287\n",
       "0.100        0.597726"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lossframe.groupby(['reg_penalty'])['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "1  layers",
          "2  layers",
          "3  layers"
         ],
         "y": [
          "2  units",
          "4  units",
          "8  units"
         ],
         "z": [
          [
           0.5931521456161563,
           0.5981348261082297,
           0.6001065820304673
          ],
          [
           0.58601119167529,
           0.5935784711969271,
           0.5997335471242642
          ],
          [
           0.5755129236595934,
           0.5976019191092374,
           0.5981081807451377
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "n_hidden_layers v. layer_size",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "n_hidden_layers"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "layer_size"
        }
       }
      },
      "text/html": [
       "<div id=\"fd1c5b57-c6f0-4b66-8709-f87f1a8553a9\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fd1c5b57-c6f0-4b66-8709-f87f1a8553a9\", [{\"type\": \"heatmap\", \"z\": [[0.5931521456161563, 0.5981348261082297, 0.6001065820304673], [0.58601119167529, 0.5935784711969271, 0.5997335471242642], [0.5755129236595934, 0.5976019191092374, 0.5981081807451377]], \"x\": [\"1  layers\", \"2  layers\", \"3  layers\"], \"y\": [\"2  units\", \"4  units\", \"8  units\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. layer_size\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"n_hidden_layers\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"layer_size\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"fd1c5b57-c6f0-4b66-8709-f87f1a8553a9\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fd1c5b57-c6f0-4b66-8709-f87f1a8553a9\", [{\"type\": \"heatmap\", \"z\": [[0.5931521456161563, 0.5981348261082297, 0.6001065820304673], [0.58601119167529, 0.5935784711969271, 0.5997335471242642], [0.5755129236595934, 0.5976019191092374, 0.5981081807451377]], \"x\": [\"1  layers\", \"2  layers\", \"3  layers\"], \"y\": [\"2  units\", \"4  units\", \"8  units\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. layer_size\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"n_hidden_layers\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"layer_size\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_matrix(lossframe, x_labels, y_labels, x_suffix=\"\", y_suffix=\"\"):\n",
    "\n",
    "    pivot = lossframe.pivot_table(index=[y_labels], columns=[x_labels], values=['accuracy'])\n",
    "#    print(pivot)\n",
    "    # specify labels as strings, to force plotly to use a discrete axis\n",
    "#    print(pivot.columns.levels[1]).values\n",
    "#    print(lossframe[x_labels].dtype)\n",
    "    \n",
    "    if lossframe[x_labels].dtype == np.float64 or lossframe[x_labels].dtype == np.float32:\n",
    "        xaxis = [\"%f %s\" % (i, x_suffix) for i in pivot.columns.levels[1].values]\n",
    "    else:\n",
    "        xaxis = [\"%d %s\" % (i, x_suffix) for i in pivot.columns.levels[1].values]\n",
    "    if lossframe[y_labels].dtype == np.float64 or lossframe[y_labels].dtype == np.float32:\n",
    "        yaxis = [\"%f %s\" % (i, y_suffix) for i in pivot.index.values]\n",
    "    else:\n",
    "        yaxis = [\"%d %s\" % (i, y_suffix) for i in pivot.index.values]\n",
    "        \n",
    "#    print(xaxis, yaxis)\n",
    "    \"\"\"plot a heat map of a matrix\"\"\"\n",
    "    chart_width=640\n",
    "    chart_height=480\n",
    "    \n",
    "    layout = Layout(\n",
    "        title=\"%s v. %s\" % (x_labels, y_labels),\n",
    "        height=chart_height,\n",
    "        width=chart_width,     \n",
    "        margin=dict(\n",
    "            l=150,\n",
    "            r=30,\n",
    "            b=120,\n",
    "            t=100,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=x_labels,\n",
    "            tickfont=dict(\n",
    "                family='Arial, sans-serif',\n",
    "                size=10,\n",
    "                color='black'\n",
    "            ),\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=y_labels,\n",
    "            tickfont=dict(\n",
    "                family='Arial, sans-serif',\n",
    "                size=10,\n",
    "                color='black'\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [Heatmap(z=pivot.values,\n",
    "                    x=xaxis,\n",
    "                    y=yaxis,\n",
    "                    colorscale=[[0, 'rgb(0,0,255)', [1, 'rgb(255,0,0)']]],\n",
    "                   )\n",
    "           ]\n",
    "\n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    return iplot(fig, link_text=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "1  layers",
          "2  layers",
          "3  layers"
         ],
         "y": [
          "2  units",
          "4  units",
          "8  units"
         ],
         "z": [
          [
           0.5931521456161563,
           0.5981348261082297,
           0.6001065820304673
          ],
          [
           0.58601119167529,
           0.5935784711969271,
           0.5997335471242642
          ],
          [
           0.5755129236595934,
           0.5976019191092374,
           0.5981081807451377
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "n_hidden_layers v. layer_size",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "n_hidden_layers"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "layer_size"
        }
       }
      },
      "text/html": [
       "<div id=\"0cfd8c66-2b8e-4991-9ab6-39496e181f66\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0cfd8c66-2b8e-4991-9ab6-39496e181f66\", [{\"type\": \"heatmap\", \"z\": [[0.5931521456161563, 0.5981348261082297, 0.6001065820304673], [0.58601119167529, 0.5935784711969271, 0.5997335471242642], [0.5755129236595934, 0.5976019191092374, 0.5981081807451377]], \"x\": [\"1  layers\", \"2  layers\", \"3  layers\"], \"y\": [\"2  units\", \"4  units\", \"8  units\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. layer_size\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"n_hidden_layers\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"layer_size\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"0cfd8c66-2b8e-4991-9ab6-39496e181f66\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0cfd8c66-2b8e-4991-9ab6-39496e181f66\", [{\"type\": \"heatmap\", \"z\": [[0.5931521456161563, 0.5981348261082297, 0.6001065820304673], [0.58601119167529, 0.5935784711969271, 0.5997335471242642], [0.5755129236595934, 0.5976019191092374, 0.5981081807451377]], \"x\": [\"1  layers\", \"2  layers\", \"3  layers\"], \"y\": [\"2  units\", \"4  units\", \"8  units\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. layer_size\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"n_hidden_layers\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"layer_size\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(lossframe, \"n_hidden_layers\", \"layer_size\", x_suffix=\" layers\", y_suffix=\" units\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "1 p",
          "2 p",
          "3 p"
         ],
         "y": [
          "0.000000  layers",
          "0.001000  layers",
          "0.100000  layers"
         ],
         "z": [
          [
           0.5808952844686073,
           0.595790035282171,
           0.5982147621498596
          ],
          [
           0.5805222494734658,
           0.5936051165560484,
           0.5997335471210877
          ],
          [
           0.5932587270089665,
           0.5999200645761746,
           0.6000000006289219
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "n_hidden_layers v. reg_penalty",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "n_hidden_layers"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "reg_penalty"
        }
       }
      },
      "text/html": [
       "<div id=\"d1c4b100-f0f8-49e8-92bf-bc0b0c32e3f9\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d1c4b100-f0f8-49e8-92bf-bc0b0c32e3f9\", [{\"type\": \"heatmap\", \"z\": [[0.5808952844686073, 0.595790035282171, 0.5982147621498596], [0.5805222494734658, 0.5936051165560484, 0.5997335471210877], [0.5932587270089665, 0.5999200645761746, 0.6000000006289219]], \"x\": [\"1 p\", \"2 p\", \"3 p\"], \"y\": [\"0.000000  layers\", \"0.001000  layers\", \"0.100000  layers\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. reg_penalty\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"n_hidden_layers\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"reg_penalty\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d1c4b100-f0f8-49e8-92bf-bc0b0c32e3f9\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d1c4b100-f0f8-49e8-92bf-bc0b0c32e3f9\", [{\"type\": \"heatmap\", \"z\": [[0.5808952844686073, 0.595790035282171, 0.5982147621498596], [0.5805222494734658, 0.5936051165560484, 0.5997335471210877], [0.5932587270089665, 0.5999200645761746, 0.6000000006289219]], \"x\": [\"1 p\", \"2 p\", \"3 p\"], \"y\": [\"0.000000  layers\", \"0.001000  layers\", \"0.100000  layers\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. reg_penalty\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"n_hidden_layers\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"reg_penalty\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(lossframe, \"n_hidden_layers\", \"reg_penalty\", x_suffix=\"p\", y_suffix=\" layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "0.000000  p",
          "0.001000  p",
          "0.100000  p"
         ],
         "y": [
          "2  units",
          "4  units",
          "8  units"
         ],
         "z": [
          [
           0.5972022388653535,
           0.5958433259797674,
           0.5983479889097323
          ],
          [
           0.5901145756363774,
           0.5900612849363988,
           0.5991473494237053
          ],
          [
           0.5875832673989071,
           0.587956302234436,
           0.5956834538806256
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "reg_penalty v. layer_size",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "reg_penalty"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "layer_size"
        }
       }
      },
      "text/html": [
       "<div id=\"51bf1ec4-fc13-48da-973a-50f2db1ab659\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"51bf1ec4-fc13-48da-973a-50f2db1ab659\", [{\"type\": \"heatmap\", \"z\": [[0.5972022388653535, 0.5958433259797674, 0.5983479889097323], [0.5901145756363774, 0.5900612849363988, 0.5991473494237053], [0.5875832673989071, 0.587956302234436, 0.5956834538806256]], \"x\": [\"0.000000  p\", \"0.001000  p\", \"0.100000  p\"], \"y\": [\"2  units\", \"4  units\", \"8  units\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"reg_penalty v. layer_size\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"reg_penalty\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"layer_size\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"51bf1ec4-fc13-48da-973a-50f2db1ab659\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"51bf1ec4-fc13-48da-973a-50f2db1ab659\", [{\"type\": \"heatmap\", \"z\": [[0.5972022388653535, 0.5958433259797674, 0.5983479889097323], [0.5901145756363774, 0.5900612849363988, 0.5991473494237053], [0.5875832673989071, 0.587956302234436, 0.5956834538806256]], \"x\": [\"0.000000  p\", \"0.001000  p\", \"0.100000  p\"], \"y\": [\"2  units\", \"4  units\", \"8  units\"], \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"reg_penalty v. layer_size\", \"height\": 480, \"width\": 640, \"margin\": {\"l\": 150, \"r\": 30, \"b\": 120, \"t\": 100}, \"xaxis\": {\"title\": \"reg_penalty\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}, \"yaxis\": {\"title\": \"layer_size\", \"tickfont\": {\"family\": \"Arial, sans-serif\", \"size\": 10, \"color\": \"black\"}}}, {\"showLink\": true, \"linkText\": \"\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(lossframe, \"reg_penalty\", \"layer_size\", x_suffix=\" p\", y_suffix=\" units\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=500\n",
    "\n",
    "nrows = X.shape[0]\n",
    "START=121\n",
    "\n",
    "def run_backtest_keras(X, Y_class, arg_dict, startindex=0, epochs=EPOCHS, step=1, minmaxscale=False, standardscale=False):\n",
    "    \"\"\"create keras model; add step, to iteratively train, predict 12 months, train up to next 12 months \"\"\"\n",
    "    global P_L, P_S\n",
    "    P_L = np.zeros((Y_class.shape[0],OUTPUT_DIM))\n",
    "    P_S = np.zeros((Y_class.shape[0],OUTPUT_DIM))\n",
    "    \n",
    "    print(\"%s Starting backtest\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    \n",
    "    count = 0\n",
    "    nrows = X.shape[0]\n",
    "\n",
    "    Xscale = X.copy()\n",
    "    \n",
    "    if minmaxscale:\n",
    "        # minmaxscale each row (min->0, max->1) - transpose, scale, transpose back because scales by columns\n",
    "        Xscale = MinMaxScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        print(\"using MinMaxScaler\")\n",
    "    elif standardscale:\n",
    "        # standardize each row (mean->0, SD->1)- transpose, scale, transpose back because scales by columns\n",
    "        Xscale = StandardScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        print(\"using StandardScaler\")\n",
    "     \n",
    "    model = create_keras_model(n_hidden_layers=arg_dict[\"n_hidden_layers\"],\n",
    "                               layer_size=arg_dict[\"hidden_layer_size\"],\n",
    "                               reg_penalty=arg_dict[\"reg_penalty\"],\n",
    "                               dropout=dropout,\n",
    "                               verbose=False)()\n",
    "        \n",
    "    for train_index in range(startindex, nrows, step):\n",
    "        if train_index + step >= nrows:\n",
    "            train_index = nrows-step\n",
    "            \n",
    "        fp_index = train_index + step # eg 1000 + 26 = 1026\n",
    "\n",
    "        # fit on e.g. 0:999, predict 1000-1025\n",
    "\n",
    "        longprobs, shortprobs, flatprobs = fit_predict_keras(Xscale[:fp_index], \n",
    "                                                             Y_class[:fp_index], \n",
    "                                                             model,\n",
    "                                                             epochs=epochs,\n",
    "                                                             npredict=step)\n",
    "        # store in 1000:1025 - lining up with future Xs\n",
    "        for i in range(step):\n",
    "            P_L[train_index + i] = longprobs[i]\n",
    "            P_S[train_index + i] = shortprobs[i]\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training %d of %d\" % (time.strftime(\"%H:%M:%S\"), count, nrows-startindex))\n",
    "            sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_returns(Y, P_L, P_S, first_pred_month, start_date='01/01/1970', freq='M', verbose=False):\n",
    "    #TODO: more general version\n",
    "    #take an indicator (P), a function to generate portfolio based on indicator, universe returns (Y), return portfolio returns\n",
    "\n",
    "    global R\n",
    "    R = np.zeros(P_L.shape[0])\n",
    "    nrows, ncols = P_L.shape\n",
    "    numstocks = 6 # top quintile (and bottom)\n",
    "\n",
    "    indcount = [0 for response in responses]\n",
    "    longcount = [0 for response in responses]\n",
    "    shortcount = [0 for response in responses]\n",
    "        \n",
    "    for month_index in range(first_pred_month, nrows):\n",
    "        # get indexes of sorted smallest to largest\n",
    "        # rightmost 6 most probable P_Ls for longs, P_Ss for shorts\n",
    "        # ignore nan\n",
    "        short_sort_array = [-999999 if np.isnan(x) else x for x in P_S[month_index]]\n",
    "        select_array = np.argsort(short_sort_array)\n",
    "        short_indexes = select_array[-numstocks:]\n",
    "        # rightmost 6 longs\n",
    "        long_sort_array = [-999999 if np.isnan(x) else x for x in P_L[month_index]]\n",
    "        select_array = np.argsort(long_sort_array)\n",
    "        long_indexes = select_array[-numstocks:]\n",
    "        # compute equal weighted long/short return\n",
    "        return_month = month_index + 1\n",
    "        if verbose:\n",
    "            print(\"Longs for month %d: %s\" %(return_month, str([(l,P_L[month_index, l]) for l in long_indexes])))\n",
    "            print(\"Shorts for month %d: %s\" %(return_month, str([(l,P_S[month_index, l]) for l in short_indexes])))\n",
    "            \n",
    "        if return_month < nrows: # last row has a prediction for following month but no following month\n",
    "            R[return_month] = np.mean(X[return_month, long_indexes])/2 - np.mean(X[return_month, short_indexes])/2\n",
    "            # count occurrences of each industry\n",
    "            for i in short_indexes:\n",
    "                indcount[i]+=1\n",
    "                shortcount[i]+=1\n",
    "            for i in long_indexes:\n",
    "                indcount[i]+=1\n",
    "                longcount[i]+=1\n",
    "\n",
    "    for response in responses:\n",
    "        i = response_reverse_dict[response]\n",
    "        print(\"%s: long %d times, short %d times, total %d times\" % (response, longcount[i], shortcount[i], indcount[i]))\n",
    "        \n",
    "    results = R[first_pred_month:]\n",
    "\n",
    "    index = pd.date_range(start_date,periods=results.shape[0], freq=freq)\n",
    "    perfdata = pd.DataFrame(results,index=index,columns=['Returns'])\n",
    "    perfdata['Equity'] = 100 * np.cumprod(1 + results / 100)\n",
    "\n",
    "    stats = perfdata['Equity'].calc_stats()\n",
    "\n",
    "    retframe = pd.DataFrame([stats.stats.loc['start'],\n",
    "                             stats.stats.loc['end'],\n",
    "                             stats.stats.loc['cagr'],\n",
    "                             stats.stats.loc['yearly_vol'],\n",
    "                             stats.stats.loc['yearly_sharpe'],\n",
    "                             stats.stats.loc['max_drawdown'],\n",
    "                             ffn.core.calc_sortino_ratio(perfdata.Returns, rf=0, nperiods=564, annualize=False),\n",
    "                            ],\n",
    "                            index = ['start',\n",
    "                                     'end',\n",
    "                                     'cagr',\n",
    "                                     'yearly_vol',\n",
    "                                     'yearly_sharpe',\n",
    "                                     'max_drawdown',\n",
    "                                     'sortino',\n",
    "                                    ],\n",
    "                            columns=['Value'])   \n",
    "    return retframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:18:10 Starting backtest\n",
      "(121, 103)\n",
      "(121, 30)\n",
      "121/121 [==============================] - 68s 562ms/step\n",
      "5/5 [==============================] - 0s 24ms/step\n",
      ".....(126, 103)\n",
      "(126, 30)\n",
      "126/126 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      ".....(131, 103)\n",
      "(131, 30)\n",
      "131/131 [==============================] - 0s 4ms/step\n",
      "5/5 [==============================] - 0s 19ms/step\n",
      ".....(136, 103)\n",
      "(136, 30)\n",
      "136/136 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....(141, 103)\n",
      "(141, 30)\n",
      "141/141 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 12ms/step\n",
      ".....(146, 103)\n",
      "(146, 30)\n",
      "146/146 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....(151, 103)\n",
      "(151, 30)\n",
      "151/151 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 13ms/step\n",
      ".....(156, 103)\n",
      "(156, 30)\n",
      "156/156 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      ".....(161, 103)\n",
      "(161, 30)\n",
      "161/161 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 20ms/step\n",
      ".....(166, 103)\n",
      "(166, 30)\n",
      "166/166 [==============================] - 1s 4ms/step\n",
      "5/5 [==============================] - 0s 21ms/step\n",
      ".....(171, 103)\n",
      "(171, 30)\n",
      "171/171 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 21ms/step\n",
      ".....(176, 103)\n",
      "(176, 30)\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 19ms/step\n",
      ".....(181, 103)\n",
      "(181, 30)\n",
      "181/181 [==============================] - 1s 4ms/step\n",
      "5/5 [==============================] - 0s 19ms/step\n",
      ".....(186, 103)\n",
      "(186, 30)\n",
      "186/186 [==============================] - 1s 4ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(191, 103)\n",
      "(191, 30)\n",
      "191/191 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 12ms/step\n",
      ".....(196, 103)\n",
      "(196, 30)\n",
      "196/196 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 13ms/step\n",
      ".....\n",
      "04:36:19 Still training 80 of 575\n",
      "(201, 103)\n",
      "(201, 30)\n",
      "201/201 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....(206, 103)\n",
      "(206, 30)\n",
      "206/206 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 11ms/step\n",
      ".....(211, 103)\n",
      "(211, 30)\n",
      "211/211 [==============================] - 1s 4ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      ".....(216, 103)\n",
      "(216, 30)\n",
      "216/216 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....(221, 103)\n",
      "(221, 30)\n",
      "221/221 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      ".....(226, 103)\n",
      "(226, 30)\n",
      "226/226 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      ".....(231, 103)\n",
      "(231, 30)\n",
      "231/231 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      ".....(236, 103)\n",
      "(236, 30)\n",
      "236/236 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(241, 103)\n",
      "(241, 30)\n",
      "241/241 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 16ms/step\n",
      ".....(246, 103)\n",
      "(246, 30)\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      ".....(251, 103)\n",
      "(251, 30)\n",
      "251/251 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 11ms/step\n",
      ".....(256, 103)\n",
      "(256, 30)\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      ".....(261, 103)\n",
      "(261, 30)\n",
      "261/261 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 16ms/step\n",
      ".....(266, 103)\n",
      "(266, 30)\n",
      "266/266 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 15ms/step\n",
      ".....(271, 103)\n",
      "(271, 30)\n",
      "271/271 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 15ms/step\n",
      ".....(276, 103)\n",
      "(276, 30)\n",
      "276/276 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....\n",
      "06:15:20 Still training 160 of 575\n",
      "(281, 103)\n",
      "(281, 30)\n",
      "281/281 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 15ms/step\n",
      ".....(286, 103)\n",
      "(286, 30)\n",
      "286/286 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 11ms/step\n",
      ".....(291, 103)\n",
      "(291, 30)\n",
      "291/291 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 15ms/step\n",
      ".....(296, 103)\n",
      "(296, 30)\n",
      "296/296 [==============================] - 1s 4ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(301, 103)\n",
      "(301, 30)\n",
      "301/301 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(306, 103)\n",
      "(306, 30)\n",
      "306/306 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 13ms/step\n",
      ".....(311, 103)\n",
      "(311, 30)\n",
      "311/311 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 12ms/step\n",
      ".....(316, 103)\n",
      "(316, 30)\n",
      "316/316 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 13ms/step\n",
      ".....(321, 103)\n",
      "(321, 30)\n",
      "321/321 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 16ms/step\n",
      ".....(326, 103)\n",
      "(326, 30)\n",
      "326/326 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 54ms/step\n",
      ".....(331, 103)\n",
      "(331, 30)\n",
      "331/331 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 13ms/step\n",
      ".....(336, 103)\n",
      "(336, 30)\n",
      "336/336 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      ".....(341, 103)\n",
      "(341, 30)\n",
      "341/341 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      ".....(346, 103)\n",
      "(346, 30)\n",
      "346/346 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....(351, 103)\n",
      "(351, 30)\n",
      "351/351 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 22ms/step\n",
      ".....(356, 103)\n",
      "(356, 30)\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....\n",
      "08:28:33 Still training 240 of 575\n",
      "(361, 103)\n",
      "(361, 30)\n",
      "361/361 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(366, 103)\n",
      "(366, 30)\n",
      "366/366 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(371, 103)\n",
      "(371, 30)\n",
      "371/371 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 19ms/step\n",
      ".....(376, 103)\n",
      "(376, 30)\n",
      "376/376 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 18ms/step\n",
      ".....(381, 103)\n",
      "(381, 30)\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 16ms/step\n",
      ".....(386, 103)\n",
      "(386, 30)\n",
      "386/386 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 63ms/step\n",
      ".....(391, 103)\n",
      "(391, 30)\n",
      "391/391 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 22ms/step\n",
      ".....(396, 103)\n",
      "(396, 30)\n",
      "396/396 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 19ms/step\n",
      ".....(401, 103)\n",
      "(401, 30)\n",
      "401/401 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(406, 103)\n",
      "(406, 30)\n",
      "406/406 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      ".....(411, 103)\n",
      "(411, 30)\n",
      "411/411 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      ".....(416, 103)\n",
      "(416, 30)\n",
      "416/416 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 57ms/step\n",
      ".....(421, 103)\n",
      "(421, 30)\n",
      "421/421 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....(426, 103)\n",
      "(426, 30)\n",
      "426/426 [==============================] - 2s 4ms/step\n",
      "5/5 [==============================] - 0s 11ms/step\n",
      ".....(431, 103)\n",
      "(431, 30)\n",
      "431/431 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(436, 103)\n",
      "(436, 30)\n",
      "436/436 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 52ms/step\n",
      ".....\n",
      "11:28:11 Still training 320 of 575\n",
      "(441, 103)\n",
      "(441, 30)\n",
      "441/441 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      ".....(446, 103)\n",
      "(446, 30)\n",
      "446/446 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 15ms/step\n",
      ".....(451, 103)\n",
      "(451, 30)\n",
      "451/451 [==============================] - 1s 3ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(456, 103)\n",
      "(456, 30)\n",
      "456/456 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(461, 103)\n",
      "(461, 30)\n",
      "461/461 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      ".....(466, 103)\n",
      "(466, 30)\n",
      "466/466 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(471, 103)\n",
      "(471, 30)\n",
      "471/471 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(476, 103)\n",
      "(476, 30)\n",
      "476/476 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      ".....(481, 103)\n",
      "(481, 30)\n",
      "481/481 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(486, 103)\n",
      "(486, 30)\n",
      "486/486 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(491, 103)\n",
      "(491, 30)\n",
      "491/491 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(496, 103)\n",
      "(496, 30)\n",
      "496/496 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      ".....(501, 103)\n",
      "(501, 30)\n",
      "501/501 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      ".....(506, 103)\n",
      "(506, 30)\n",
      "506/506 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 10ms/step\n",
      ".....(511, 103)\n",
      "(511, 30)\n",
      "511/511 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 12ms/step\n",
      ".....(516, 103)\n",
      "(516, 30)\n",
      "516/516 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 10ms/step\n",
      ".....\n",
      "13:42:23 Still training 400 of 575\n",
      "(521, 103)\n",
      "(521, 30)\n",
      "521/521 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(526, 103)\n",
      "(526, 30)\n",
      "526/526 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(531, 103)\n",
      "(531, 30)\n",
      "531/531 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(536, 103)\n",
      "(536, 30)\n",
      "536/536 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      ".....(541, 103)\n",
      "(541, 30)\n",
      "541/541 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      ".....(546, 103)\n",
      "(546, 30)\n",
      "546/546 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(551, 103)\n",
      "(551, 30)\n",
      "551/551 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(556, 103)\n",
      "(556, 30)\n",
      "556/556 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      ".....(561, 103)\n",
      "(561, 30)\n",
      "561/561 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      ".....(566, 103)\n",
      "(566, 30)\n",
      "566/566 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      ".....(571, 103)\n",
      "(571, 30)\n",
      "571/571 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(576, 103)\n",
      "(576, 30)\n",
      "576/576 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(581, 103)\n",
      "(581, 30)\n",
      "581/581 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      ".....(586, 103)\n",
      "(586, 30)\n",
      "586/586 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      ".....(591, 103)\n",
      "(591, 30)\n",
      "591/591 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 10ms/step\n",
      ".....(596, 103)\n",
      "(596, 30)\n",
      "596/596 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....\n",
      "15:52:11 Still training 480 of 575\n",
      "(601, 103)\n",
      "(601, 30)\n",
      "601/601 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      ".....(606, 103)\n",
      "(606, 30)\n",
      "606/606 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(611, 103)\n",
      "(611, 30)\n",
      "611/611 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(616, 103)\n",
      "(616, 30)\n",
      "616/616 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(621, 103)\n",
      "(621, 30)\n",
      "621/621 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 10ms/step\n",
      ".....(626, 103)\n",
      "(626, 30)\n",
      "626/626 [==============================] - 1s 842us/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      ".....(631, 103)\n",
      "(631, 30)\n",
      "631/631 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(636, 103)\n",
      "(636, 30)\n",
      "636/636 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      ".....(641, 103)\n",
      "(641, 30)\n",
      "641/641 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      ".....(646, 103)\n",
      "(646, 30)\n",
      "646/646 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      ".....(651, 103)\n",
      "(651, 30)\n",
      "651/651 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(656, 103)\n",
      "(656, 30)\n",
      "656/656 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(661, 103)\n",
      "(661, 30)\n",
      "661/661 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 10ms/step\n",
      ".....(666, 103)\n",
      "(666, 30)\n",
      "666/666 [==============================] - 1s 971us/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      ".....(671, 103)\n",
      "(671, 30)\n",
      "671/671 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(676, 103)\n",
      "(676, 30)\n",
      "676/676 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      ".....\n",
      "18:13:46 Still training 560 of 575\n",
      "(681, 103)\n",
      "(681, 30)\n",
      "681/681 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 8ms/step\n",
      ".....(686, 103)\n",
      "(686, 30)\n",
      "686/686 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      ".....(691, 103)\n",
      "(691, 30)\n",
      "691/691 [==============================] - 1s 1ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      "....."
     ]
    }
   ],
   "source": [
    "START=121\n",
    "EPOCHS=500\n",
    "STEP=5\n",
    "arg_dict = {\"n_hidden_layers\" : 3,\n",
    "            \"hidden_layer_size\" : 2,\n",
    "            \"reg_penalty\" : 0.1,\n",
    "            \"dropout\": 0.25,\n",
    "            'verbose' : False\n",
    "           }\n",
    "     \n",
    "#model = build_model(**arg_dict)\n",
    "run_backtest_keras(X, Y_class, arg_dict, startindex=START, step=STEP, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food.lead: long 0 times, short 0 times, total 0 times\n",
      "Beer.lead: long 74 times, short 0 times, total 74 times\n",
      "Smoke.lead: long 574 times, short 574 times, total 1148 times\n",
      "Games.lead: long 574 times, short 325 times, total 899 times\n",
      "Books.lead: long 0 times, short 0 times, total 0 times\n",
      "Hshld.lead: long 10 times, short 0 times, total 10 times\n",
      "Clths.lead: long 0 times, short 0 times, total 0 times\n",
      "Hlth.lead: long 0 times, short 0 times, total 0 times\n",
      "Chems.lead: long 0 times, short 0 times, total 0 times\n",
      "Txtls.lead: long 0 times, short 0 times, total 0 times\n",
      "Cnstr.lead: long 0 times, short 0 times, total 0 times\n",
      "Steel.lead: long 0 times, short 449 times, total 449 times\n",
      "FabPr.lead: long 0 times, short 0 times, total 0 times\n",
      "ElcEq.lead: long 0 times, short 0 times, total 0 times\n",
      "Autos.lead: long 0 times, short 100 times, total 100 times\n",
      "Carry.lead: long 0 times, short 45 times, total 45 times\n",
      "Mines.lead: long 514 times, short 384 times, total 898 times\n",
      "Coal.lead: long 574 times, short 544 times, total 1118 times\n",
      "Oil.lead: long 564 times, short 29 times, total 593 times\n",
      "Util.lead: long 270 times, short 294 times, total 564 times\n",
      "Telcm.lead: long 45 times, short 435 times, total 480 times\n",
      "Servs.lead: long 35 times, short 40 times, total 75 times\n",
      "BusEq.lead: long 0 times, short 0 times, total 0 times\n",
      "Paper.lead: long 0 times, short 0 times, total 0 times\n",
      "Trans.lead: long 0 times, short 0 times, total 0 times\n",
      "Whlsl.lead: long 0 times, short 0 times, total 0 times\n",
      "Rtail.lead: long 0 times, short 0 times, total 0 times\n",
      "Meals.lead: long 195 times, short 175 times, total 370 times\n",
      "Fin.lead: long 0 times, short 0 times, total 0 times\n",
      "Other.lead: long 15 times, short 50 times, total 65 times\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-11-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>-0.00239437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0288752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>-0.101873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.258829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>-0.0325975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-11-30 00:00:00\n",
       "cagr                   -0.00239437\n",
       "yearly_vol               0.0288752\n",
       "yearly_sharpe            -0.101873\n",
       "max_drawdown             -0.258829\n",
       "sortino                 -0.0325975"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_returns(X, P_L, P_S, START, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate [Dynamic Return Dependencies Across Industries: A Machine Learning Approach](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3120110&download=yes) by David Rapach, Jack Strauss, Jun Tu and Guofu Zhou.\n",
    "\n",
    "1) Use industry returns from [Ken French](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html)\n",
    "\n",
    "2) Forecast (for example) this month's Chemical industry return using last month's returns from all 30 industries \n",
    "\n",
    "3) Use LASSO for predictor subset selection over the entire 1960-2016 period to determine that e.g. Beer is predicted by Food, Clothing, Coal\n",
    "\n",
    "4) Use those predictors and simple linear regression to predict returns\n",
    "\n",
    "5) Generate portfolios and run backtests.\n",
    "\n",
    "- Predictor selection - finds same predictors except 2 industries. Possibly use of AICc instead of AIC (don't see an sklearn implementation that uses AICc)\n",
    "\n",
    "- Prediction by industry - R-squareds line up pretty closely\n",
    "\n",
    "- Portfolio performance, similar ballpark results. Since prediction is similar but return profile is different, must be some difference in portfolio construction. (am taking equal weight top 6 predicted as long and bottom 6 as short, every month)\n",
    "\n",
    "- For some reason their mean returns don't line up to geometric mean annualized, they seem to be calculating something different.\n",
    "\n",
    "- But it does replicate closely and perform pretty well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as datareader\n",
    "import time \n",
    "import datetime\n",
    "import copy\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Hide messy TensorFlow warnings\n",
    "warnings.filterwarnings(\"ignore\") #Hide messy numpy warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1764)\n",
    "\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.regularizers import l1\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "import ffn\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly as py\n",
    "# print (py.__version__) # requires version >= 1.9.0\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "random.seed(1764)\n",
    "np.random.seed(1764)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(697, 133)\n",
      "['Food', 'Beer', 'Smoke', 'Games', 'Books', 'Hshld', 'Clths', 'Hlth', 'Chems', 'Txtls', 'Cnstr', 'Steel', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Mines', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'BusEq', 'Paper', 'Trans', 'Whlsl', 'Rtail', 'Meals', 'Fin', 'Other', '3month', '10year', 'curve', 'month', 'Mkt-RF', 'Food.3m', 'Beer.3m', 'Smoke.3m', 'Games.3m', 'Books.3m', 'Hshld.3m', 'Clths.3m', 'Hlth.3m', 'Chems.3m', 'Txtls.3m', 'Cnstr.3m', 'Steel.3m', 'FabPr.3m', 'ElcEq.3m', 'Autos.3m', 'Carry.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Whlsl.3m', 'Rtail.3m', 'Meals.3m', 'Fin.3m', 'Other.3m', '3month.3m', '10year.3m', 'curve.3m', 'Mkt-RF.3m', 'Food.12m', 'Beer.12m', 'Smoke.12m', 'Games.12m', 'Books.12m', 'Hshld.12m', 'Clths.12m', 'Hlth.12m', 'Chems.12m', 'Txtls.12m', 'Cnstr.12m', 'Steel.12m', 'FabPr.12m', 'ElcEq.12m', 'Autos.12m', 'Carry.12m', 'Mines.12m', 'Coal.12m', 'Oil.12m', 'Util.12m', 'Telcm.12m', 'Servs.12m', 'BusEq.12m', 'Paper.12m', 'Trans.12m', 'Whlsl.12m', 'Rtail.12m', 'Meals.12m', 'Fin.12m', 'Other.12m', '3month.12m', '10year.12m', 'curve.12m', 'Mkt-RF.12m', 'Food.lead', 'Beer.lead', 'Smoke.lead', 'Games.lead', 'Books.lead', 'Hshld.lead', 'Clths.lead', 'Hlth.lead', 'Chems.lead', 'Txtls.lead', 'Cnstr.lead', 'Steel.lead', 'FabPr.lead', 'ElcEq.lead', 'Autos.lead', 'Carry.lead', 'Mines.lead', 'Coal.lead', 'Oil.lead', 'Util.lead', 'Telcm.lead', 'Servs.lead', 'BusEq.lead', 'Paper.lead', 'Trans.lead', 'Whlsl.lead', 'Rtail.lead', 'Meals.lead', 'Fin.lead', 'Other.lead']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3month</th>\n",
       "      <th>10year</th>\n",
       "      <th>curve</th>\n",
       "      <th>month</th>\n",
       "      <th>Mkt-RF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195912</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196001</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196002</th>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196003</th>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196004</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196005</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196006</th>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>-2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196008</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196009</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196010</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196011</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196012</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196101</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196102</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196103</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196104</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196105</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196106</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196108</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196201</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196202</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196203</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196204</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196205</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-8.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201507</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201508</th>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201509</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201510</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201511</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201512</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201606</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201607</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201608</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201609</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201610</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201611</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201612</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201701</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201702</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201703</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201705</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201706</th>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201707</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201708</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201709</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201711</th>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        3month  10year  curve     month  Mkt-RF\n",
       "yyyymm                                         \n",
       "195912    0.34    0.16   0.20  1.000000    2.45\n",
       "196001   -0.14    0.03   0.37  0.083333   -6.98\n",
       "196002   -0.39   -0.23   0.53  0.166667    1.17\n",
       "196003   -0.65   -0.24   0.94  0.250000   -1.63\n",
       "196004   -0.08    0.03   1.05  0.333333   -1.71\n",
       "196005    0.06    0.07   1.06  0.416667    3.12\n",
       "196006   -0.83   -0.20   1.69  0.500000    2.08\n",
       "196007   -0.16   -0.25   1.60  0.583333   -2.37\n",
       "196008    0.00   -0.10   1.50  0.666667    3.01\n",
       "196009    0.18    0.00   1.32  0.750000   -5.99\n",
       "196010   -0.18    0.09   1.59  0.833333   -0.71\n",
       "196011    0.07    0.04   1.56  0.916667    4.69\n",
       "196012   -0.12   -0.09   1.59  1.000000    4.71\n",
       "196101   -0.01    0.00   1.60  0.083333    6.20\n",
       "196102    0.18   -0.06   1.36  0.166667    3.57\n",
       "196103   -0.03   -0.04   1.35  0.250000    2.89\n",
       "196104   -0.10    0.04   1.49  0.333333    0.29\n",
       "196105    0.00   -0.07   1.42  0.416667    2.40\n",
       "196106    0.04    0.17   1.55  0.500000   -3.08\n",
       "196107   -0.09    0.04   1.68  0.583333    2.83\n",
       "196108    0.15    0.12   1.65  0.666667    2.57\n",
       "196109   -0.11   -0.06   1.70  0.750000   -2.15\n",
       "196110    0.02   -0.06   1.62  0.833333    2.57\n",
       "196111    0.18    0.02   1.46  0.916667    4.45\n",
       "196112    0.12    0.12   1.46  1.000000   -0.18\n",
       "196201    0.12    0.02   1.36  0.083333   -3.87\n",
       "196202    0.01   -0.04   1.31  0.166667    1.81\n",
       "196203   -0.01   -0.11   1.21  0.250000   -0.68\n",
       "196204    0.01   -0.09   1.11  0.333333   -6.59\n",
       "196205   -0.04    0.03   1.18  0.416667   -8.65\n",
       "...        ...     ...    ...       ...     ...\n",
       "201507    0.01   -0.04   2.29  0.583333    1.54\n",
       "201508    0.04   -0.15   2.10  0.666667   -6.04\n",
       "201509   -0.05    0.00   2.15  0.750000   -3.08\n",
       "201510    0.00   -0.10   2.05  0.833333    7.75\n",
       "201511    0.10    0.19   2.14  0.916667    0.56\n",
       "201512    0.11   -0.02   2.01  1.000000   -2.17\n",
       "201601    0.03   -0.15   1.83  0.083333   -5.77\n",
       "201602    0.05   -0.31   1.47  0.166667   -0.07\n",
       "201603   -0.02    0.11   1.60  0.250000    6.96\n",
       "201604   -0.06   -0.08   1.58  0.333333    0.92\n",
       "201605    0.04    0.00   1.54  0.416667    1.78\n",
       "201606    0.00   -0.17   1.37  0.500000   -0.05\n",
       "201607    0.03   -0.14   1.20  0.583333    3.95\n",
       "201608    0.00    0.06   1.26  0.666667    0.50\n",
       "201609   -0.01    0.07   1.34  0.750000    0.25\n",
       "201610    0.04    0.13   1.43  0.833333   -2.02\n",
       "201611    0.12    0.38   1.69  0.916667    4.86\n",
       "201612    0.06    0.35   1.98  1.000000    1.82\n",
       "201701    0.00   -0.06   1.92  0.083333    1.94\n",
       "201702    0.01   -0.01   1.90  0.166667    3.57\n",
       "201703    0.22    0.06   1.74  0.250000    0.17\n",
       "201704    0.06   -0.18   1.50  0.333333    1.09\n",
       "201705    0.09    0.00   1.41  0.416667    1.06\n",
       "201706    0.09   -0.11   1.21  0.500000    0.78\n",
       "201707    0.09    0.13   1.25  0.583333    1.87\n",
       "201708   -0.06   -0.11   1.20  0.666667    0.16\n",
       "201709    0.02   -0.01   1.17  0.750000    2.51\n",
       "201710    0.04    0.16   1.29  0.833333    2.25\n",
       "201711    0.16   -0.01   1.12  0.916667    3.12\n",
       "201712    0.09    0.05   1.08  1.000000    1.06\n",
       "\n",
       "[697 rows x 5 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(\"30_Industry_Portfolios.csv\")\n",
    "data = data.set_index('yyyymm')\n",
    "industries = list(data.columns)\n",
    "# map industry names to col nums\n",
    "ind_reverse_dict = dict([(industries[i], i) for i in range(len(industries))])\n",
    "\n",
    "rfdata = pd.read_csv(\"F-F_Research_Data_Factors.csv\")\n",
    "rfdata = rfdata.set_index('yyyymm')\n",
    "data['rf'] = rfdata['RF']\n",
    "\n",
    "# subtract risk-free rate\n",
    "# create a response variable led by 1 period to predict\n",
    "for ind in industries:\n",
    "    data[ind] = data[ind] - data['rf']\n",
    "\n",
    "    \n",
    "# add rates data from FRED\n",
    "start_date = datetime.datetime(1926, 9, 1)\n",
    "end_date = datetime.datetime(2017, 12, 1)\n",
    "TB3MS = datareader.DataReader(\"TB3MS\", \"fred\", start_date, end_date)\n",
    "TB3MS['yyyymm'] = TB3MS.index.strftime('%Y%m')\n",
    "TB3MS['yyyymm'] = [int(datestr) for datestr in TB3MS['yyyymm']]\n",
    "TB3MS=TB3MS.set_index(['yyyymm'])\n",
    "data['3month']=TB3MS['TB3MS']\n",
    "\n",
    "GS10 =  datareader.DataReader(\"GS10\", \"fred\", start_date, end_date)\n",
    "GS10['yyyymm'] = GS10.index.strftime('%Y%m')\n",
    "GS10['yyyymm'] = [int(datestr) for datestr in GS10['yyyymm']]\n",
    "GS10=GS10.set_index(['yyyymm'])\n",
    "data['10year']=GS10['GS10']\n",
    "\n",
    "data['curve'] = data['10year'] - data['3month']\n",
    "data['10year'] = data['10year'].diff() # first difference 10-year yield\n",
    "data['3month'] = data['3month'].diff() # first difference 3-month\n",
    "data['month'] = (data.index  % 100)/12.0 # for possible seasonality\n",
    "data['Mkt-RF'] = rfdata['Mkt-RF']\n",
    "\n",
    "for ind in industries + ['3month', '10year', 'curve', 'Mkt-RF',]:\n",
    "    data[ind+\".3m\"] = pd.rolling_mean(data[ind],3)\n",
    "    \n",
    "#for ind in industries + ['3month', '10year', 'curve', 'Mkt-RF',]:\n",
    "#    data[ind+\".6m\"] = pd.rolling_mean(data[ind],6)\n",
    "\n",
    "for ind in industries + ['3month', '10year', 'curve', 'Mkt-RF',]:\n",
    "    data[ind+\".12m\"] = pd.rolling_mean(data[ind],12)\n",
    "\n",
    "for ind in industries:\n",
    "    data[ind+\".lead\"] = data[ind].shift(-1)\n",
    "\n",
    "data = data.loc[data.index[data.index > 195911]]\n",
    "data = data.drop(columns=['rf'])    \n",
    "data = data.dropna(axis=0, how='any')\n",
    "\n",
    "nresponses = len(industries)\n",
    "npredictors = data.shape[1]-nresponses\n",
    "\n",
    "predictors = list(data.columns[:npredictors])\n",
    "predictor_reverse_dict = dict([(predictors[i], i) for i in range(len(predictors))])\n",
    "\n",
    "responses = list(data.columns[-nresponses:])\n",
    "response_reverse_dict = dict([(responses[i], i) for i in range(len(responses))])\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n",
    "data[['3month', '10year', 'curve', 'month', 'Mkt-RF',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Beer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Games</th>\n",
       "      <th>Books</th>\n",
       "      <th>Hshld</th>\n",
       "      <th>Clths</th>\n",
       "      <th>Hlth</th>\n",
       "      <th>Chems</th>\n",
       "      <th>Txtls</th>\n",
       "      <th>...</th>\n",
       "      <th>Telcm.lead</th>\n",
       "      <th>Servs.lead</th>\n",
       "      <th>BusEq.lead</th>\n",
       "      <th>Paper.lead</th>\n",
       "      <th>Trans.lead</th>\n",
       "      <th>Whlsl.lead</th>\n",
       "      <th>Rtail.lead</th>\n",
       "      <th>Meals.lead</th>\n",
       "      <th>Fin.lead</th>\n",
       "      <th>Other.lead</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195912</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>1.64</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>-4.31</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>-6.09</td>\n",
       "      <td>-10.08</td>\n",
       "      <td>-4.68</td>\n",
       "      <td>-3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196001</th>\n",
       "      <td>-4.49</td>\n",
       "      <td>-5.71</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-5.47</td>\n",
       "      <td>-7.84</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>-6.68</td>\n",
       "      <td>-10.03</td>\n",
       "      <td>-4.77</td>\n",
       "      <td>...</td>\n",
       "      <td>8.07</td>\n",
       "      <td>9.13</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196002</th>\n",
       "      <td>3.35</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>2.27</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.39</td>\n",
       "      <td>9.31</td>\n",
       "      <td>1.44</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-3.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196003</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>-6.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>8.86</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196004</th>\n",
       "      <td>1.17</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>1.35</td>\n",
       "      <td>6.46</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.49</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>11.90</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196005</th>\n",
       "      <td>8.20</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>2.44</td>\n",
       "      <td>7.28</td>\n",
       "      <td>11.67</td>\n",
       "      <td>7.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>13.50</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-8.07</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.72</td>\n",
       "      <td>6.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196006</th>\n",
       "      <td>5.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.38</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.84</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>-6.16</td>\n",
       "      <td>-2.99</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>-2.11</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>4.60</td>\n",
       "      <td>-4.72</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>-6.80</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>...</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.69</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.51</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196008</th>\n",
       "      <td>4.57</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.20</td>\n",
       "      <td>7.16</td>\n",
       "      <td>3.63</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.07</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-7.61</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-8.44</td>\n",
       "      <td>-8.57</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196009</th>\n",
       "      <td>-3.88</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-6.20</td>\n",
       "      <td>-9.18</td>\n",
       "      <td>-4.23</td>\n",
       "      <td>-8.87</td>\n",
       "      <td>-6.70</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.62</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-4.54</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196010</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.48</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.06</td>\n",
       "      <td>9.49</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.35</td>\n",
       "      <td>9.72</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4.40</td>\n",
       "      <td>7.71</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196011</th>\n",
       "      <td>9.46</td>\n",
       "      <td>6.57</td>\n",
       "      <td>5.44</td>\n",
       "      <td>13.91</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.04</td>\n",
       "      <td>...</td>\n",
       "      <td>12.29</td>\n",
       "      <td>8.18</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.57</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.56</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196012</th>\n",
       "      <td>4.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.54</td>\n",
       "      <td>7.77</td>\n",
       "      <td>7.41</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.28</td>\n",
       "      <td>6.06</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>7.70</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.08</td>\n",
       "      <td>4.56</td>\n",
       "      <td>8.35</td>\n",
       "      <td>7.93</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.08</td>\n",
       "      <td>7.12</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196101</th>\n",
       "      <td>4.70</td>\n",
       "      <td>5.23</td>\n",
       "      <td>8.77</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.47</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.94</td>\n",
       "      <td>5.86</td>\n",
       "      <td>6.46</td>\n",
       "      <td>11.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.54</td>\n",
       "      <td>6.83</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.31</td>\n",
       "      <td>4.82</td>\n",
       "      <td>8.23</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196102</th>\n",
       "      <td>4.21</td>\n",
       "      <td>8.16</td>\n",
       "      <td>5.41</td>\n",
       "      <td>22.33</td>\n",
       "      <td>2.15</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>2.13</td>\n",
       "      <td>6.81</td>\n",
       "      <td>...</td>\n",
       "      <td>7.23</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4.45</td>\n",
       "      <td>5.76</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.34</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196103</th>\n",
       "      <td>4.64</td>\n",
       "      <td>2.55</td>\n",
       "      <td>5.60</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.77</td>\n",
       "      <td>6.34</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196104</th>\n",
       "      <td>-1.39</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.39</td>\n",
       "      <td>4.74</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196105</th>\n",
       "      <td>4.20</td>\n",
       "      <td>5.38</td>\n",
       "      <td>3.39</td>\n",
       "      <td>-3.91</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-3.31</td>\n",
       "      <td>-4.46</td>\n",
       "      <td>-4.57</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>-2.88</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196106</th>\n",
       "      <td>-2.17</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-5.87</td>\n",
       "      <td>-3.85</td>\n",
       "      <td>3.43</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.35</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>2.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.95</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196108</th>\n",
       "      <td>4.92</td>\n",
       "      <td>3.20</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10.45</td>\n",
       "      <td>5.21</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.24</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.30</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>3.73</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>7.05</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>8.28</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.11</td>\n",
       "      <td>...</td>\n",
       "      <td>8.34</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.65</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>5.28</td>\n",
       "      <td>4.47</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.51</td>\n",
       "      <td>5.30</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.49</td>\n",
       "      <td>7.37</td>\n",
       "      <td>...</td>\n",
       "      <td>3.14</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-4.44</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>-3.69</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-6.12</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>-3.78</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.32</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>-6.91</td>\n",
       "      <td>-5.22</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-9.56</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196201</th>\n",
       "      <td>-6.67</td>\n",
       "      <td>-3.45</td>\n",
       "      <td>-4.28</td>\n",
       "      <td>-13.23</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>-5.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>3.59</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196202</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>2.01</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-4.07</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196203</th>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-6.67</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.93</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>-12.01</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-6.27</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>-9.09</td>\n",
       "      <td>-7.69</td>\n",
       "      <td>-2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196204</th>\n",
       "      <td>-4.59</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-12.99</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>-8.74</td>\n",
       "      <td>-7.03</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>-11.23</td>\n",
       "      <td>-6.23</td>\n",
       "      <td>-7.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>-10.97</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>-11.03</td>\n",
       "      <td>-5.17</td>\n",
       "      <td>-11.34</td>\n",
       "      <td>-9.09</td>\n",
       "      <td>-7.46</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>-11.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196205</th>\n",
       "      <td>-11.25</td>\n",
       "      <td>-9.05</td>\n",
       "      <td>-14.14</td>\n",
       "      <td>-11.39</td>\n",
       "      <td>-14.87</td>\n",
       "      <td>-10.19</td>\n",
       "      <td>-10.01</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>-7.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>-13.11</td>\n",
       "      <td>-12.59</td>\n",
       "      <td>-9.62</td>\n",
       "      <td>-7.81</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-10.43</td>\n",
       "      <td>-12.90</td>\n",
       "      <td>-11.01</td>\n",
       "      <td>-14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201507</th>\n",
       "      <td>4.03</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.59</td>\n",
       "      <td>6.09</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.42</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-6.50</td>\n",
       "      <td>-5.69</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-4.13</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>-6.48</td>\n",
       "      <td>-6.54</td>\n",
       "      <td>-5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201508</th>\n",
       "      <td>-4.37</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>-8.61</td>\n",
       "      <td>-6.94</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>-8.37</td>\n",
       "      <td>-7.15</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201509</th>\n",
       "      <td>-1.19</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-9.94</td>\n",
       "      <td>-5.32</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-7.28</td>\n",
       "      <td>-8.38</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>8.84</td>\n",
       "      <td>11.26</td>\n",
       "      <td>8.16</td>\n",
       "      <td>10.19</td>\n",
       "      <td>6.48</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.90</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201510</th>\n",
       "      <td>5.81</td>\n",
       "      <td>8.06</td>\n",
       "      <td>10.90</td>\n",
       "      <td>14.61</td>\n",
       "      <td>12.21</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.74</td>\n",
       "      <td>16.62</td>\n",
       "      <td>7.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201511</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.03</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-4.64</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>-5.02</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201512</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>1.86</td>\n",
       "      <td>-4.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-5.09</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>-5.27</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>-8.68</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-9.63</td>\n",
       "      <td>-3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>4.28</td>\n",
       "      <td>-8.15</td>\n",
       "      <td>-5.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-9.43</td>\n",
       "      <td>-11.10</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.89</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>0.95</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4.25</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.76</td>\n",
       "      <td>8.86</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.18</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.36</td>\n",
       "      <td>6.65</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>4.69</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.04</td>\n",
       "      <td>8.61</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.90</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>-5.46</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.19</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.42</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-4.96</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-2.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-5.30</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201606</th>\n",
       "      <td>4.75</td>\n",
       "      <td>5.31</td>\n",
       "      <td>6.87</td>\n",
       "      <td>-4.43</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>...</td>\n",
       "      <td>2.27</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.20</td>\n",
       "      <td>2.52</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.04</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201607</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-2.79</td>\n",
       "      <td>6.15</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.62</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.29</td>\n",
       "      <td>8.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.56</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.09</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>4.88</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201608</th>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4.33</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>2.86</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>-3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201609</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>4.62</td>\n",
       "      <td>-3.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-6.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>-4.87</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.24</td>\n",
       "      <td>-5.49</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201610</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>4.59</td>\n",
       "      <td>5.59</td>\n",
       "      <td>-10.28</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-7.45</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.39</td>\n",
       "      <td>4.21</td>\n",
       "      <td>12.75</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.99</td>\n",
       "      <td>8.47</td>\n",
       "      <td>12.84</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201611</th>\n",
       "      <td>-4.41</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-5.12</td>\n",
       "      <td>3.87</td>\n",
       "      <td>8.15</td>\n",
       "      <td>-4.18</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.55</td>\n",
       "      <td>1.58</td>\n",
       "      <td>...</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201612</th>\n",
       "      <td>4.43</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>...</td>\n",
       "      <td>3.36</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201701</th>\n",
       "      <td>0.95</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>5.61</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.23</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.35</td>\n",
       "      <td>4.58</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201702</th>\n",
       "      <td>1.71</td>\n",
       "      <td>6.13</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>4.83</td>\n",
       "      <td>3.81</td>\n",
       "      <td>7.03</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201703</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.03</td>\n",
       "      <td>5.79</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>3.21</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>0.76</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>2.74</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6.93</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201705</th>\n",
       "      <td>1.63</td>\n",
       "      <td>4.28</td>\n",
       "      <td>6.12</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201706</th>\n",
       "      <td>-2.65</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9.38</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1.93</td>\n",
       "      <td>4.39</td>\n",
       "      <td>...</td>\n",
       "      <td>5.25</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-2.42</td>\n",
       "      <td>-4.49</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201707</th>\n",
       "      <td>1.52</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-6.06</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-4.55</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201708</th>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.44</td>\n",
       "      <td>6.20</td>\n",
       "      <td>4.46</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201709</th>\n",
       "      <td>0.43</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.54</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.40</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.75</td>\n",
       "      <td>5.93</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4.28</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.22</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>0.71</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-3.09</td>\n",
       "      <td>3.47</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1.23</td>\n",
       "      <td>...</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.41</td>\n",
       "      <td>4.21</td>\n",
       "      <td>6.66</td>\n",
       "      <td>3.37</td>\n",
       "      <td>9.38</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201711</th>\n",
       "      <td>4.15</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.30</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.94</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.21</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.31</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.77</td>\n",
       "      <td>...</td>\n",
       "      <td>3.08</td>\n",
       "      <td>9.25</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4.84</td>\n",
       "      <td>11.37</td>\n",
       "      <td>3.12</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Food  Beer  Smoke  Games  Books  Hshld  Clths   Hlth  Chems  Txtls  \\\n",
       "yyyymm                                                                        \n",
       "195912   2.01  0.35  -3.02   1.64   7.29   0.67   1.87  -1.97   3.08   0.74   \n",
       "196001  -4.49 -5.71  -2.05   1.21  -5.47  -7.84  -8.53  -6.68 -10.03  -4.77   \n",
       "196002   3.35 -2.14   2.27   4.23   2.39   9.31   1.44  -0.02  -0.74   0.32   \n",
       "196003  -1.67 -2.94  -0.18  -0.65   2.18  -0.56  -2.59   1.26  -2.75  -6.79   \n",
       "196004   1.17 -2.16   1.35   6.46  -1.17  -1.27   0.21   1.49  -5.53  -1.10   \n",
       "196005   8.20 -0.52   2.44   7.28  11.67   7.74   1.74  13.50   3.40   2.10   \n",
       "196006   5.39  0.47   4.73   2.24   0.02   6.38  -1.59  -0.40   0.45   4.04   \n",
       "196007  -2.11 -0.79   4.60  -4.72   0.23  -0.60  -1.10  -3.99  -6.80  -3.14   \n",
       "196008   4.57  3.24   5.20   7.16   3.63   5.09   3.34   2.29   1.17  -0.84   \n",
       "196009  -3.88 -5.00  -2.09  -2.33  -6.20  -9.18  -4.23  -8.87  -6.70  -5.25   \n",
       "196010   1.02  0.54   3.87   0.11   2.38   6.48  -3.50  -3.71  -1.59  -3.06   \n",
       "196011   9.46  6.57   5.44  13.91  10.11   9.13   3.15   3.91   4.25   2.04   \n",
       "196012   4.51 -0.31   3.54   7.77   7.41   1.76   3.28   6.06   2.85   0.52   \n",
       "196101   4.70  5.23   8.77   0.56   9.47   4.36   5.94   5.86   6.46  11.21   \n",
       "196102   4.21  8.16   5.41  22.33   2.15   5.90   7.84   5.05   2.13   6.81   \n",
       "196103   4.64  2.55   5.60   7.18   4.77   6.34   3.08   3.60   0.92   5.92   \n",
       "196104  -1.39  1.40  -0.23  -2.21  -6.37   2.66   2.60  -0.47  -1.47  -5.31   \n",
       "196105   4.20  5.38   3.39  -3.91   2.71  -0.02   6.80   2.10   5.50   5.47   \n",
       "196106  -2.17 -3.12   3.97  -5.87  -3.85   3.43  -5.50  -3.58  -1.32  -3.36   \n",
       "196107   2.72  0.88   5.95  -1.21  -2.55   1.97   2.03   3.27   2.95   1.53   \n",
       "196108   4.92  3.20   7.74   0.89   0.89  10.45   5.21   3.70   2.35   5.77   \n",
       "196109  -0.62 -1.48  -0.07   1.24   0.75  -3.05  -1.14  -1.48  -4.45  -4.25   \n",
       "196110   3.73 -0.84   7.05  -5.26   0.99  -0.67   8.28   3.33   0.05   3.11   \n",
       "196111   5.28  4.47   8.03   0.25   3.75   4.51   5.30   3.12   2.49   7.37   \n",
       "196112  -3.69  1.41  -6.12   1.97  -3.66  -3.78   0.32  -2.21  -0.16  -1.17   \n",
       "196201  -6.67 -3.45  -4.28 -13.23  -3.44  -7.37  -5.89  -4.86  -4.76   0.57   \n",
       "196202  -0.25  0.28   0.68  -2.02  -0.52  -0.90   2.01   3.56   3.30   1.93   \n",
       "196203   0.98 -0.34  -6.67  -5.34   0.41   4.31  -1.18   0.34  -2.72  -0.74   \n",
       "196204  -4.59 -3.59 -12.99 -11.04  -8.74  -7.03  -8.01 -11.23  -6.23  -7.53   \n",
       "196205 -11.25 -9.05 -14.14 -11.39 -14.87 -10.19 -10.01 -11.14  -8.25  -7.50   \n",
       "...       ...   ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "201507   4.03  3.51   9.59   6.09  -2.90   0.71   5.96   3.66  -4.90  -0.72   \n",
       "201508  -4.37 -3.12  -4.06  -7.35  -8.61  -6.94  -3.86  -8.37  -7.15  -3.11   \n",
       "201509  -1.19  2.58   2.37  -9.94  -5.32  -0.53   1.18  -7.28  -8.38  -5.92   \n",
       "201510   5.81  8.06  10.90  14.61  12.21   5.81   0.98   7.74  16.62   7.96   \n",
       "201511   0.11 -0.71  -3.00  -0.41  -1.17  -1.10  -1.08   0.71   1.68  -2.59   \n",
       "201512   1.96  0.30   1.59  -1.70  -6.18   1.86  -4.38   0.39  -4.82  -2.65   \n",
       "201601  -1.67 -0.23   4.28  -8.15  -5.28   0.16   1.52  -9.43 -11.10  -5.33   \n",
       "201602   0.95 -2.34   0.93   4.25  -0.96   0.34   0.81  -1.09   6.79   0.63   \n",
       "201603   4.69  5.61   5.04   8.61   6.88   4.65   2.30   2.90   8.33   3.58   \n",
       "201604   0.63  0.31  -0.25  -6.30   1.82  -0.42  -2.27   3.55   3.77   1.55   \n",
       "201605   2.06 -0.91   0.83   5.42  -0.53  -0.09  -4.96   2.46  -1.42  -1.70   \n",
       "201606   4.75  5.31   6.87  -4.43  -0.34   3.16   1.63   0.11  -1.14  -4.67   \n",
       "201607  -0.51  1.82  -2.79   6.15   7.38   2.58   1.62   6.00   4.29   8.39   \n",
       "201608  -0.52 -0.90  -1.22   0.94   0.29   1.24   1.37  -3.24   2.48   1.03   \n",
       "201609  -2.92  1.63  -2.78   4.62  -3.95   0.00  -6.92   0.35  -1.76  -4.87   \n",
       "201610  -0.33 -1.65   4.59   5.59 -10.28  -2.96  -5.76  -7.45  -1.95  -4.17   \n",
       "201611  -4.41 -5.76  -5.12   3.87   8.15  -4.18   1.80   1.37   7.55   1.58   \n",
       "201612   4.43  3.00   5.39  -3.36   1.98   1.43  -0.44   0.82   0.32  -1.27   \n",
       "201701   0.95 -1.02   5.61   4.84   1.60   2.72  -0.01   2.17   3.79   6.90   \n",
       "201702   1.71  6.13   7.93   0.65  -0.39   4.83   3.81   7.03   3.10  -2.12   \n",
       "201703   0.52  0.89   1.03   5.79  -0.92  -0.36   0.32  -0.19   2.17   1.94   \n",
       "201704   0.76  1.89  -0.14   2.74  -0.83  -0.36  -1.24   1.06   1.28   3.24   \n",
       "201705   1.63  4.28   6.12   2.80  -1.71   1.77  -2.17  -0.50  -0.63  -0.14   \n",
       "201706  -2.65 -1.20  -1.00  -0.20   2.69  -0.19   9.38   5.49   1.93   4.39   \n",
       "201707   1.52  1.18  -6.06   4.98   2.34   1.79   1.25   0.45   1.03   1.22   \n",
       "201708  -2.77  0.94  -0.96  -1.78  -6.37  -0.04  -5.79   1.69   1.52   2.62   \n",
       "201709   0.43 -3.00  -2.13   3.66   3.54  -0.21   1.01   1.40   6.58   0.45   \n",
       "201710   0.71  1.24  -2.90   2.02  -1.90  -3.09   3.47  -2.35   5.52   1.23   \n",
       "201711   4.15  4.33   1.34   3.30  10.00   4.55   7.94   2.28   2.17   3.80   \n",
       "201712  -0.10  4.31   4.87   0.80   1.06   2.31   4.65  -0.32   0.49  -1.77   \n",
       "\n",
       "           ...      Telcm.lead  Servs.lead  BusEq.lead  Paper.lead  \\\n",
       "yyyymm     ...                                                       \n",
       "195912     ...            0.62       -6.18       -7.93       -9.41   \n",
       "196001     ...            8.07        9.13        5.09        3.00   \n",
       "196002     ...           -0.21       -0.31        3.34       -2.43   \n",
       "196003     ...           -1.24        7.14        1.77        0.41   \n",
       "196004     ...            3.05       -1.75       11.90        2.85   \n",
       "196005     ...           -0.58       -8.07        2.39        3.50   \n",
       "196006     ...           -0.03        2.84       -2.02       -4.10   \n",
       "196007     ...            6.94        5.69        2.71        1.18   \n",
       "196008     ...           -6.07       -3.53       -7.61       -7.37   \n",
       "196009     ...           -0.08        4.62       -3.40       -1.85   \n",
       "196010     ...            4.06        9.49        8.19        5.31   \n",
       "196011     ...           12.29        8.18        4.29        5.57   \n",
       "196012     ...            7.70        4.29        5.08        4.56   \n",
       "196101     ...            0.61        0.20        4.54        6.83   \n",
       "196102     ...            7.23       -0.20        2.31       -0.69   \n",
       "196103     ...            0.63       -0.12        2.19       -0.37   \n",
       "196104     ...           -1.22       -0.70        1.57        1.39   \n",
       "196105     ...           -4.19        0.13       -3.31       -4.46   \n",
       "196106     ...            6.25       -8.25        0.56       -0.50   \n",
       "196107     ...            0.12        8.99        5.11        5.37   \n",
       "196108     ...           -2.94       -6.04        1.01       -2.74   \n",
       "196109     ...            0.00        2.24        6.53        1.74   \n",
       "196110     ...            8.34        8.27        0.99        2.05   \n",
       "196111     ...            3.14       -0.68       -0.55       -2.65   \n",
       "196112     ...           -6.32       -4.88       -6.91       -5.22   \n",
       "196201     ...            3.89       -1.94       -0.07        3.87   \n",
       "196202     ...           -3.14       -1.41       -0.76        1.13   \n",
       "196203     ...           -4.93       -3.11      -12.01       -7.93   \n",
       "196204     ...           -7.35      -10.97      -13.19      -11.03   \n",
       "196205     ...           -8.72      -13.11      -12.59       -9.62   \n",
       "...        ...             ...         ...         ...         ...   \n",
       "201507     ...           -8.42       -5.29       -6.50       -5.69   \n",
       "201508     ...           -2.63       -1.47       -1.72       -2.66   \n",
       "201509     ...            8.84       11.26        8.16       10.19   \n",
       "201510     ...           -1.92        1.99        0.12       -0.02   \n",
       "201511     ...           -3.03       -1.19       -4.64       -3.75   \n",
       "201512     ...           -0.36       -5.09       -7.95       -5.27   \n",
       "201601     ...            1.15       -2.45        1.45        2.99   \n",
       "201602     ...            6.00        7.76        8.86        8.18   \n",
       "201603     ...            0.59       -2.55       -5.46        0.80   \n",
       "201604     ...            0.30        4.67        5.64        1.79   \n",
       "201605     ...            3.10       -2.12       -1.63        2.06   \n",
       "201606     ...            2.27        7.33        8.20        2.52   \n",
       "201607     ...           -3.56        1.19        2.38        2.46   \n",
       "201608     ...            0.52        0.82        4.33       -0.64   \n",
       "201609     ...           -2.85       -0.55       -2.24       -5.49   \n",
       "201610     ...            6.25       -0.01        2.39        4.21   \n",
       "201611     ...            4.65       -0.19        2.07        1.86   \n",
       "201612     ...            3.36        5.45        3.20        2.28   \n",
       "201701     ...           -0.11        3.23        6.99        3.21   \n",
       "201702     ...            0.81        1.81        2.45        0.26   \n",
       "201703     ...           -0.16        3.83        0.99        2.06   \n",
       "201704     ...           -2.00        3.60        4.57        2.60   \n",
       "201705     ...           -2.33       -0.82       -3.44        1.87   \n",
       "201706     ...            5.25        3.98        3.05       -2.42   \n",
       "201707     ...           -2.58        1.36        4.98        0.72   \n",
       "201708     ...           -1.68        0.60        0.80        2.44   \n",
       "201709     ...           -5.75        5.93        7.12        4.28   \n",
       "201710     ...            3.91        0.05        2.41        4.21   \n",
       "201711     ...            4.41        0.39       -0.89       -0.95   \n",
       "201712     ...            3.08        9.25        5.02        4.28   \n",
       "\n",
       "        Trans.lead  Whlsl.lead  Rtail.lead  Meals.lead  Fin.lead  Other.lead  \n",
       "yyyymm                                                                        \n",
       "195912       -4.31       -5.33       -6.09      -10.08     -4.68       -3.98  \n",
       "196001       -0.94        1.42        4.00        1.81     -0.98        6.32  \n",
       "196002       -4.99       -1.37       -0.13       -3.88      0.05       -2.43  \n",
       "196003       -2.13        0.45       -0.53        8.86     -0.64        0.55  \n",
       "196004        0.90        1.65        3.11        0.80     -0.45        1.02  \n",
       "196005        2.17        5.96        3.41        1.03      3.72        6.41  \n",
       "196006       -3.11       -6.16       -2.99       -1.25      0.09       -5.95  \n",
       "196007        1.98        4.51        2.85        2.05      3.47        3.48  \n",
       "196008       -7.07       -8.44       -8.57       -1.90     -5.78       -4.21  \n",
       "196009       -1.02       -4.22        0.31       -4.54     -0.40        0.38  \n",
       "196010        5.35        9.72        6.50        4.40      7.71        4.01  \n",
       "196011        2.27        2.06        2.05        2.08      5.56        3.80  \n",
       "196012        8.35        7.93        2.28        4.08      7.12        8.23  \n",
       "196101        4.22        3.31        4.82        8.23      7.00        6.00  \n",
       "196102        0.86        4.45        5.76        4.06      4.34        7.08  \n",
       "196103       -1.62        3.08        0.22        4.23      1.38       -3.67  \n",
       "196104        4.74       -0.04        4.31       -1.90      4.00        3.32  \n",
       "196105       -4.57       -4.90        0.80       -5.63     -2.88        0.37  \n",
       "196106       -0.32       -0.01        2.45        2.69      3.35        5.37  \n",
       "196107        3.52        3.09        3.03        0.46      8.65        1.64  \n",
       "196108       -1.16       -4.22        0.66       -6.21     -0.40        3.14  \n",
       "196109        2.16        4.30        9.35        0.71      2.02        0.39  \n",
       "196110        0.47        5.65        4.90        1.08      7.22        1.69  \n",
       "196111       -0.24        0.46       -0.63       -2.21     -4.44       -0.77  \n",
       "196112        2.52       -0.79       -9.56       -3.90     -4.99       -3.62  \n",
       "196201        0.32       -0.09        1.58       -0.59      3.59        4.20  \n",
       "196202       -1.68       -2.30        0.90       -4.07     -2.13       -1.83  \n",
       "196203       -6.27       -5.78       -4.61       -9.09     -7.69       -2.12  \n",
       "196204       -5.17      -11.34       -9.09       -7.46    -10.02      -11.83  \n",
       "196205       -7.81      -11.11      -10.43      -12.90    -11.01      -14.25  \n",
       "...            ...         ...         ...         ...       ...         ...  \n",
       "201507       -6.37       -4.13       -5.44       -6.48     -6.54       -5.20  \n",
       "201508       -0.71       -6.04       -1.75        0.44     -3.14       -1.87  \n",
       "201509        6.48        5.07        4.56        5.05      5.90        6.98  \n",
       "201510       -1.10        2.67        0.61       -1.01      2.16        0.05  \n",
       "201511       -5.02       -1.88        0.82       -0.95     -2.92        0.25  \n",
       "201512       -8.53       -8.68       -4.45       -0.94     -9.63       -3.20  \n",
       "201601        6.89        3.85       -0.36        1.03     -2.85        2.71  \n",
       "201602        6.86        6.18        5.99        5.36      6.65        6.68  \n",
       "201603       -1.08        0.49       -0.38       -2.38      3.96        0.67  \n",
       "201604       -2.18        1.78        1.19       -1.48      2.15       -2.02  \n",
       "201605       -2.53        1.81        0.71        1.16     -5.30        3.61  \n",
       "201606        5.39        3.65        3.78        2.19      4.04       -0.21  \n",
       "201607        1.09       -1.03       -1.69       -0.24      4.88        2.24  \n",
       "201608        2.86       -2.56       -0.18       -2.25     -1.45       -3.48  \n",
       "201609       -0.64       -8.18       -3.59       -1.96      1.40       -0.53  \n",
       "201610       12.75        9.29        2.99        8.47     12.84        8.29  \n",
       "201611        0.84        2.34       -0.98        0.58      3.80        2.57  \n",
       "201612        1.70        1.69        0.93        0.71      0.56       -0.87  \n",
       "201701        2.56        2.73        2.79        2.35      4.58        3.71  \n",
       "201702       -2.80       -1.29        0.80        2.73     -2.26       -1.83  \n",
       "201703        2.14       -2.96        3.21        4.30      0.18       -1.06  \n",
       "201704        2.68        2.56        0.45        6.93     -1.03       -0.74  \n",
       "201705        3.38        0.15       -2.45       -2.54      5.70        1.31  \n",
       "201706       -4.49       -0.99        0.93       -2.63      1.86        1.44  \n",
       "201707        0.60       -4.55       -1.81        1.26     -1.31        1.53  \n",
       "201708        6.20        4.46        2.71        0.79      5.07        0.99  \n",
       "201709       -1.99       -1.18        3.22        3.64      3.22       -1.20  \n",
       "201710        6.66        3.37        9.38        5.45      3.76        1.60  \n",
       "201711        2.73        4.21        2.45        1.18      0.88        1.14  \n",
       "201712        2.56        4.84       11.37        3.12      6.00        5.41  \n",
       "\n",
       "[697 rows x 133 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = data.loc[data.index[data.index < 201701]]\n",
    "data = data.loc[data.index[data.index > 195911]]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Beer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Games</th>\n",
       "      <th>Books</th>\n",
       "      <th>Hshld</th>\n",
       "      <th>Clths</th>\n",
       "      <th>Hlth</th>\n",
       "      <th>Chems</th>\n",
       "      <th>Txtls</th>\n",
       "      <th>...</th>\n",
       "      <th>Telcm.lead</th>\n",
       "      <th>Servs.lead</th>\n",
       "      <th>BusEq.lead</th>\n",
       "      <th>Paper.lead</th>\n",
       "      <th>Trans.lead</th>\n",
       "      <th>Whlsl.lead</th>\n",
       "      <th>Rtail.lead</th>\n",
       "      <th>Meals.lead</th>\n",
       "      <th>Fin.lead</th>\n",
       "      <th>Other.lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.688666</td>\n",
       "      <td>0.727030</td>\n",
       "      <td>0.985079</td>\n",
       "      <td>0.732095</td>\n",
       "      <td>0.532253</td>\n",
       "      <td>0.564333</td>\n",
       "      <td>0.690387</td>\n",
       "      <td>0.665825</td>\n",
       "      <td>0.552367</td>\n",
       "      <td>0.687145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515968</td>\n",
       "      <td>0.729928</td>\n",
       "      <td>0.622970</td>\n",
       "      <td>0.534806</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.631076</td>\n",
       "      <td>0.698235</td>\n",
       "      <td>0.728766</td>\n",
       "      <td>0.637547</td>\n",
       "      <td>0.396628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.308660</td>\n",
       "      <td>5.058992</td>\n",
       "      <td>6.032324</td>\n",
       "      <td>7.128170</td>\n",
       "      <td>5.780362</td>\n",
       "      <td>4.728000</td>\n",
       "      <td>6.355251</td>\n",
       "      <td>4.897557</td>\n",
       "      <td>5.482363</td>\n",
       "      <td>6.970961</td>\n",
       "      <td>...</td>\n",
       "      <td>4.607931</td>\n",
       "      <td>6.486956</td>\n",
       "      <td>6.698787</td>\n",
       "      <td>5.021876</td>\n",
       "      <td>5.707154</td>\n",
       "      <td>5.571040</td>\n",
       "      <td>5.334178</td>\n",
       "      <td>6.065564</td>\n",
       "      <td>5.381389</td>\n",
       "      <td>5.771655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.150000</td>\n",
       "      <td>-20.190000</td>\n",
       "      <td>-25.320000</td>\n",
       "      <td>-33.400000</td>\n",
       "      <td>-26.560000</td>\n",
       "      <td>-22.240000</td>\n",
       "      <td>-31.500000</td>\n",
       "      <td>-21.060000</td>\n",
       "      <td>-28.600000</td>\n",
       "      <td>-33.110000</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.440000</td>\n",
       "      <td>-28.670000</td>\n",
       "      <td>-32.070000</td>\n",
       "      <td>-27.740000</td>\n",
       "      <td>-28.500000</td>\n",
       "      <td>-29.250000</td>\n",
       "      <td>-29.740000</td>\n",
       "      <td>-31.890000</td>\n",
       "      <td>-22.530000</td>\n",
       "      <td>-28.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.630000</td>\n",
       "      <td>-2.080000</td>\n",
       "      <td>-2.740000</td>\n",
       "      <td>-3.390000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>-2.030000</td>\n",
       "      <td>-2.800000</td>\n",
       "      <td>-2.230000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-3.170000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110000</td>\n",
       "      <td>-3.050000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-2.780000</td>\n",
       "      <td>-2.560000</td>\n",
       "      <td>-2.380000</td>\n",
       "      <td>-2.840000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.070000</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>4.660000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>3.640000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>4.310000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>4.480000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>4.260000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>3.460000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.890000</td>\n",
       "      <td>25.510000</td>\n",
       "      <td>32.380000</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>33.130000</td>\n",
       "      <td>18.220000</td>\n",
       "      <td>31.790000</td>\n",
       "      <td>29.010000</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>59.030000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.220000</td>\n",
       "      <td>23.380000</td>\n",
       "      <td>24.660000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>17.530000</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>27.380000</td>\n",
       "      <td>20.590000</td>\n",
       "      <td>19.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Food        Beer       Smoke       Games       Books       Hshld  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000  697.000000  697.000000   \n",
       "mean     0.688666    0.727030    0.985079    0.732095    0.532253    0.564333   \n",
       "std      4.308660    5.058992    6.032324    7.128170    5.780362    4.728000   \n",
       "min    -18.150000  -20.190000  -25.320000  -33.400000  -26.560000  -22.240000   \n",
       "25%     -1.630000   -2.080000   -2.740000   -3.390000   -2.600000   -2.030000   \n",
       "50%      0.740000    0.750000    1.270000    0.940000    0.510000    0.750000   \n",
       "75%      3.070000    3.690000    4.660000    5.260000    3.640000    3.540000   \n",
       "max     19.890000   25.510000   32.380000   34.520000   33.130000   18.220000   \n",
       "\n",
       "            Clths        Hlth       Chems       Txtls     ...      Telcm.lead  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000     ...      697.000000   \n",
       "mean     0.690387    0.665825    0.552367    0.687145     ...        0.515968   \n",
       "std      6.355251    4.897557    5.482363    6.970961     ...        4.607931   \n",
       "min    -31.500000  -21.060000  -28.600000  -33.110000     ...      -16.440000   \n",
       "25%     -2.800000   -2.230000   -2.750000   -3.170000     ...       -2.110000   \n",
       "50%      0.700000    0.760000    0.720000    0.640000     ...        0.590000   \n",
       "75%      4.310000    3.550000    3.760000    4.480000     ...        3.360000   \n",
       "max     31.790000   29.010000   21.680000   59.030000     ...       21.220000   \n",
       "\n",
       "       Servs.lead  BusEq.lead  Paper.lead  Trans.lead  Whlsl.lead  Rtail.lead  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000  697.000000  697.000000   \n",
       "mean     0.729928    0.622970    0.534806    0.601090    0.631076    0.698235   \n",
       "std      6.486956    6.698787    5.021876    5.707154    5.571040    5.334178   \n",
       "min    -28.670000  -32.070000  -27.740000  -28.500000  -29.250000  -29.740000   \n",
       "25%     -3.050000   -3.220000   -2.400000   -2.780000   -2.560000   -2.380000   \n",
       "50%      1.010000    0.670000    0.710000    0.900000    0.940000    0.540000   \n",
       "75%      4.260000    4.630000    3.460000    4.040000    3.880000    3.980000   \n",
       "max     23.380000   24.660000   21.000000   18.500000   17.530000   26.490000   \n",
       "\n",
       "       Meals.lead    Fin.lead  Other.lead  \n",
       "count  697.000000  697.000000  697.000000  \n",
       "mean     0.728766    0.637547    0.396628  \n",
       "std      6.065564    5.381389    5.771655  \n",
       "min    -31.890000  -22.530000  -28.090000  \n",
       "25%     -2.840000   -2.400000   -2.930000  \n",
       "50%      1.080000    0.870000    0.540000  \n",
       "75%      4.300000    4.000000    4.200000  \n",
       "max     27.380000   20.590000   19.960000  \n",
       "\n",
       "[8 rows x 133 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = data.describe()\n",
    "desc\n",
    "# min, max line up with Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 103)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run LASSO, then OLS on selected variables\n",
    "\n",
    "# skip last row to better match published r-squared\n",
    "# looks like they forecast actuals 1960-2016 using 1959m12 to 2016m11\n",
    "# not exact matches to Table 2 R-squared but almost within rounding error \n",
    "X = data.values[:-1,:npredictors]\n",
    "Y = data.values[:-1,-nresponses:]\n",
    "nrows = X.shape[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.49  -5.71  -2.05   1.21  -5.47  -7.84  -8.53  -6.68 -10.03  -4.77\n",
      "  -6.67  -9.38  -4.42 -12.3  -11.71  -5.03  -3.81  -7.91  -7.82  -2.4\n",
      "   0.62  -6.18  -7.93  -9.41  -4.31  -5.33  -6.09 -10.08  -4.68  -3.98]\n",
      "[13 14 27  8 23 11  6 22 17  5 18  7 10 21 26  1  4 25 15  9 28  0 12 24\n",
      " 29 16 19  2 20  3]\n",
      "[-12.3  -11.71 -10.08 -10.03  -9.41  -9.38  -8.53  -7.93  -7.91  -7.84\n",
      "  -7.82  -6.68  -6.67  -6.18  -6.09  -5.71  -5.47  -5.33  -5.03  -4.77\n",
      "  -4.68  -4.49  -4.42  -4.31  -3.98  -3.81  -2.4   -2.05   0.62   1.21]\n",
      "[0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 0. 2. 0. 2. 2. 0. 1. 0. 0. 1. 1. 0. 0. 2.\n",
      " 0. 0. 0. 2. 0. 1.]\n",
      "[-2.05, 1.21, -3.81, -2.4, 0.6199999999999999, -3.98]\n",
      "[]\n",
      "(696, 30)\n",
      "(696, 103)\n"
     ]
    }
   ],
   "source": [
    "# convert Ys to 3 classes\n",
    "# long = 1\n",
    "# short = 2\n",
    "# neither = 0\n",
    "ISLONG=1\n",
    "ISSHORT=2\n",
    "ISFLAT=0\n",
    "\n",
    "Y_sortindex = np.argsort(Y)\n",
    "print(Y[0])\n",
    "# sorted position\n",
    "print(Y_sortindex[0]) \n",
    "# sorted array\n",
    "print(Y[0,Y_sortindex[0]])\n",
    "# initialize class to 0\n",
    "Y_class=np.zeros_like(Y)\n",
    "for row in range(Y_class.shape[0]):\n",
    "    # if index in last 6, long\n",
    "    longlist = Y_sortindex[row,-6:]\n",
    "    Y_class[row, longlist]=ISLONG\n",
    "    # if index is in first 6, short\n",
    "    shortlist = Y_sortindex[row,:6]\n",
    "    Y_class[row, shortlist]=ISSHORT\n",
    "    \n",
    "print(Y_class[0])\n",
    "print([Y[0,i] for i in range(30) if Y_class[0,i]==1])\n",
    "print([Y[0,i] for i in range(30) if Y_class[0,i]==-1])\n",
    "print(Y_class.shape)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "layer 1 size 32, reg_penalty 0.00010000, dropout 0.333\n",
      "layer 2 size 32, reg_penalty 0.00010000, dropout 0.333\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 103)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dense01 (Dense)                 (None, 32)           3328        main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dropout00 (Dropout)             (None, 32)           0           Dense01[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense02 (Dense)                 (None, 32)           1056        Dropout00[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dropout01 (Dropout)             (None, 32)           0           Dense02[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output01 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output02 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output03 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output04 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output05 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output06 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output07 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output08 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output09 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output10 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output11 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output12 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output13 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output14 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output15 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output16 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output17 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output18 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output19 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output20 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output21 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output22 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output23 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output24 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output25 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output26 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output27 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output28 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output29 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Output30 (Dense)                (None, 3)            99          Dropout01[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,354\n",
      "Trainable params: 7,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 5s 7ms/step - loss: 72.5101 - Output01_loss: 2.4124 - Output02_loss: 2.1713 - Output03_loss: 2.1698 - Output04_loss: 2.4353 - Output05_loss: 2.1508 - Output06_loss: 2.8134 - Output07_loss: 2.3008 - Output08_loss: 2.7573 - Output09_loss: 2.8213 - Output10_loss: 2.3184 - Output11_loss: 1.9868 - Output12_loss: 2.1085 - Output13_loss: 2.1920 - Output14_loss: 1.7980 - Output15_loss: 3.0812 - Output16_loss: 2.9140 - Output17_loss: 2.2914 - Output18_loss: 2.8460 - Output19_loss: 3.1521 - Output20_loss: 2.3393 - Output21_loss: 3.3997 - Output22_loss: 2.1212 - Output23_loss: 2.9919 - Output24_loss: 1.7213 - Output25_loss: 1.8332 - Output26_loss: 1.9838 - Output27_loss: 2.1681 - Output28_loss: 2.5474 - Output29_loss: 2.0453 - Output30_loss: 2.5887 - Output01_acc: 0.3118 - Output02_acc: 0.3678 - Output03_acc: 0.3736 - Output04_acc: 0.3132 - Output05_acc: 0.3951 - Output06_acc: 0.2730 - Output07_acc: 0.4167 - Output08_acc: 0.3348 - Output09_acc: 0.2802 - Output10_acc: 0.4152 - Output11_acc: 0.4497 - Output12_acc: 0.3261 - Output13_acc: 0.4210 - Output14_acc: 0.4799 - Output15_acc: 0.2787 - Output16_acc: 0.2802 - Output17_acc: 0.3376 - Output18_acc: 0.3247 - Output19_acc: 0.2902 - Output20_acc: 0.3319 - Output21_acc: 0.2744 - Output22_acc: 0.4037 - Output23_acc: 0.2917 - Output24_acc: 0.4526 - Output25_acc: 0.5029 - Output26_acc: 0.3865 - Output27_acc: 0.3534 - Output28_acc: 0.3060 - Output29_acc: 0.3793 - Output30_acc: 0.2644\n",
      "Epoch 2/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 50.5311 - Output01_loss: 1.7009 - Output02_loss: 1.6931 - Output03_loss: 1.6301 - Output04_loss: 1.7558 - Output05_loss: 1.5344 - Output06_loss: 1.7869 - Output07_loss: 1.5729 - Output08_loss: 1.8206 - Output09_loss: 1.8600 - Output10_loss: 1.7692 - Output11_loss: 1.4036 - Output12_loss: 1.6876 - Output13_loss: 1.5086 - Output14_loss: 1.2992 - Output15_loss: 1.9769 - Output16_loss: 1.9414 - Output17_loss: 1.7706 - Output18_loss: 2.0032 - Output19_loss: 2.1317 - Output20_loss: 1.7438 - Output21_loss: 2.0700 - Output22_loss: 1.5424 - Output23_loss: 2.0244 - Output24_loss: 1.2215 - Output25_loss: 1.3740 - Output26_loss: 1.3313 - Output27_loss: 1.5984 - Output28_loss: 1.6787 - Output29_loss: 1.4414 - Output30_loss: 1.6096 - Output01_acc: 0.3908 - Output02_acc: 0.3736 - Output03_acc: 0.3549 - Output04_acc: 0.3233 - Output05_acc: 0.4483 - Output06_acc: 0.3549 - Output07_acc: 0.4368 - Output08_acc: 0.3779 - Output09_acc: 0.3578 - Output10_acc: 0.3793 - Output11_acc: 0.5187 - Output12_acc: 0.3491 - Output13_acc: 0.4784 - Output14_acc: 0.4943 - Output15_acc: 0.3089 - Output16_acc: 0.3175 - Output17_acc: 0.3376 - Output18_acc: 0.3463 - Output19_acc: 0.2960 - Output20_acc: 0.3075 - Output21_acc: 0.3003 - Output22_acc: 0.4626 - Output23_acc: 0.3060 - Output24_acc: 0.5374 - Output25_acc: 0.5029 - Output26_acc: 0.4526 - Output27_acc: 0.4023 - Output28_acc: 0.3707 - Output29_acc: 0.4713 - Output30_acc: 0.3649\n",
      "Epoch 3/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 42.0894 - Output01_loss: 1.4584 - Output02_loss: 1.4237 - Output03_loss: 1.4936 - Output04_loss: 1.5879 - Output05_loss: 1.3046 - Output06_loss: 1.5236 - Output07_loss: 1.3573 - Output08_loss: 1.4685 - Output09_loss: 1.4098 - Output10_loss: 1.5243 - Output11_loss: 1.1329 - Output12_loss: 1.4365 - Output13_loss: 1.2201 - Output14_loss: 1.2393 - Output15_loss: 1.5272 - Output16_loss: 1.4878 - Output17_loss: 1.5684 - Output18_loss: 1.6248 - Output19_loss: 1.4966 - Output20_loss: 1.4273 - Output21_loss: 1.5651 - Output22_loss: 1.3550 - Output23_loss: 1.6292 - Output24_loss: 1.1167 - Output25_loss: 1.2086 - Output26_loss: 1.2187 - Output27_loss: 1.2883 - Output28_loss: 1.4397 - Output29_loss: 1.2052 - Output30_loss: 1.3019 - Output01_acc: 0.3980 - Output02_acc: 0.3736 - Output03_acc: 0.3333 - Output04_acc: 0.3477 - Output05_acc: 0.4727 - Output06_acc: 0.3807 - Output07_acc: 0.4583 - Output08_acc: 0.4052 - Output09_acc: 0.4066 - Output10_acc: 0.4239 - Output11_acc: 0.5833 - Output12_acc: 0.3736 - Output13_acc: 0.5115 - Output14_acc: 0.4842 - Output15_acc: 0.3506 - Output16_acc: 0.3822 - Output17_acc: 0.3218 - Output18_acc: 0.3448 - Output19_acc: 0.3563 - Output20_acc: 0.3563 - Output21_acc: 0.3391 - Output22_acc: 0.4756 - Output23_acc: 0.3305 - Output24_acc: 0.6307 - Output25_acc: 0.5043 - Output26_acc: 0.5043 - Output27_acc: 0.4684 - Output28_acc: 0.4095 - Output29_acc: 0.5057 - Output30_acc: 0.4353\n",
      "Epoch 4/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 37.3638 - Output01_loss: 1.2319 - Output02_loss: 1.2553 - Output03_loss: 1.2876 - Output04_loss: 1.3534 - Output05_loss: 1.0968 - Output06_loss: 1.2940 - Output07_loss: 1.2026 - Output08_loss: 1.3950 - Output09_loss: 1.2295 - Output10_loss: 1.3864 - Output11_loss: 1.0382 - Output12_loss: 1.2374 - Output13_loss: 1.1078 - Output14_loss: 1.1213 - Output15_loss: 1.3612 - Output16_loss: 1.3187 - Output17_loss: 1.3947 - Output18_loss: 1.4156 - Output19_loss: 1.3632 - Output20_loss: 1.3215 - Output21_loss: 1.3320 - Output22_loss: 1.2811 - Output23_loss: 1.3435 - Output24_loss: 0.9972 - Output25_loss: 1.1327 - Output26_loss: 1.0600 - Output27_loss: 1.1768 - Output28_loss: 1.2888 - Output29_loss: 1.1050 - Output30_loss: 1.1865 - Output01_acc: 0.4928 - Output02_acc: 0.4454 - Output03_acc: 0.3851 - Output04_acc: 0.3707 - Output05_acc: 0.5330 - Output06_acc: 0.4282 - Output07_acc: 0.4756 - Output08_acc: 0.4181 - Output09_acc: 0.4899 - Output10_acc: 0.4095 - Output11_acc: 0.5920 - Output12_acc: 0.4195 - Output13_acc: 0.5647 - Output14_acc: 0.5115 - Output15_acc: 0.3477 - Output16_acc: 0.4080 - Output17_acc: 0.3333 - Output18_acc: 0.3463 - Output19_acc: 0.3649 - Output20_acc: 0.3534 - Output21_acc: 0.3779 - Output22_acc: 0.4440 - Output23_acc: 0.4052 - Output24_acc: 0.6351 - Output25_acc: 0.5144 - Output26_acc: 0.5618 - Output27_acc: 0.4899 - Output28_acc: 0.4239 - Output29_acc: 0.5345 - Output30_acc: 0.4856\n",
      "Epoch 5/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 34.6080 - Output01_loss: 1.1130 - Output02_loss: 1.2390 - Output03_loss: 1.2705 - Output04_loss: 1.2286 - Output05_loss: 1.0459 - Output06_loss: 1.1755 - Output07_loss: 1.1285 - Output08_loss: 1.2278 - Output09_loss: 1.1204 - Output10_loss: 1.2019 - Output11_loss: 1.0036 - Output12_loss: 1.2135 - Output13_loss: 1.0053 - Output14_loss: 1.0657 - Output15_loss: 1.2498 - Output16_loss: 1.1816 - Output17_loss: 1.2717 - Output18_loss: 1.2640 - Output19_loss: 1.2786 - Output20_loss: 1.2323 - Output21_loss: 1.2525 - Output22_loss: 1.2273 - Output23_loss: 1.2197 - Output24_loss: 0.8790 - Output25_loss: 1.0586 - Output26_loss: 1.0350 - Output27_loss: 1.1102 - Output28_loss: 1.1357 - Output29_loss: 1.0547 - Output30_loss: 1.0706 - Output01_acc: 0.5302 - Output02_acc: 0.4325 - Output03_acc: 0.3506 - Output04_acc: 0.3879 - Output05_acc: 0.5402 - Output06_acc: 0.4957 - Output07_acc: 0.4856 - Output08_acc: 0.4626 - Output09_acc: 0.5172 - Output10_acc: 0.4483 - Output11_acc: 0.6279 - Output12_acc: 0.4080 - Output13_acc: 0.6006 - Output14_acc: 0.5287 - Output15_acc: 0.3664 - Output16_acc: 0.4497 - Output17_acc: 0.3463 - Output18_acc: 0.3534 - Output19_acc: 0.3894 - Output20_acc: 0.4009 - Output21_acc: 0.3922 - Output22_acc: 0.4698 - Output23_acc: 0.4253 - Output24_acc: 0.6868 - Output25_acc: 0.5575 - Output26_acc: 0.5977 - Output27_acc: 0.5144 - Output28_acc: 0.4698 - Output29_acc: 0.5862 - Output30_acc: 0.5632\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 2s 3ms/step - loss: 33.3468 - Output01_loss: 1.0784 - Output02_loss: 1.1506 - Output03_loss: 1.2420 - Output04_loss: 1.2229 - Output05_loss: 1.0873 - Output06_loss: 1.0671 - Output07_loss: 1.0958 - Output08_loss: 1.1648 - Output09_loss: 1.0512 - Output10_loss: 1.1724 - Output11_loss: 0.9743 - Output12_loss: 1.1437 - Output13_loss: 0.9849 - Output14_loss: 1.0587 - Output15_loss: 1.2056 - Output16_loss: 1.1096 - Output17_loss: 1.2633 - Output18_loss: 1.2486 - Output19_loss: 1.1956 - Output20_loss: 1.1937 - Output21_loss: 1.1497 - Output22_loss: 1.1324 - Output23_loss: 1.1835 - Output24_loss: 0.8789 - Output25_loss: 1.0481 - Output26_loss: 0.9542 - Output27_loss: 1.0433 - Output28_loss: 1.1477 - Output29_loss: 1.0179 - Output30_loss: 1.0331 - Output01_acc: 0.5575 - Output02_acc: 0.4555 - Output03_acc: 0.3707 - Output04_acc: 0.4224 - Output05_acc: 0.5647 - Output06_acc: 0.5402 - Output07_acc: 0.4986 - Output08_acc: 0.4655 - Output09_acc: 0.5503 - Output10_acc: 0.4555 - Output11_acc: 0.6509 - Output12_acc: 0.4353 - Output13_acc: 0.6164 - Output14_acc: 0.5316 - Output15_acc: 0.4095 - Output16_acc: 0.4741 - Output17_acc: 0.3420 - Output18_acc: 0.3448 - Output19_acc: 0.4195 - Output20_acc: 0.4009 - Output21_acc: 0.4382 - Output22_acc: 0.5101 - Output23_acc: 0.4353 - Output24_acc: 0.7040 - Output25_acc: 0.5618 - Output26_acc: 0.6365 - Output27_acc: 0.5417 - Output28_acc: 0.4713 - Output29_acc: 0.6106 - Output30_acc: 0.5747\n",
      "Epoch 7/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 31.9092 - Output01_loss: 1.0290 - Output02_loss: 1.1330 - Output03_loss: 1.1781 - Output04_loss: 1.1624 - Output05_loss: 1.0025 - Output06_loss: 1.0408 - Output07_loss: 1.0778 - Output08_loss: 1.1171 - Output09_loss: 0.9666 - Output10_loss: 1.1165 - Output11_loss: 0.9131 - Output12_loss: 1.0791 - Output13_loss: 0.9854 - Output14_loss: 1.0130 - Output15_loss: 1.1379 - Output16_loss: 1.0756 - Output17_loss: 1.2383 - Output18_loss: 1.1760 - Output19_loss: 1.1322 - Output20_loss: 1.1234 - Output21_loss: 1.1146 - Output22_loss: 1.0990 - Output23_loss: 1.1399 - Output24_loss: 0.8663 - Output25_loss: 0.9949 - Output26_loss: 0.9019 - Output27_loss: 1.0154 - Output28_loss: 1.0796 - Output29_loss: 0.9683 - Output30_loss: 0.9841 - Output01_acc: 0.5661 - Output02_acc: 0.4713 - Output03_acc: 0.3635 - Output04_acc: 0.4009 - Output05_acc: 0.5675 - Output06_acc: 0.5546 - Output07_acc: 0.5057 - Output08_acc: 0.4885 - Output09_acc: 0.5920 - Output10_acc: 0.4598 - Output11_acc: 0.6710 - Output12_acc: 0.4583 - Output13_acc: 0.6207 - Output14_acc: 0.5503 - Output15_acc: 0.4425 - Output16_acc: 0.5101 - Output17_acc: 0.3103 - Output18_acc: 0.3592 - Output19_acc: 0.4210 - Output20_acc: 0.4267 - Output21_acc: 0.4612 - Output22_acc: 0.5517 - Output23_acc: 0.4382 - Output24_acc: 0.7256 - Output25_acc: 0.5876 - Output26_acc: 0.6940 - Output27_acc: 0.5575 - Output28_acc: 0.5216 - Output29_acc: 0.6523 - Output30_acc: 0.6178\n",
      "Epoch 8/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 31.3414 - Output01_loss: 0.9954 - Output02_loss: 1.0922 - Output03_loss: 1.1937 - Output04_loss: 1.1557 - Output05_loss: 0.9873 - Output06_loss: 1.0003 - Output07_loss: 1.0510 - Output08_loss: 1.0677 - Output09_loss: 0.9834 - Output10_loss: 1.1201 - Output11_loss: 0.8867 - Output12_loss: 1.1024 - Output13_loss: 0.9683 - Output14_loss: 1.0102 - Output15_loss: 1.1462 - Output16_loss: 1.0607 - Output17_loss: 1.1629 - Output18_loss: 1.1744 - Output19_loss: 1.1312 - Output20_loss: 1.1126 - Output21_loss: 1.0764 - Output22_loss: 1.0759 - Output23_loss: 1.0837 - Output24_loss: 0.8694 - Output25_loss: 1.0007 - Output26_loss: 0.8589 - Output27_loss: 0.9816 - Output28_loss: 1.0225 - Output29_loss: 0.9586 - Output30_loss: 0.9645 - Output01_acc: 0.6049 - Output02_acc: 0.4914 - Output03_acc: 0.3420 - Output04_acc: 0.4253 - Output05_acc: 0.5934 - Output06_acc: 0.6164 - Output07_acc: 0.5201 - Output08_acc: 0.5273 - Output09_acc: 0.5905 - Output10_acc: 0.4670 - Output11_acc: 0.7083 - Output12_acc: 0.4497 - Output13_acc: 0.6523 - Output14_acc: 0.5546 - Output15_acc: 0.4684 - Output16_acc: 0.5345 - Output17_acc: 0.3592 - Output18_acc: 0.3793 - Output19_acc: 0.4483 - Output20_acc: 0.4353 - Output21_acc: 0.4813 - Output22_acc: 0.5417 - Output23_acc: 0.4698 - Output24_acc: 0.7414 - Output25_acc: 0.5905 - Output26_acc: 0.7213 - Output27_acc: 0.6006 - Output28_acc: 0.5445 - Output29_acc: 0.6595 - Output30_acc: 0.6336\n",
      "Epoch 9/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 30.4908 - Output01_loss: 0.9799 - Output02_loss: 1.0778 - Output03_loss: 1.1212 - Output04_loss: 1.1074 - Output05_loss: 0.9595 - Output06_loss: 0.9773 - Output07_loss: 1.0144 - Output08_loss: 1.0373 - Output09_loss: 0.9284 - Output10_loss: 1.0757 - Output11_loss: 0.8691 - Output12_loss: 1.0916 - Output13_loss: 0.9411 - Output14_loss: 0.9982 - Output15_loss: 1.1010 - Output16_loss: 1.0243 - Output17_loss: 1.1708 - Output18_loss: 1.1511 - Output19_loss: 1.0873 - Output20_loss: 1.0974 - Output21_loss: 1.0695 - Output22_loss: 1.0560 - Output23_loss: 1.0591 - Output24_loss: 0.8016 - Output25_loss: 0.9720 - Output26_loss: 0.8482 - Output27_loss: 0.9463 - Output28_loss: 1.0340 - Output29_loss: 0.9208 - Output30_loss: 0.9258 - Output01_acc: 0.6135 - Output02_acc: 0.4957 - Output03_acc: 0.3822 - Output04_acc: 0.4511 - Output05_acc: 0.6264 - Output06_acc: 0.6293 - Output07_acc: 0.5589 - Output08_acc: 0.5402 - Output09_acc: 0.6695 - Output10_acc: 0.4828 - Output11_acc: 0.7098 - Output12_acc: 0.4583 - Output13_acc: 0.6638 - Output14_acc: 0.5848 - Output15_acc: 0.4756 - Output16_acc: 0.5560 - Output17_acc: 0.3520 - Output18_acc: 0.3520 - Output19_acc: 0.4612 - Output20_acc: 0.4497 - Output21_acc: 0.5057 - Output22_acc: 0.5560 - Output23_acc: 0.5086 - Output24_acc: 0.7586 - Output25_acc: 0.6078 - Output26_acc: 0.7356 - Output27_acc: 0.6293 - Output28_acc: 0.5474 - Output29_acc: 0.6925 - Output30_acc: 0.6466\n",
      "Epoch 10/100\n",
      "696/696 [==============================] - 2s 3ms/step - loss: 30.1152 - Output01_loss: 0.9649 - Output02_loss: 1.0456 - Output03_loss: 1.1168 - Output04_loss: 1.1144 - Output05_loss: 0.9709 - Output06_loss: 1.0002 - Output07_loss: 1.0021 - Output08_loss: 1.0188 - Output09_loss: 0.9097 - Output10_loss: 1.1022 - Output11_loss: 0.8541 - Output12_loss: 1.0943 - Output13_loss: 0.9118 - Output14_loss: 0.9787 - Output15_loss: 1.0773 - Output16_loss: 1.0253 - Output17_loss: 1.1146 - Output18_loss: 1.1478 - Output19_loss: 1.0812 - Output20_loss: 1.0808 - Output21_loss: 1.0420 - Output22_loss: 1.0119 - Output23_loss: 1.0248 - Output24_loss: 0.8017 - Output25_loss: 0.9830 - Output26_loss: 0.8362 - Output27_loss: 0.9441 - Output28_loss: 1.0034 - Output29_loss: 0.8950 - Output30_loss: 0.9149 - Output01_acc: 0.6379 - Output02_acc: 0.5287 - Output03_acc: 0.3951 - Output04_acc: 0.4382 - Output05_acc: 0.6336 - Output06_acc: 0.6336 - Output07_acc: 0.5632 - Output08_acc: 0.5618 - Output09_acc: 0.6782 - Output10_acc: 0.5115 - Output11_acc: 0.7184 - Output12_acc: 0.4497 - Output13_acc: 0.6652 - Output14_acc: 0.5891 - Output15_acc: 0.4856 - Output16_acc: 0.5632 - Output17_acc: 0.3822 - Output18_acc: 0.3678 - Output19_acc: 0.4684 - Output20_acc: 0.4598 - Output21_acc: 0.5201 - Output22_acc: 0.5718 - Output23_acc: 0.5101 - Output24_acc: 0.7629 - Output25_acc: 0.6236 - Output26_acc: 0.7443 - Output27_acc: 0.6279 - Output28_acc: 0.5761 - Output29_acc: 0.6925 - Output30_acc: 0.6724\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 2s 3ms/step - loss: 29.7827 - Output01_loss: 0.9248 - Output02_loss: 1.0548 - Output03_loss: 1.1099 - Output04_loss: 1.1013 - Output05_loss: 0.9423 - Output06_loss: 0.9588 - Output07_loss: 1.0162 - Output08_loss: 0.9895 - Output09_loss: 0.8921 - Output10_loss: 1.0671 - Output11_loss: 0.8381 - Output12_loss: 1.0599 - Output13_loss: 0.9337 - Output14_loss: 0.9661 - Output15_loss: 1.0656 - Output16_loss: 0.9783 - Output17_loss: 1.1420 - Output18_loss: 1.1452 - Output19_loss: 1.0795 - Output20_loss: 1.0769 - Output21_loss: 1.0173 - Output22_loss: 1.0199 - Output23_loss: 1.0309 - Output24_loss: 0.7797 - Output25_loss: 0.9545 - Output26_loss: 0.8246 - Output27_loss: 0.9414 - Output28_loss: 1.0059 - Output29_loss: 0.9119 - Output30_loss: 0.9081 - Output01_acc: 0.6437 - Output02_acc: 0.5244 - Output03_acc: 0.3851 - Output04_acc: 0.4454 - Output05_acc: 0.6365 - Output06_acc: 0.6523 - Output07_acc: 0.5718 - Output08_acc: 0.5747 - Output09_acc: 0.6897 - Output10_acc: 0.4943 - Output11_acc: 0.7213 - Output12_acc: 0.4770 - Output13_acc: 0.6825 - Output14_acc: 0.6178 - Output15_acc: 0.5072 - Output16_acc: 0.5805 - Output17_acc: 0.3606 - Output18_acc: 0.3578 - Output19_acc: 0.4842 - Output20_acc: 0.4670 - Output21_acc: 0.5431 - Output22_acc: 0.5733 - Output23_acc: 0.5273 - Output24_acc: 0.7672 - Output25_acc: 0.6236 - Output26_acc: 0.7557 - Output27_acc: 0.6322 - Output28_acc: 0.5575 - Output29_acc: 0.6868 - Output30_acc: 0.6839\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-c16238b9d70c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponselist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_DIM = X.shape[1]\n",
    "print(INPUT_DIM)\n",
    "OUTPUT_DIM = len(responses) # 30\n",
    "NCLASSES=3\n",
    "EPOCHS=100\n",
    "BATCH_SIZE=32\n",
    "\n",
    "def build_model(n_hidden_layers = 2,\n",
    "                hidden_layer_size = 32,\n",
    "                reg_penalty = 0.0001,\n",
    "                dropout = 0.25,\n",
    "                verbose=True):\n",
    "\n",
    "    main_input = Input(shape=(INPUT_DIM,), \n",
    "                       dtype='float32', \n",
    "                       name='main_input')\n",
    "    lastlayer=main_input\n",
    "\n",
    "    for i in range(n_hidden_layers):\n",
    "        if verbose:\n",
    "            print(\"layer %d size %d, reg_penalty %.8f, dropout %.3f\" % (i+1, hidden_layer_size, reg_penalty, dropout))\n",
    "        lastlayer = Dense(units = hidden_layer_size, \n",
    "                          activation = 'relu',\n",
    "                          kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                          kernel_regularizer=keras.regularizers.l1(reg_penalty),\n",
    "                          name = \"Dense%02d\" % (i+1))(lastlayer)\n",
    "\n",
    "        if dropout:\n",
    "            lastlayer = Dropout(dropout, name = \"Dropout%02d\" % i)(lastlayer)\n",
    "\n",
    "    outputs=[]\n",
    "    for i in range(OUTPUT_DIM):\n",
    "        outputs.append (Dense(NCLASSES, \n",
    "                              activation='softmax',\n",
    "                              name = \"Output%02d\" % (i+1))(lastlayer))\n",
    "    \n",
    "    model = Model(inputs=[main_input], outputs=outputs)\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer=\"rmsprop\", \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "responselist=[]\n",
    "for i in range(OUTPUT_DIM):\n",
    "    one_hot_labels = keras.utils.to_categorical(Y_class[:,i], num_classes=NCLASSES)\n",
    "    responselist.append(one_hot_labels)\n",
    "    \n",
    "# Train the model\n",
    "model = build_model()\n",
    "model.fit(X, responselist, epochs=EPOCHS, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 30)\n",
      "(696, 103)\n"
     ]
    }
   ],
   "source": [
    "print(Y_class.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73989958 0.65319115 0.37609512 0.40323266 0.80311286 0.74617505\n",
      " 0.6212942  0.64905483 0.72370356 0.59356982 0.68947989 0.47099158\n",
      " 0.76194555 0.60890096 0.5783779  0.64159554 0.34791061 0.31595203\n",
      " 0.50655687 0.50013244 0.5098682  0.51675969 0.55720091 0.75047684\n",
      " 0.63351041 0.73421443 0.68504792 0.42430332 0.65795249 0.61809278]\n",
      "[0.19224735 0.22811031 0.4466413  0.24164565 0.08169506 0.12297722\n",
      " 0.18395223 0.23416008 0.12024333 0.17163867 0.1214458  0.18858837\n",
      " 0.09768605 0.1967954  0.14716461 0.23285027 0.34185773 0.34976387\n",
      " 0.29882261 0.34258679 0.27609351 0.18421379 0.14238672 0.0820222\n",
      " 0.13043307 0.12787752 0.17103112 0.3213014  0.16271211 0.21429649]\n",
      "[0.06785309 0.11869853 0.1772636  0.35512167 0.11519214 0.13084772\n",
      " 0.19475353 0.11678501 0.15605308 0.23479158 0.18907437 0.34042007\n",
      " 0.14036842 0.19430368 0.27445748 0.12555422 0.31023166 0.33428407\n",
      " 0.19462046 0.15728082 0.21403825 0.29902649 0.30041239 0.16750097\n",
      " 0.23605649 0.13790806 0.14392091 0.25439528 0.17933537 0.16761073]\n",
      "[1.00000001 0.99999999 1.00000001 0.99999999 1.00000006 0.99999999\n",
      " 0.99999996 0.99999992 0.99999997 1.00000006 1.00000005 1.00000001\n",
      " 1.00000001 1.00000004 1.         1.00000003 1.         0.99999997\n",
      " 0.99999994 1.00000004 0.99999997 0.99999997 1.00000001 1.00000001\n",
      " 0.99999997 1.         0.99999996 1.         0.99999997 1.        ]\n"
     ]
    }
   ],
   "source": [
    "z=model.predict(X)\n",
    "\n",
    "longprobs = np.zeros([nrows, OUTPUT_DIM])\n",
    "shortprobs = np.zeros([nrows, OUTPUT_DIM])\n",
    "flatprobs = np.zeros([nrows, OUTPUT_DIM])\n",
    "\n",
    "for response in range(OUTPUT_DIM):\n",
    "    for row in range(nrows):\n",
    "        longprobs[row, response] = z[response][row, ISLONG]\n",
    "        shortprobs[row, response] = z[response][row, ISSHORT]\n",
    "        flatprobs[row, response] = z[response][row, ISFLAT]\n",
    "\n",
    "i=0\n",
    "print(flatprobs[i])\n",
    "print(longprobs[i])\n",
    "print(shortprobs[i])\n",
    "print(flatprobs[i] + longprobs[i] + shortprobs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:24:56 Start fit\n",
      "Fit on 693 rows 0 to 692\n",
      "Predict on 3 rows 693 to 695\n",
      "07:25:03 End fit\n",
      "[0.70094669 0.51267409 0.31555802 0.44605982 0.59227812 0.7188375\n",
      " 0.55687535 0.50699764 0.69770986 0.37999946 0.65319031 0.36464691\n",
      " 0.60252029 0.42291167 0.53504634 0.58563578 0.42592576 0.2714169\n",
      " 0.46737468 0.48804164 0.47585663 0.43772307 0.41322729 0.71564108\n",
      " 0.62680173 0.70730972 0.51187742 0.54305059 0.6116854  0.55277431]\n",
      "[0.16946118 0.25575674 0.3817496  0.20946449 0.23679402 0.15219443\n",
      " 0.25443757 0.22677046 0.16205129 0.37216562 0.18914914 0.27369422\n",
      " 0.21843782 0.38384637 0.20601651 0.18339613 0.20547722 0.40441355\n",
      " 0.22401495 0.25103316 0.34085706 0.32618108 0.29546684 0.13189632\n",
      " 0.2010155  0.15134814 0.30073386 0.15996478 0.14782324 0.20963971]\n",
      "[0.12959212 0.23156922 0.30269241 0.34447569 0.17092781 0.1289681\n",
      " 0.1886871  0.26623181 0.14023887 0.24783494 0.15766062 0.36165884\n",
      " 0.17904189 0.19324192 0.25893721 0.23096813 0.36859697 0.32416952\n",
      " 0.30861044 0.26092517 0.18328634 0.23609586 0.29130593 0.15246253\n",
      " 0.17218278 0.14134209 0.18738864 0.29698458 0.24049135 0.23758595]\n",
      "[0.99999999 1.00000004 1.00000003 1.         0.99999996 1.00000003\n",
      " 1.00000001 0.99999991 1.00000001 1.00000001 1.00000007 0.99999997\n",
      " 1.         0.99999997 1.00000006 1.00000004 0.99999996 0.99999997\n",
      " 1.00000007 0.99999997 1.00000003 1.00000001 1.00000006 0.99999993\n",
      " 1.00000001 0.99999996 0.99999993 0.99999996 0.99999999 0.99999997]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "BATCH_SIZE=32\n",
    "\n",
    "def fit_predict(X, Y, model, epochs=EPOCHS, npredict=1, verbose=False):\n",
    "    \"\"\"for backtest, train model using Y_list v. X using n-npredict rows\n",
    "    generate npredict prediction Y_list using last npredict rows of X\n",
    "    if npredict=1, fit using n-1 rows, return prediction using X for final month\n",
    "    if npredict=26, fit using n-26 rows, return prediction using X for final 26 months\"\"\"\n",
    "    \n",
    "    nrows, ncols = X.shape\n",
    "    if verbose:\n",
    "        print(\"Fit on %d rows 0 to %d\" % (nrows-npredict, nrows-npredict-1))\n",
    "        print(\"Predict on %d rows %d to %d\" % (npredict, nrows-npredict, nrows-1))\n",
    "        \n",
    "    # keep last rows to predict against\n",
    "    X_predict = X[-npredict:]\n",
    "    X_predict = X_predict.reshape(npredict,X.shape[1])\n",
    "    # fit on remaining rows\n",
    "    X_fit = X[:-npredict]\n",
    "    Y_fit = Y[:-npredict]\n",
    "    \n",
    "    # make a list of Ys expected by Keras\n",
    "    Y_list = []\n",
    "    for i in range(OUTPUT_DIM):\n",
    "        Y_list.append(keras.utils.to_categorical(Y_fit[:,i], num_classes=NCLASSES))\n",
    "         \n",
    "    fit = model.fit(\n",
    "        X_fit,\n",
    "        Y_list,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=epochs,\n",
    "        verbose=0)\n",
    "\n",
    "    Z = model.predict(X_predict)\n",
    "    \n",
    "    longprobs = np.zeros([npredict, OUTPUT_DIM])\n",
    "    shortprobs = np.zeros([npredict, OUTPUT_DIM])\n",
    "    flatprobs = np.zeros([npredict, OUTPUT_DIM])\n",
    "\n",
    "    for response in range(OUTPUT_DIM):\n",
    "        for row in range(npredict):\n",
    "            longprobs[row, response] = Z[response][row, ISLONG]\n",
    "            shortprobs[row, response] = Z[response][row, ISSHORT]\n",
    "            flatprobs[row, response] = Z[response][row, ISFLAT]\n",
    "    \n",
    "    return flatprobs, longprobs, shortprobs\n",
    "\n",
    "print(\"%s Start fit\" % (time.strftime(\"%H:%M:%S\")))\n",
    "flatprobs, longprobs, shortprobs = fit_predict(X, Y_class, model,epochs=3,npredict=3, verbose=True)\n",
    "print(\"%s End fit\" % (time.strftime(\"%H:%M:%S\")))\n",
    "\n",
    "i=0\n",
    "print(flatprobs[i])\n",
    "print(longprobs[i])\n",
    "print(shortprobs[i])\n",
    "print(flatprobs[i] + longprobs[i] + shortprobs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.34000000e+00 -1.95000000e+00 -7.59000000e+00 -7.76000000e+00\n",
      " -1.20500000e+01 -7.50000000e+00 -5.69000000e+00 -7.71000000e+00\n",
      " -7.37000000e+00 -5.26000000e+00 -9.84000000e+00 -6.31000000e+00\n",
      " -7.15000000e+00 -6.89000000e+00 -9.35000000e+00 -1.24900000e+01\n",
      " -2.34000000e+00 -7.70000000e-01 -1.21600000e+01 -4.83000000e+00\n",
      " -3.16000000e+00 -1.11700000e+01 -9.73000000e+00 -8.89000000e+00\n",
      " -8.17000000e+00 -8.28000000e+00 -6.31000000e+00 -1.31200000e+01\n",
      " -9.78000000e+00 -6.20000000e+00  5.00000000e-02  1.40000000e-01\n",
      " -8.00000000e-02  8.33333333e-02 -8.10000000e+00 -2.28666667e+00\n",
      " -2.18000000e+00 -3.36000000e+00 -7.00333333e+00 -6.82000000e+00\n",
      " -3.37666667e+00 -5.32666667e+00 -1.41000000e+00 -5.58666667e+00\n",
      " -5.43333333e+00 -6.02666667e+00 -4.45000000e+00 -4.68333333e+00\n",
      " -4.67666667e+00 -5.93666667e+00 -9.37000000e+00 -2.48000000e+00\n",
      "  2.38666667e+00 -6.98666667e+00 -3.96666667e+00 -2.88333333e+00\n",
      " -4.91666667e+00 -4.43000000e+00 -4.87666667e+00 -8.43000000e+00\n",
      " -6.69666667e+00 -4.78333333e+00 -7.30333333e+00 -6.68333333e+00\n",
      " -6.49000000e+00  2.90000000e-01  2.30000000e-01 -1.16666667e-01\n",
      " -4.84000000e+00 -9.10833333e-01 -2.67500000e-01 -1.33000000e+00\n",
      " -2.70416667e+00 -2.59416667e+00 -5.53333333e-01 -3.83000000e+00\n",
      "  7.08333333e-01 -3.15416667e+00 -3.34500000e+00 -2.33333333e+00\n",
      " -2.93750000e+00 -2.89416667e+00 -1.85166667e+00 -2.21083333e+00\n",
      " -5.60750000e+00 -2.63666667e+00 -8.89166667e-01 -3.62333333e+00\n",
      " -2.29916667e+00 -1.30000000e+00 -1.72083333e+00 -4.71666667e-01\n",
      " -1.09000000e+00 -4.73666667e+00 -3.74666667e+00 -8.60833333e-01\n",
      " -2.40916667e+00 -2.34833333e+00 -3.80000000e+00  1.44166667e-01\n",
      "  1.45833333e-01  5.83333333e-03 -1.98166667e+00]\n",
      "Food     -3.34\n",
      "Beer     -1.95\n",
      "Smoke    -7.59\n",
      "Games    -7.76\n",
      "Books   -12.05\n",
      "Hshld    -7.50\n",
      "Clths    -5.69\n",
      "Hlth     -7.71\n",
      "Chems    -7.37\n",
      "Txtls    -5.26\n",
      "Cnstr    -9.84\n",
      "Steel    -6.31\n",
      "FabPr    -7.15\n",
      "ElcEq    -6.89\n",
      "Autos    -9.35\n",
      "Carry   -12.49\n",
      "Mines    -2.34\n",
      "Coal     -0.77\n",
      "Oil     -12.16\n",
      "Util     -4.83\n",
      "Telcm    -3.16\n",
      "Servs   -11.17\n",
      "BusEq    -9.73\n",
      "Paper    -8.89\n",
      "Trans    -8.17\n",
      "Whlsl    -8.28\n",
      "Rtail    -6.31\n",
      "Meals   -13.12\n",
      "Fin      -9.78\n",
      "Other    -6.20\n",
      "Name: 197001, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 197001 = 121\n",
    "STARTMONTH = 121\n",
    "print(X[STARTMONTH])\n",
    "print(data.iloc[STARTMONTH][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all months starting STARTMONTH\n",
    "# initialize predictions matrix P\n",
    "\n",
    "EPOCHS=500\n",
    "\n",
    "nrows = X.shape[0]\n",
    "startindex = 1000\n",
    "\n",
    "def run_backtest(X, Y, arg_dict, startindex=0, epochs=EPOCHS, step=1, minmaxscale=False, standardscale=False):\n",
    "    \"\"\"create keras model; add step, to iteratively train, predict 12 months, train up to next 12 months \"\"\"\n",
    "    \n",
    "    global P_long\n",
    "    global P_short\n",
    "    global P_flat\n",
    "    global R \n",
    "    \n",
    "\n",
    "    print(\"%s Starting backtest\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    count = 0\n",
    "    nrows, ncols = X.shape\n",
    "    P_long = np.zeros([nrows,OUTPUT_DIM])\n",
    "    P_short = np.zeros([nrows,OUTPUT_DIM])\n",
    "    P_flat = np.zeros([nrows,OUTPUT_DIM])\n",
    "\n",
    "    Xscale = X.copy()\n",
    "    Yscale = Y.copy()\n",
    "    \n",
    "    if minmaxscale:\n",
    "        # minmaxscale each row (min->0, max->1) - transpose, scale, transpose back because scales by columns\n",
    "        Xscale = MinMaxScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        print(\"using MinMaxScaler\")\n",
    "    elif standardscale:\n",
    "        # standardize each row (mean->0, SD->1)- transpose, scale, transpose back because scales by columns\n",
    "        Xscale = StandardScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        print(\"using StandardScaler\")\n",
    "     \n",
    "    for train_index in range(startindex, nrows, step):\n",
    "        # don't exceed bounds\n",
    "        train_index = max(train_index, nrows-step)\n",
    "        fp_index = train_index + step # eg 1000 + 26 = 1026\n",
    "            \n",
    "        model = build_model(n_hidden_layers = arg_dict[\"n_hidden_layers\"],\n",
    "                            hidden_layer_size = arg_dict[\"hidden_layer_size\"], \n",
    "                            reg_penalty = arg_dict[\"reg_penalty\"], \n",
    "                            dropout = arg_dict[\"dropout\"],\n",
    "                            verbose=arg_dict[\"verbose\"])\n",
    "        \n",
    "        # fit on e.g. 0:999, predict 1000-1025\n",
    "        flatprobs, longprobs, shortprobs = fit_predict(Xscale[:fp_index, :], \n",
    "                                                       Yscale[:fp_index], \n",
    "                                                       model,\n",
    "                                                       epochs=epochs,\n",
    "                                                       npredict=step)\n",
    "        \n",
    "        # store in 1000:1025 - lining up with future Xs not current X/Ys\n",
    "        for i in range(step):\n",
    "            if train_index + i >= nrows:\n",
    "                break\n",
    "            P_flat[train_index + i]= flatprobs[i]\n",
    "            P_long[train_index + i]= longprobs[i]\n",
    "            P_short[train_index + i]= shortprobs[i]\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training %d of %d\" % (time.strftime(\"%H:%M:%S\"), count, nrows-startindex))\n",
    "            sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_returns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-59017e075927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTARTMONTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_returns' is not defined"
     ]
    }
   ],
   "source": [
    "gen(STARTMONTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:37:12 Starting backtest\n",
      "................................................................................\n",
      "09:25:01 Still training 80 of 575\n",
      "................................................................................\n",
      "11:13:34 Still training 160 of 575\n",
      "................................................................................\n",
      "12:46:59 Still training 240 of 575\n",
      "................................................................................\n",
      "14:37:14 Still training 320 of 575\n",
      "................................................................................\n",
      "16:28:16 Still training 400 of 575\n",
      "................................................................................\n",
      "18:04:15 Still training 480 of 575\n",
      "................................................................................\n",
      "19:56:59 Still training 560 of 575\n",
      "..............."
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 696 is out of bounds for axis 0 with size 696",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-7c1d86e8e376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#model = build_model(**arg_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_backtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#gen_returns(START)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-e8ea7011a15b>\u001b[0m in \u001b[0;36mrun_backtest\u001b[0;34m(X, Y, arg_dict, startindex, epochs, step, minmaxscale, standardscale)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# store in 1000:1025 - lining up with future Xs not current X/Ys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mP_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mflatprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mP_long\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlongprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mP_short\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mshortprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 696 is out of bounds for axis 0 with size 696"
     ]
    }
   ],
   "source": [
    "START=121\n",
    "EPOCHS=500\n",
    "STEP=12\n",
    "arg_dict = {\"n_hidden_layers\" : 2,\n",
    "            \"hidden_layer_size\" : 32,\n",
    "            \"reg_penalty\" : 0.0001,\n",
    "            \"dropout\": 0.25,\n",
    "            'verbose' : False\n",
    "           }\n",
    "     \n",
    "#model = build_model(**arg_dict)\n",
    "run_backtest(X, Y_class, arg_dict, startindex=START, step=STEP, epochs=EPOCHS)\n",
    "#gen_returns(START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check results_post_LASSO\n",
    "#model = LinearRegression()\n",
    "#R = run_backtest(X, Y, model, coef_dict_paper, startmonth=STARTMONTH, summary=False)\n",
    "results_post_LASSO = R[STARTMONTH:]\n",
    "print(len(results_post_LASSO))\n",
    "#print(results_post_LASSO)\n",
    "print(np.mean(results_post_LASSO))\n",
    "print(np.std(results_post_LASSO) * np.sqrt(12))\n",
    "print(np.prod(1 + results_post_LASSO / 100))\n",
    "print(np.prod(1 + results_post_LASSO / 100) ** (12.0/results_post_LASSO.shape[0]))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run performance chart\n",
    "perf_post_LASSO = 100 * np.cumprod(1 + results_post_LASSO / 100)\n",
    "\n",
    "def mychart(args, names=None):\n",
    "    x_coords = np.linspace(1970, 2016, args[0].shape[0])\n",
    "    \n",
    "    plotdata = []\n",
    "    for i in range(len(args)):\n",
    "        tracelabel = \"Trace %d\" % i\n",
    "        if names:\n",
    "                tracelabel=names[i]\n",
    "        plotdata.append(Scatter(x=x_coords,\n",
    "                                y=args[i].reshape(-1),\n",
    "                                mode = 'line',\n",
    "                                name=tracelabel))    \n",
    "\n",
    "    layout = Layout(\n",
    "        autosize=False,\n",
    "        width=600,\n",
    "        height=480,\n",
    "        yaxis=dict(\n",
    "            type='log',\n",
    "            autorange=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig = Figure(data=plotdata, layout=layout)\n",
    "    \n",
    "    return iplot(fig)\n",
    "    \n",
    "mychart([perf_post_LASSO],[\"Post-LASSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass coef_dict as None\n",
    "# fit_predict will do subset selection at each timestep using data it trains on\n",
    "model = LinearRegression()\n",
    "run_backtest(X, Y, model, coef_dict=None, startmonth=STARTMONTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_LASSO_each_timestep = R[STARTMONTH:]\n",
    "perf_LASSO_each_timestep = 100 * np.cumprod(1 + results_LASSO_each_timestep / 100)\n",
    "mychart([perf_LASSO_each_timestep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass coef_dict as 'all'\n",
    "# fit_predict will use all predictors (no subset selection)\n",
    "model = LinearRegression()\n",
    "run_backtest(X, Y, model, coef_dict='all', startmonth=STARTMONTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_OLS = R[STARTMONTH:]\n",
    "perf_OLS = 100 * np.cumprod(1 + results_OLS / 100)\n",
    "mychart([perf_OLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mychart([perf_post_LASSO, perf_LASSO_each_timestep, perf_OLS],[\"Post-LASSO\", \"LASSO each timestep\", \"OLS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walkforward_xval (X, Y, model, coef_dict=None):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # generate k-folds\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    kf.get_n_splits(X)\n",
    "    last_indexes = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # use test_index as last index to train\n",
    "        last_index = test_index[-1] + 1\n",
    "        last_indexes.append(last_index)\n",
    "    print(\"%s Generate splits %s\" % (time.strftime(\"%H:%M:%S\"), str([i for i in last_indexes])))\n",
    "\n",
    "    print(\"%s Starting training\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    \n",
    "    avg_bests = []\n",
    "    for i in range(1, n_splits-1):\n",
    "\n",
    "        models = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        count = 0        \n",
    "        # skip kfold 0 so you start with train 2x size of eval set\n",
    "        last_train_index = last_indexes[i]\n",
    "        last_xval_index = last_indexes[i+1]\n",
    "\n",
    "        # set up train, xval\n",
    "        # train from beginning to last_train_index        \n",
    "        print(\"Training indexes 0 to %d\" % (last_train_index-1))\n",
    "        X_fit = X[:last_train_index]\n",
    "        Y_fit = Y[:last_train_index]\n",
    "        # xval from last_train_index to last_xval_index\n",
    "        print(\"Cross-validating indexes %d to %d\" % (last_train_index, last_xval_index -1 ))\n",
    "        X_xval = X[last_train_index:last_xval_index]\n",
    "        Y_xval = Y[last_train_index:last_xval_index]\n",
    "\n",
    "        if coef_dict is None:\n",
    "            print(\"Performing LASSO subset selection on training set\")\n",
    "            coef_dict = subset_selection(X_fit, Y_fit, LassoLarsIC(criterion='aic'), verbose=False)\n",
    "        \n",
    "        mse_list = []\n",
    "        \n",
    "        for response in responses:\n",
    "            predcols = [predictor_reverse_dict[indstr] for indstr in coef_dict[response]]\n",
    "            if len(predcols) == 0:\n",
    "                continue\n",
    "            responsecol = response_reverse_dict[response]\n",
    "            \n",
    "            fit = model.fit(X_fit[:,predcols], Y_fit[:,responsecol])\n",
    "            # evaluate ... run prediction, calc MSE by industry, and average\n",
    "            y_xval_pred = fit.predict(X_xval[:,predcols])\n",
    "            mse_list.append(mean_squared_error(Y_xval[:,i], y_xval_pred))\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training\" % (time.strftime(\"%H:%M:%S\")))\n",
    "            sys.stdout.flush()             \n",
    "        # mean mse over industry ys for this fold\n",
    "        xval_score = np.mean(np.array(mse_list))            \n",
    "\n",
    "        # choose model with lowest xval loss\n",
    "        print (\"\\n%s Xval MSE %f\" % (time.strftime(\"%H:%M:%S\"), xval_score))\n",
    "        avg_bests.append(xval_score)\n",
    "    \n",
    "    print (\"Last Xval loss %f\" % (xval_score))\n",
    "    # mean over folds\n",
    "    avg_loss = np.mean(np.array(avg_bests))\n",
    "    print (\"Avg Xval loss %f\" % avg_loss)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    return (avg_loss, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline using linear regression\n",
    "model = LinearRegression()\n",
    "walkforward_xval (X, Y, model, coef_dict=coef_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(1,1,1),\n",
    "                     alpha=1.0,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "walkforward_xval (X, Y, model, coef_dict=coef_dict)\n",
    "# slightly better MSE than linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try many combos\n",
    "MODELPREFIX = \"MLP\"\n",
    "\n",
    "n_hiddens = [1, 2, 3]\n",
    "layer_sizes = [1, 2, 4, 8]\n",
    "reg_penalties = [0.0, 0.001, 0.01, 0.1, 1]\n",
    "hyperparameter_combos = list(product(n_hiddens, layer_sizes, reg_penalties))\n",
    "\n",
    "print(\"%s Running %d experiments\" % (time.strftime(\"%H:%M:%S\"), len(hyperparameter_combos)))\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "# minmaxscale each row\n",
    "\n",
    "Xscale = X.copy()\n",
    "Yscale = Y.copy()\n",
    "\n",
    "for i in range(Xscale.shape[0]):\n",
    "    Xscale[i] = Xscale[i] - np.min(Xscale[i])\n",
    "    Xscale[i] = Xscale[i]/np.max(Xscale[i])\n",
    "\n",
    "\n",
    "for i in range(Yscale.shape[0]):\n",
    "    Yscale[i] = Yscale[i] - np.min(Yscale[i])\n",
    "    Yscale[i] = Yscale[i]/np.max(Yscale[i])\n",
    "        \n",
    "for counter, param_list in enumerate(hyperparameter_combos):\n",
    "    n_hidden_layers, layer_size, reg_penalty = param_list\n",
    "    print(\"%s Running experiment %d of %d\" % (time.strftime(\"%H:%M:%S\"), counter+1, len(hyperparameter_combos)))\n",
    "    key = (n_hidden_layers, layer_size, reg_penalty)\n",
    "    print(\"%s n_hidden_layers = %d, hidden_layer_size = %d, reg_penalty = %.6f\" % \n",
    "          (time.strftime(\"%H:%M:%S\"), n_hidden_layers, layer_size, reg_penalty))\n",
    "    hls = tuple([layer_size]*n_hidden_layers)\n",
    "    model = MLPRegressor(hidden_layer_sizes=hls,\n",
    "                         alpha=reg_penalty,\n",
    "                         activation='tanh',\n",
    "                         max_iter=10000, \n",
    "                         tol=1e-10,\n",
    "                         solver='lbfgs')\n",
    "    \n",
    "    score, model = walkforward_xval (X, Y, model, coef_dict=coef_dict)\n",
    "\n",
    "    experiments[key] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list and chart experiments\n",
    "flatlist = [list(l[0]) + [l[1]] for l in experiments.items()]\n",
    " \n",
    "lossframe = pd.DataFrame(flatlist, columns=[\"n_hidden_layers\", \"layer_size\", \"reg_penalty\", \"loss\"])\n",
    "lossframe.sort_values(['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pick lowest loss , but first we look at patterns by hyperparameter\n",
    "pd.DataFrame(lossframe.groupby(['n_hidden_layers'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lossframe.groupby(['layer_size'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lossframe.groupby(['reg_penalty'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(lossframe, x_labels, y_labels, x_suffix=\"\", y_suffix=\"\"):\n",
    "\n",
    "    pivot = lossframe.pivot_table(index=[x_labels], columns=[y_labels], values=['loss'])\n",
    "    # specify labels as strings, to force it to use a discrete axis\n",
    "    if lossframe[x_labels].dtype == np.float64 or lossframe[x_labels].dtype == np.float32:\n",
    "        xaxis = [\"%f %s\" % (i, x_suffix) for i in pivot.columns.levels[1].values]\n",
    "    else:\n",
    "        xaxis = [\"%d %s\" % (i, x_suffix) for i in pivot.columns.levels[1].values]\n",
    "    if lossframe[y_labels].dtype == np.float64 or lossframe[y_labels].dtype == np.float32:\n",
    "        yaxis = [\"%f %s\" % (i, y_suffix) for i in pivot.index.values]\n",
    "    else:\n",
    "        yaxis = [\"%d %s\" % (i, y_suffix) for i in pivot.index.values]\n",
    "        \n",
    "    print(xaxis, yaxis)\n",
    "    \"\"\"plot a heat map of a matrix\"\"\"\n",
    "    chart_width=640\n",
    "    chart_height=480\n",
    "    \n",
    "    layout = Layout(\n",
    "        title=\"%s v. %s\" % (x_labels, y_labels),\n",
    "        height=chart_height,\n",
    "        width=chart_width,     \n",
    "        margin=dict(\n",
    "            l=150,\n",
    "            r=30,\n",
    "            b=120,\n",
    "            t=100,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=y_labels,\n",
    "            tickfont=dict(\n",
    "                family='Arial, sans-serif',\n",
    "                size=10,\n",
    "                color='black'\n",
    "            ),\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=x_labels,\n",
    "            tickfont=dict(\n",
    "                family='Arial, sans-serif',\n",
    "                size=10,\n",
    "                color='black'\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [Heatmap(z=pivot.values,\n",
    "                    x=xaxis,\n",
    "                    y=yaxis,\n",
    "                    colorscale=[[0, 'rgb(0,0,255)', [1, 'rgb(255,0,0)']]],\n",
    "                   )\n",
    "           ]\n",
    "\n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    return iplot(fig, link_text=\"\")\n",
    "\n",
    "plot_matrix(lossframe, \"n_hidden_layers\", \"layer_size\", x_suffix=\" units\", y_suffix=\" layers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(lossframe, \"n_hidden_layers\", \"reg_penalty\", x_suffix=\"p\", y_suffix=\" layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(lossframe, \"reg_penalty\", \"layer_size\", x_suffix=\" units\", y_suffix=\"p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these results are not very good, ~same as LinearRegression, but try best one\n",
    "# 1-unit layers is not really a NN but anyway let's see how it does\n",
    "model = MLPRegressor(hidden_layer_sizes=(1,1,1),\n",
    "                     alpha=0.01,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "run_backtest(X, Y, model, startmonth=STARTMONTH, minmaxscale=False)\n",
    "# slightly better OOS MSE than linear regression (45.2) but worse Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try keras instead of sklearn MLPRegressor\n",
    "INPUT_DIM = X.shape[1]\n",
    "print(INPUT_DIM)\n",
    "OUTPUT_DIM = len(responses) # 30\n",
    "\n",
    "def build_model(n_hidden_layers = 2,\n",
    "                hidden_layer_size = 32,\n",
    "                reg_penalty = 0.0001,\n",
    "                dropout = 0.333,\n",
    "                verbose=True):\n",
    "\n",
    "    main_input = Input(shape=(INPUT_DIM,), \n",
    "                       dtype='float32', \n",
    "                       name='main_input')\n",
    "    lastlayer=main_input\n",
    "\n",
    "    for i in range(n_hidden_layers):\n",
    "        if verbose:\n",
    "            print(\"layer %d size %d, reg_penalty %.8f, dropout %.3f\" % (i, hidden_layer_size, reg_penalty, dropout))\n",
    "        lastlayer = Dense(units = hidden_layer_size, \n",
    "                          activation = 'relu',\n",
    "                          kernel_initializer = keras.initializers.glorot_uniform(),\n",
    "                          kernel_regularizer=keras.regularizers.l1(reg_penalty),\n",
    "                          name = \"Dense%02d\" % i)(lastlayer)\n",
    "\n",
    "        if dropout:\n",
    "            lastlayer = Dropout(dropout, name = \"Dropout%02d\" % i)(lastlayer)\n",
    "    \n",
    "    outputs = []\n",
    "    for i in range(OUTPUT_DIM):\n",
    "        # OUTPUT_DIM outputs\n",
    "        output01 = Dense(1,\n",
    "                         activation='linear', \n",
    "                         name='output%02d' % i)(lastlayer)\n",
    "        outputs.append(output01)\n",
    "    \n",
    "    model = Model(inputs=[main_input], outputs=outputs)\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\", loss_weights=[1.]*OUTPUT_DIM)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment with walk-forward cross-validation\n",
    "\n",
    "EPOCHS = 500\n",
    "#VAL_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def run_experiment (n_hidden_layers = 2,\n",
    "                    hidden_layer_size = 8,\n",
    "                    reg_penalty = 0.0,\n",
    "                    dropout = 0.5,\n",
    "                    epochs = EPOCHS\n",
    "                   ):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # generate k-folds\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    kf.get_n_splits(X)\n",
    "    last_indexes = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # use test_index as last index to train\n",
    "        last_index = test_index[-1] + 1\n",
    "        last_indexes.append(last_index)\n",
    "\n",
    "    print(\"%s Generate splits %s\" % (time.strftime(\"%H:%M:%S\"), str([i for i in last_indexes])))\n",
    "    \n",
    "    avg_bests = []\n",
    "\n",
    "    print(\"%s Build model\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    model = build_model(n_hidden_layers = n_hidden_layers,\n",
    "                        hidden_layer_size = hidden_layer_size,\n",
    "                        reg_penalty = reg_penalty,\n",
    "                        dropout = dropout)\n",
    "    print(\"Compile time : %s\" % str(time.time() - start))\n",
    "    print(\"Starting to train : %s\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    for i in range(1, n_splits-1):\n",
    "\n",
    "        models = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        count = 0        \n",
    "        # skip kfold 0 so you start with train 2x size of eval set\n",
    "        last_train_index = last_indexes[i]\n",
    "        last_xval_index = last_indexes[i+1]\n",
    "\n",
    "        # set up train, xval\n",
    "        # train from beginning to last_train_index\n",
    "        print(\"Training indexes 0 to %d\" % (last_train_index-1))\n",
    "        X_fit = X[:last_train_index]\n",
    "        Y_fit = Y[:last_train_index]\n",
    "        # xval from last_train_index to last_xval_index\n",
    "        print(\"Cross-validating indexes %d to %d\" % (last_train_index, last_xval_index -1 ))\n",
    "        X_xval = X[last_train_index:last_xval_index]\n",
    "        Y_xval = Y[last_train_index:last_xval_index]\n",
    "\n",
    "        responses = []\n",
    "        for i in range(OUTPUT_DIM):\n",
    "            responses.append(Y_fit[:,i])\n",
    "        # train for epochs\n",
    "        for epoch in range(epochs):\n",
    "            fit = model.fit(\n",
    "                X_fit,\n",
    "                responses,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                #validation_split=VAL_SPLIT,\n",
    "                epochs=1,\n",
    "                verbose=0)\n",
    "            \n",
    "            train_loss = fit.history['loss'][-1]\n",
    "            # evaluate ... run prediction, calc MSE by industry, and average\n",
    "            y_xval_pred = np.array(model.predict(X_xval))\n",
    "            y_xval_pred = y_xval_pred.reshape(Y_xval.T.shape)\n",
    "            y_xval_pred = y_xval_pred.T\n",
    "            mse_list = []\n",
    "            for i in range(len(industries)):\n",
    "                mse_list.append(mean_squared_error(Y_xval[:,i], y_xval_pred[:,i]))\n",
    "            xval_score = np.mean(np.array(mse_list))            \n",
    "            \n",
    "            losses.append(train_loss)\n",
    "            scores.append(xval_score)\n",
    "            models.append(copy.copy(model))\n",
    "\n",
    "            bestloss_index = np.argmin(scores)\n",
    "            bestloss_value = scores[bestloss_index]\n",
    "\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training %d of %d\" % (time.strftime(\"%H:%M:%S\"), count, epochs))\n",
    "                \n",
    "            sys.stdout.flush()            \n",
    "            \n",
    "            # stop if loss rises by 20% from best\n",
    "            if xval_score / bestloss_value > 1.2:\n",
    "                print(\"Stopping early\" )\n",
    "                break\n",
    "\n",
    "        # choose model with lowest xval loss\n",
    "        print(\"\")\n",
    "        print (\"%s Best Xval loss epoch %d, value %f\" % (time.strftime(\"%H:%M:%S\"), bestloss_index, bestloss_value))\n",
    "        avg_bests.append(bestloss_value)\n",
    "        model = models[bestloss_index]\n",
    "    \n",
    "    print (\"Last Xval loss %f\" % (bestloss_value))\n",
    "    avg_loss = np.mean(np.array(avg_bests))\n",
    "    print (\"Avg Xval loss %f\" % avg_loss)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    return (avg_loss, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notable improvement in MSE vs. LinearRegression\n",
    "# run a lot of experiments in big xval loop to pick best hyperparameters\n",
    "\n",
    "MODELPREFIX = \"FFNN\"\n",
    "\n",
    "n_hiddens = [1, 2, 3]\n",
    "layer_sizes = [2, 4, 8, 16]\n",
    "reg_penalties = [0.0, 0.0001, 0.001, 0.01]\n",
    "dropouts = [0.25]\n",
    "\n",
    "hyperparameter_combos = list(product(n_hiddens, layer_sizes, reg_penalties, dropouts))\n",
    "\n",
    "print(\"%s Running %d experiments\" % (time.strftime(\"%H:%M:%S\"), len(hyperparameter_combos)))\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "for counter, param_list in enumerate(hyperparameter_combos):\n",
    "    n_hidden_layers, layer_size, reg_penalty, dropout = param_list\n",
    "    print(\"%s Running experiment %d of %d\" % (time.strftime(\"%H:%M:%S\"), counter+1, len(hyperparameter_combos)))\n",
    "    key = (n_hidden_layers, layer_size, reg_penalty, dropout)\n",
    "    score, model = run_experiment(n_hidden_layers = n_hidden_layers,\n",
    "                                  hidden_layer_size = layer_size,\n",
    "                                  reg_penalty = reg_penalty,\n",
    "                                  dropout = dropout,\n",
    "                                  epochs=EPOCHS)\n",
    "    experiments[key] = score \n",
    "    modelname = \"%s_%.6f_%d_%d_%.6f_%.3f\" % (MODELPREFIX, score, n_hidden_layers, layer_size, reg_penalty, dropout)\n",
    "    print(\"%s Saving %s.h5\" % (time.strftime(\"%H:%M:%S\"), modelname))\n",
    "    model.save(\"%s.h5\" % modelname)\n",
    "    model.save_weights(\"%s_weights.h5\" % modelname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list and chart experiments\n",
    "flatlist = [list(l[0]) + [l[1]] for l in experiments.items()]\n",
    "\n",
    "lossframe = pd.DataFrame(flatlist, columns=[\"n_hidden_layers\", \"layer_size\", \"reg_penalty\", \"dropout\",\n",
    "                                            \"loss\"])\n",
    "lossframe.sort_values(['loss'])\n",
    "# better than LinearRegression or MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pick lowest loss , but first we look at patterns by hyperparameter\n",
    "pd.DataFrame(lossframe.groupby(['n_hidden_layers'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lossframe.groupby(['layer_size'])['loss'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lossframe.groupby(['reg_penalty'])['loss'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(lossframe, \"n_hidden_layers\", \"layer_size\", x_suffix=\" units\", y_suffix=\" layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(lossframe, \"n_hidden_layers\", \"reg_penalty\", x_suffix=\"p\", y_suffix=\" layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(lossframe, \"reg_penalty\", \"layer_size\", x_suffix=\" units\", y_suffix=\"p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=500\n",
    "\n",
    "def fit_predict(X, Y, model, epochs=EPOCHS, npredict=1, verbose=False):\n",
    "    \"\"\"for backtest, train model using Y_list v. X using n-npredict rows\n",
    "    generate npredict prediction Y_list using last npredict rows of X\n",
    "    if npredict=1, fit using n-1 rows, return prediction using X for final month\n",
    "    if npredict=26, fit using n-26 rows, return prediction using X for final 26 months\"\"\"\n",
    "    \n",
    "    nrows = X.shape[0]\n",
    "    if verbose:\n",
    "        print(\"Fit on %d rows 0 to %d\" % (nrows-npredict, nrows-npredict-1))\n",
    "        print(\"Predict on %d rows %d to %d\" % (npredict, nrows-npredict, nrows-1))\n",
    "        \n",
    "    # keep last rows to predict against\n",
    "    X_predict = X[-npredict:]\n",
    "    X_predict = X_predict.reshape(npredict,X.shape[1])\n",
    "    # fit on remaining rows\n",
    "    X_fit = X[:-npredict]\n",
    "    Y_fit = Y[:-npredict]\n",
    "    \n",
    "    # make a list of Ys expected by Keras\n",
    "    Y_list = []\n",
    "    for i in range(OUTPUT_DIM):\n",
    "        Y_list.append(Y_fit[:,i])\n",
    "        \n",
    "    fit = model.fit(\n",
    "        X_fit,\n",
    "        Y_list,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=epochs,\n",
    "        verbose=0)\n",
    "    \n",
    "    Z = model.predict(X_predict)\n",
    "    # get back a list of ncols arrays, reshape each to 1D 1 x npredict array\n",
    "    Z = [z.reshape(npredict) for z in Z]\n",
    "    # return npredict x ncols array\n",
    "    return np.array(Z).transpose()\n",
    "\n",
    "print(\"%s Start fit\" % (time.strftime(\"%H:%M:%S\")))\n",
    "predictions = fit_predict(X, Y, model,epochs=3,npredict=3, verbose=True)\n",
    "print(\"%s End fit\" % (time.strftime(\"%H:%M:%S\")))\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=500\n",
    "\n",
    "nrows = X.shape[0]\n",
    "startindex = 1000\n",
    "\n",
    "def run_backtest(X, Y, arg_dict, startindex=0, epochs=EPOCHS, step=1, minmaxscale=False, standardscale=False):\n",
    "    \"\"\"create keras model; add step, to iteratively train, predict 12 months, train up to next 12 months \"\"\"\n",
    "    global P\n",
    "    global R \n",
    "    \n",
    "    print(\"%s Starting backtest\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    P = np.zeros((Y.shape[0],OUTPUT_DIM))\n",
    "    \n",
    "    count = 0\n",
    "    nrows = X.shape[0]\n",
    "\n",
    "    Xscale = X.copy()\n",
    "    Yscale = Y.copy()\n",
    "    \n",
    "    if minmaxscale:\n",
    "        # minmaxscale each row (min->0, max->1) - transpose, scale, transpose back because scales by columns\n",
    "        Xscale = MinMaxScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        Yscale = MinMaxScaler().fit_transform(Yscale.transpose()).transpose()\n",
    "        print(\"using MinMaxScaler\")\n",
    "    elif standardscale:\n",
    "        # standardize each row (mean->0, SD->1)- transpose, scale, transpose back because scales by columns\n",
    "        Xscale = StandardScaler().fit_transform(Xscale.transpose()).transpose()\n",
    "        Yscale = StandardScaler().fit_transform(Yscale.transpose()).transpose()\n",
    "        print(\"using StandardScaler\")\n",
    "     \n",
    "    for train_index in range(startindex, nrows, step):\n",
    "        if train_index + step >= nrows:\n",
    "            train_index = nrows-step\n",
    "            \n",
    "        model = build_model(n_hidden_layers = arg_dict[\"n_hidden_layers\"],\n",
    "                            hidden_layer_size = arg_dict[\"hidden_layer_size\"], \n",
    "                            reg_penalty = arg_dict[\"reg_penalty\"], \n",
    "                            dropout = arg_dict[\"dropout\"],\n",
    "                            verbose=arg_dict[\"verbose\"])\n",
    "        \n",
    "        fp_index = train_index + step # eg 1000 + 26 = 1026\n",
    "\n",
    "        # fit on e.g. 0:999, predict 1000-1025\n",
    "        predictions = fit_predict(Xscale[:fp_index, :], \n",
    "                                  Yscale[:fp_index], \n",
    "                                  model,\n",
    "                                  epochs=epochs,\n",
    "                                  npredict=step)\n",
    "        # store in 1000:1025 - lining up with future Xs not current X/Ys\n",
    "        for i in range(step):\n",
    "            P[train_index + i]= predictions[i]\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training %d of %d\" % (time.strftime(\"%H:%M:%S\"), count, nrows-startindex))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    msetemp = (P-Yscale)**2\n",
    "    mse = np.mean(np.nan_to_num(msetemp))\n",
    "    print(\"MSE across all predictions: %.4f\" % mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_returns(startindex):\n",
    "    # generate returns\n",
    "    global X\n",
    "    global Y\n",
    "    global P\n",
    "    global R\n",
    "\n",
    "    msetemp = (P[startindex+1:]-Y[startindex:-1])**2\n",
    "#    msetemp = (P[startindex:]-Y[startindex:])**2\n",
    "    mse = np.mean(np.nan_to_num(msetemp))\n",
    "    print(\"MSE across all predictions: %.4f\" % mse)\n",
    "    print(\"Variance: %.4f\" % (np.mean(Y[startindex:]**2)))\n",
    "    print(\"R-squared: %.4f\" % (1- mse/np.mean(Y[startindex:]**2)))\n",
    "\n",
    "    \n",
    "    nrows = P.shape[0]\n",
    "\n",
    "    R = np.zeros(nrows)\n",
    "    NUM_POSITIONS = 6 # top quintile (and bottom)\n",
    "    \n",
    "    for train_index in range(startindex, nrows):\n",
    "        # get indexes, sorted smallest to largest\n",
    "        select_array = np.argsort(P[train_index])\n",
    "#        print(P[train_index, select_array])\n",
    "#        print(Y[train_index, select_array])\n",
    "#        print(\"--\")\n",
    "        # leftmost 6\n",
    "        short_indexes = select_array[:NUM_POSITIONS]\n",
    "        # rightmost 6\n",
    "        long_indexes = select_array[-NUM_POSITIONS:]\n",
    "        # compute equal weighted long/short return\n",
    "        # + 50% long * 0.25 * perf of long indexes\n",
    "        R[train_index] = R[train_index] + 0.50 * np.mean(X[train_index, long_indexes])\n",
    "        # - 50% short * 0.25 * perf of short indexes\n",
    "        R[train_index] = R[train_index] - 0.50 * np.mean(X[train_index, short_indexes])\n",
    "                \n",
    "    # truncate to nonzero part of R            \n",
    "    results = R[startindex:]\n",
    "    \n",
    "    index = pd.date_range(data.iloc[START].name,periods=results.shape[0], freq='M')\n",
    "    perfdata = pd.DataFrame(results,index=index,columns=['Returns'])\n",
    "    perfdata['Equity'] = 100 * np.cumprod(1 + results / 100)\n",
    "    \n",
    "    stats = perfdata['Equity'].calc_stats()\n",
    "    \n",
    "    retframe = pd.DataFrame([stats.stats.loc['start'],\n",
    "                             stats.stats.loc['end'],\n",
    "                             stats.stats.loc['cagr'],\n",
    "                             stats.stats.loc['yearly_vol'],\n",
    "                             stats.stats.loc['yearly_sharpe'],\n",
    "                             stats.stats.loc['max_drawdown'],\n",
    "                             ffn.core.calc_sortino_ratio(perfdata.Returns, rf=0, nperiods=results.shape[0], annualize=False),\n",
    "                            ],\n",
    "                            index = ['start',\n",
    "                                     'end',\n",
    "                                     'cagr',\n",
    "                                     'yearly_vol',\n",
    "                                     'yearly_sharpe',\n",
    "                                     'max_drawdown',\n",
    "                                     'sortino',\n",
    "                                    ],\n",
    "                            columns=['Value'])\n",
    "    return retframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_returns(START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START=121\n",
    "EPOCHS=500\n",
    "STEP=12\n",
    "arg_dict = {\"n_hidden_layers\" : 3,\n",
    "            \"hidden_layer_size\" : 4,\n",
    "            \"reg_penalty\" : 0.0,\n",
    "            \"dropout\": 0.25,\n",
    "            'verbose' : False\n",
    "           }\n",
    "     \n",
    "#model = build_model(**arg_dict)\n",
    "run_backtest(X, Y, arg_dict, startindex=START, step=STEP, epochs=EPOCHS)\n",
    "gen_returns(START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

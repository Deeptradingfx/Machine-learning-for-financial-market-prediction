{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate [Dynamic Return Dependencies Across Industries: A Machine Learning Approach](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3120110&download=yes) by David Rapach, Jack Strauss, Jun Tu and Guofu Zhou.\n",
    "\n",
    "1) Use industry returns from [Ken French](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html)\n",
    "\n",
    "2) Forecast (for example) this month's Chemical industry return using last month's returns from all 30 industries \n",
    "\n",
    "3) Use LASSO for predictor subset selection over the entire 1960-2016 period to determine that e.g. Beer is predicted by Food, Clothing, Coal\n",
    "\n",
    "4) Use those predictors and simple linear regression to predict returns\n",
    "\n",
    "5) Generate portfolios and run backtests.\n",
    "\n",
    "- Predictor selection - finds same predictors except 2 industries. Possibly use of AICc instead of AIC (don't see an sklearn implementation that uses AICc)\n",
    "\n",
    "- Prediction by industry - R-squareds line up pretty closely\n",
    "\n",
    "- Portfolio performance, similar ballpark results. Since prediction is similar but return profile is different, must be some difference in portfolio construction. (am taking equal weight top 6 predicted as long and bottom 6 as short, every month)\n",
    "\n",
    "- For some reason their mean returns don't line up to geometric mean annualized, they seem to be calculating something different.\n",
    "\n",
    "- But it does replicate closely and perform pretty well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run MLP with and without scaling, see if you get better prediction\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import copy\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Hide messy TensorFlow warnings\n",
    "warnings.filterwarnings(\"ignore\") #Hide messy numpy warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, lasso_path, lars_path, LassoLarsIC\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import ffn\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly as py\n",
    "# print (py.__version__) # requires version >= 1.9.0\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "random.seed(1764)\n",
    "np.random.seed(1764)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(697, 90)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Food.lead</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195912</th>\n",
       "      <td>2.01</td>\n",
       "      <td>-4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196001</th>\n",
       "      <td>-4.49</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196002</th>\n",
       "      <td>3.35</td>\n",
       "      <td>-1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196003</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196004</th>\n",
       "      <td>1.17</td>\n",
       "      <td>8.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196005</th>\n",
       "      <td>8.20</td>\n",
       "      <td>5.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196006</th>\n",
       "      <td>5.39</td>\n",
       "      <td>-2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>-2.11</td>\n",
       "      <td>4.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196008</th>\n",
       "      <td>4.57</td>\n",
       "      <td>-3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196009</th>\n",
       "      <td>-3.88</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196010</th>\n",
       "      <td>1.02</td>\n",
       "      <td>9.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196011</th>\n",
       "      <td>9.46</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196012</th>\n",
       "      <td>4.51</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196101</th>\n",
       "      <td>4.70</td>\n",
       "      <td>4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196102</th>\n",
       "      <td>4.21</td>\n",
       "      <td>4.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196103</th>\n",
       "      <td>4.64</td>\n",
       "      <td>-1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196104</th>\n",
       "      <td>-1.39</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196105</th>\n",
       "      <td>4.20</td>\n",
       "      <td>-2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196106</th>\n",
       "      <td>-2.17</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>2.72</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196108</th>\n",
       "      <td>4.92</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>3.73</td>\n",
       "      <td>5.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>5.28</td>\n",
       "      <td>-3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>-3.69</td>\n",
       "      <td>-6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196201</th>\n",
       "      <td>-6.67</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196202</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196203</th>\n",
       "      <td>0.98</td>\n",
       "      <td>-4.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196204</th>\n",
       "      <td>-4.59</td>\n",
       "      <td>-11.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196205</th>\n",
       "      <td>-11.25</td>\n",
       "      <td>-8.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201507</th>\n",
       "      <td>4.03</td>\n",
       "      <td>-4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201508</th>\n",
       "      <td>-4.37</td>\n",
       "      <td>-1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201509</th>\n",
       "      <td>-1.19</td>\n",
       "      <td>5.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201510</th>\n",
       "      <td>5.81</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201511</th>\n",
       "      <td>0.11</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201512</th>\n",
       "      <td>1.96</td>\n",
       "      <td>-1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>0.95</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>4.69</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>0.63</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>2.06</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201606</th>\n",
       "      <td>4.75</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201607</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201608</th>\n",
       "      <td>-0.52</td>\n",
       "      <td>-2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201609</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>-0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201610</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>-4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201611</th>\n",
       "      <td>-4.41</td>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201612</th>\n",
       "      <td>4.43</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201701</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201702</th>\n",
       "      <td>1.71</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201703</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>0.76</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201705</th>\n",
       "      <td>1.63</td>\n",
       "      <td>-2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201706</th>\n",
       "      <td>-2.65</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201707</th>\n",
       "      <td>1.52</td>\n",
       "      <td>-2.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201708</th>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201709</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>0.71</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201711</th>\n",
       "      <td>4.15</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Food  Food.lead\n",
       "yyyymm                  \n",
       "195912   2.01      -4.49\n",
       "196001  -4.49       3.35\n",
       "196002   3.35      -1.67\n",
       "196003  -1.67       1.17\n",
       "196004   1.17       8.20\n",
       "196005   8.20       5.39\n",
       "196006   5.39      -2.11\n",
       "196007  -2.11       4.57\n",
       "196008   4.57      -3.88\n",
       "196009  -3.88       1.02\n",
       "196010   1.02       9.46\n",
       "196011   9.46       4.51\n",
       "196012   4.51       4.70\n",
       "196101   4.70       4.21\n",
       "196102   4.21       4.64\n",
       "196103   4.64      -1.39\n",
       "196104  -1.39       4.20\n",
       "196105   4.20      -2.17\n",
       "196106  -2.17       2.72\n",
       "196107   2.72       4.92\n",
       "196108   4.92      -0.62\n",
       "196109  -0.62       3.73\n",
       "196110   3.73       5.28\n",
       "196111   5.28      -3.69\n",
       "196112  -3.69      -6.67\n",
       "196201  -6.67      -0.25\n",
       "196202  -0.25       0.98\n",
       "196203   0.98      -4.59\n",
       "196204  -4.59     -11.25\n",
       "196205 -11.25      -8.75\n",
       "...       ...        ...\n",
       "201507   4.03      -4.37\n",
       "201508  -4.37      -1.19\n",
       "201509  -1.19       5.81\n",
       "201510   5.81       0.11\n",
       "201511   0.11       1.96\n",
       "201512   1.96      -1.67\n",
       "201601  -1.67       0.95\n",
       "201602   0.95       4.69\n",
       "201603   4.69       0.63\n",
       "201604   0.63       2.06\n",
       "201605   2.06       4.75\n",
       "201606   4.75      -0.51\n",
       "201607  -0.51      -0.52\n",
       "201608  -0.52      -2.92\n",
       "201609  -2.92      -0.33\n",
       "201610  -0.33      -4.41\n",
       "201611  -4.41       4.43\n",
       "201612   4.43       0.95\n",
       "201701   0.95       1.71\n",
       "201702   1.71       0.52\n",
       "201703   0.52       0.76\n",
       "201704   0.76       1.63\n",
       "201705   1.63      -2.65\n",
       "201706  -2.65       1.52\n",
       "201707   1.52      -2.77\n",
       "201708  -2.77       0.43\n",
       "201709   0.43       0.71\n",
       "201710   0.71       4.15\n",
       "201711   4.15      -0.10\n",
       "201712  -0.10       2.27\n",
       "\n",
       "[697 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(\"30_Industry_Portfolios.csv\")\n",
    "data = data.set_index('yyyymm')\n",
    "industries = list(data.columns)\n",
    "# map industry names to col nums\n",
    "ind_reverse_dict = dict([(industries[i], i) for i in range(len(industries))])\n",
    "\n",
    "rfdata = pd.read_csv(\"F-F_Research_Data_Factors.csv\")\n",
    "rfdata = rfdata.set_index('yyyymm')\n",
    "data['rf'] = rfdata['RF']\n",
    "\n",
    "# subtract risk-free rate\n",
    "# create a response variable led by 1 period to predict\n",
    "for ind in industries:\n",
    "    data[ind] = data[ind] - data['rf']\n",
    "\n",
    "for ind in industries:\n",
    "    data[ind+\".3m\"] = pd.rolling_mean(data[ind],3)\n",
    "    \n",
    "#for ind in industries:\n",
    "#    data[ind+\".6m\"] = pd.rolling_mean(data[ind],6)\n",
    "\n",
    "#for ind in industries:\n",
    "#    data[ind+\".12m\"] = pd.rolling_mean(data[ind],12)\n",
    "    \n",
    "for ind in industries:\n",
    "    data[ind+\".lead\"] = data[ind].shift(-1)\n",
    "\n",
    "data = data.loc[data.index[data.index > 195911]]\n",
    "data = data.drop(columns=['rf'])    \n",
    "data = data.dropna(axis=0, how='any')\n",
    "\n",
    "nresponses = len(industries)\n",
    "npredictors = data.shape[1]-nresponses\n",
    "\n",
    "predictors = list(data.columns[:npredictors])\n",
    "predictor_reverse_dict = dict([(predictors[i], i) for i in range(len(predictors))])\n",
    "\n",
    "responses = list(data.columns[-nresponses:])\n",
    "response_reverse_dict = dict([(responses[i], i) for i in range(len(responses))])\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data[['Food', 'Food.lead']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Beer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Games</th>\n",
       "      <th>Books</th>\n",
       "      <th>Hshld</th>\n",
       "      <th>Clths</th>\n",
       "      <th>Hlth</th>\n",
       "      <th>Chems</th>\n",
       "      <th>Txtls</th>\n",
       "      <th>...</th>\n",
       "      <th>Telcm.lead</th>\n",
       "      <th>Servs.lead</th>\n",
       "      <th>BusEq.lead</th>\n",
       "      <th>Paper.lead</th>\n",
       "      <th>Trans.lead</th>\n",
       "      <th>Whlsl.lead</th>\n",
       "      <th>Rtail.lead</th>\n",
       "      <th>Meals.lead</th>\n",
       "      <th>Fin.lead</th>\n",
       "      <th>Other.lead</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195912</th>\n",
       "      <td>2.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>1.64</td>\n",
       "      <td>7.29</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>-4.31</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>-6.09</td>\n",
       "      <td>-10.08</td>\n",
       "      <td>-4.68</td>\n",
       "      <td>-3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196001</th>\n",
       "      <td>-4.49</td>\n",
       "      <td>-5.71</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-5.47</td>\n",
       "      <td>-7.84</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>-6.68</td>\n",
       "      <td>-10.03</td>\n",
       "      <td>-4.77</td>\n",
       "      <td>...</td>\n",
       "      <td>8.07</td>\n",
       "      <td>9.13</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196002</th>\n",
       "      <td>3.35</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>2.27</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.39</td>\n",
       "      <td>9.31</td>\n",
       "      <td>1.44</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-3.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196003</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>-6.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>8.86</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196004</th>\n",
       "      <td>1.17</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>1.35</td>\n",
       "      <td>6.46</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.49</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>11.90</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196005</th>\n",
       "      <td>8.20</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>2.44</td>\n",
       "      <td>7.28</td>\n",
       "      <td>11.67</td>\n",
       "      <td>7.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>13.50</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-8.07</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.72</td>\n",
       "      <td>6.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196006</th>\n",
       "      <td>5.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.38</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2.84</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>-6.16</td>\n",
       "      <td>-2.99</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>-2.11</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>4.60</td>\n",
       "      <td>-4.72</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>-6.80</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>...</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.69</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.51</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196008</th>\n",
       "      <td>4.57</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.20</td>\n",
       "      <td>7.16</td>\n",
       "      <td>3.63</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.07</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-7.61</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-8.44</td>\n",
       "      <td>-8.57</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196009</th>\n",
       "      <td>-3.88</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-6.20</td>\n",
       "      <td>-9.18</td>\n",
       "      <td>-4.23</td>\n",
       "      <td>-8.87</td>\n",
       "      <td>-6.70</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.62</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-4.54</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196010</th>\n",
       "      <td>1.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.48</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.06</td>\n",
       "      <td>9.49</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.35</td>\n",
       "      <td>9.72</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4.40</td>\n",
       "      <td>7.71</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196011</th>\n",
       "      <td>9.46</td>\n",
       "      <td>6.57</td>\n",
       "      <td>5.44</td>\n",
       "      <td>13.91</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.04</td>\n",
       "      <td>...</td>\n",
       "      <td>12.29</td>\n",
       "      <td>8.18</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.57</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.56</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196012</th>\n",
       "      <td>4.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>3.54</td>\n",
       "      <td>7.77</td>\n",
       "      <td>7.41</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.28</td>\n",
       "      <td>6.06</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>7.70</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.08</td>\n",
       "      <td>4.56</td>\n",
       "      <td>8.35</td>\n",
       "      <td>7.93</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.08</td>\n",
       "      <td>7.12</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196101</th>\n",
       "      <td>4.70</td>\n",
       "      <td>5.23</td>\n",
       "      <td>8.77</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.47</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.94</td>\n",
       "      <td>5.86</td>\n",
       "      <td>6.46</td>\n",
       "      <td>11.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.54</td>\n",
       "      <td>6.83</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.31</td>\n",
       "      <td>4.82</td>\n",
       "      <td>8.23</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196102</th>\n",
       "      <td>4.21</td>\n",
       "      <td>8.16</td>\n",
       "      <td>5.41</td>\n",
       "      <td>22.33</td>\n",
       "      <td>2.15</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>2.13</td>\n",
       "      <td>6.81</td>\n",
       "      <td>...</td>\n",
       "      <td>7.23</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4.45</td>\n",
       "      <td>5.76</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.34</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196103</th>\n",
       "      <td>4.64</td>\n",
       "      <td>2.55</td>\n",
       "      <td>5.60</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.77</td>\n",
       "      <td>6.34</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196104</th>\n",
       "      <td>-1.39</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.39</td>\n",
       "      <td>4.74</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196105</th>\n",
       "      <td>4.20</td>\n",
       "      <td>5.38</td>\n",
       "      <td>3.39</td>\n",
       "      <td>-3.91</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-3.31</td>\n",
       "      <td>-4.46</td>\n",
       "      <td>-4.57</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>-2.88</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196106</th>\n",
       "      <td>-2.17</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-5.87</td>\n",
       "      <td>-3.85</td>\n",
       "      <td>3.43</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.35</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196107</th>\n",
       "      <td>2.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.95</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.99</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.65</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196108</th>\n",
       "      <td>4.92</td>\n",
       "      <td>3.20</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10.45</td>\n",
       "      <td>5.21</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-2.74</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.24</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.30</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>3.73</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>7.05</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>8.28</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.11</td>\n",
       "      <td>...</td>\n",
       "      <td>8.34</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.65</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>5.28</td>\n",
       "      <td>4.47</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.51</td>\n",
       "      <td>5.30</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.49</td>\n",
       "      <td>7.37</td>\n",
       "      <td>...</td>\n",
       "      <td>3.14</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-4.44</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>-3.69</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-6.12</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>-3.78</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.32</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>-6.91</td>\n",
       "      <td>-5.22</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-9.56</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196201</th>\n",
       "      <td>-6.67</td>\n",
       "      <td>-3.45</td>\n",
       "      <td>-4.28</td>\n",
       "      <td>-13.23</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>-5.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>3.59</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196202</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>2.01</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-4.07</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196203</th>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-6.67</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.93</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>-12.01</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-6.27</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>-9.09</td>\n",
       "      <td>-7.69</td>\n",
       "      <td>-2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196204</th>\n",
       "      <td>-4.59</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-12.99</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>-8.74</td>\n",
       "      <td>-7.03</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>-11.23</td>\n",
       "      <td>-6.23</td>\n",
       "      <td>-7.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>-10.97</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>-11.03</td>\n",
       "      <td>-5.17</td>\n",
       "      <td>-11.34</td>\n",
       "      <td>-9.09</td>\n",
       "      <td>-7.46</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>-11.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196205</th>\n",
       "      <td>-11.25</td>\n",
       "      <td>-9.05</td>\n",
       "      <td>-14.14</td>\n",
       "      <td>-11.39</td>\n",
       "      <td>-14.87</td>\n",
       "      <td>-10.19</td>\n",
       "      <td>-10.01</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>-7.50</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>-13.11</td>\n",
       "      <td>-12.59</td>\n",
       "      <td>-9.62</td>\n",
       "      <td>-7.81</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-10.43</td>\n",
       "      <td>-12.90</td>\n",
       "      <td>-11.01</td>\n",
       "      <td>-14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201507</th>\n",
       "      <td>4.03</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.59</td>\n",
       "      <td>6.09</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.42</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-6.50</td>\n",
       "      <td>-5.69</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-4.13</td>\n",
       "      <td>-5.44</td>\n",
       "      <td>-6.48</td>\n",
       "      <td>-6.54</td>\n",
       "      <td>-5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201508</th>\n",
       "      <td>-4.37</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>-8.61</td>\n",
       "      <td>-6.94</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>-8.37</td>\n",
       "      <td>-7.15</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201509</th>\n",
       "      <td>-1.19</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-9.94</td>\n",
       "      <td>-5.32</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-7.28</td>\n",
       "      <td>-8.38</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>...</td>\n",
       "      <td>8.84</td>\n",
       "      <td>11.26</td>\n",
       "      <td>8.16</td>\n",
       "      <td>10.19</td>\n",
       "      <td>6.48</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.90</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201510</th>\n",
       "      <td>5.81</td>\n",
       "      <td>8.06</td>\n",
       "      <td>10.90</td>\n",
       "      <td>14.61</td>\n",
       "      <td>12.21</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.74</td>\n",
       "      <td>16.62</td>\n",
       "      <td>7.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201511</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.03</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-4.64</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>-5.02</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201512</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>-6.18</td>\n",
       "      <td>1.86</td>\n",
       "      <td>-4.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-5.09</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>-5.27</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>-8.68</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-9.63</td>\n",
       "      <td>-3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>4.28</td>\n",
       "      <td>-8.15</td>\n",
       "      <td>-5.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-9.43</td>\n",
       "      <td>-11.10</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.89</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>0.95</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4.25</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.76</td>\n",
       "      <td>8.86</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.18</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.36</td>\n",
       "      <td>6.65</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>4.69</td>\n",
       "      <td>5.61</td>\n",
       "      <td>5.04</td>\n",
       "      <td>8.61</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.90</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>-5.46</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.19</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.42</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-4.96</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-2.53</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-5.30</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201606</th>\n",
       "      <td>4.75</td>\n",
       "      <td>5.31</td>\n",
       "      <td>6.87</td>\n",
       "      <td>-4.43</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>...</td>\n",
       "      <td>2.27</td>\n",
       "      <td>7.33</td>\n",
       "      <td>8.20</td>\n",
       "      <td>2.52</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.04</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201607</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-2.79</td>\n",
       "      <td>6.15</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.62</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.29</td>\n",
       "      <td>8.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.56</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.09</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>4.88</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201608</th>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4.33</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>2.86</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>-3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201609</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>4.62</td>\n",
       "      <td>-3.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-6.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>-4.87</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.85</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.24</td>\n",
       "      <td>-5.49</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201610</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>4.59</td>\n",
       "      <td>5.59</td>\n",
       "      <td>-10.28</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-7.45</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.39</td>\n",
       "      <td>4.21</td>\n",
       "      <td>12.75</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.99</td>\n",
       "      <td>8.47</td>\n",
       "      <td>12.84</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201611</th>\n",
       "      <td>-4.41</td>\n",
       "      <td>-5.76</td>\n",
       "      <td>-5.12</td>\n",
       "      <td>3.87</td>\n",
       "      <td>8.15</td>\n",
       "      <td>-4.18</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.55</td>\n",
       "      <td>1.58</td>\n",
       "      <td>...</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201612</th>\n",
       "      <td>4.43</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>...</td>\n",
       "      <td>3.36</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201701</th>\n",
       "      <td>0.95</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>5.61</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.79</td>\n",
       "      <td>6.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.23</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.35</td>\n",
       "      <td>4.58</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201702</th>\n",
       "      <td>1.71</td>\n",
       "      <td>6.13</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>4.83</td>\n",
       "      <td>3.81</td>\n",
       "      <td>7.03</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201703</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.03</td>\n",
       "      <td>5.79</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>3.21</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>0.76</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>2.74</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6.93</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201705</th>\n",
       "      <td>1.63</td>\n",
       "      <td>4.28</td>\n",
       "      <td>6.12</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201706</th>\n",
       "      <td>-2.65</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9.38</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1.93</td>\n",
       "      <td>4.39</td>\n",
       "      <td>...</td>\n",
       "      <td>5.25</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-2.42</td>\n",
       "      <td>-4.49</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201707</th>\n",
       "      <td>1.52</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-6.06</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-4.55</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201708</th>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.44</td>\n",
       "      <td>6.20</td>\n",
       "      <td>4.46</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201709</th>\n",
       "      <td>0.43</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.54</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.40</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.75</td>\n",
       "      <td>5.93</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4.28</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.22</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>0.71</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-3.09</td>\n",
       "      <td>3.47</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1.23</td>\n",
       "      <td>...</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.41</td>\n",
       "      <td>4.21</td>\n",
       "      <td>6.66</td>\n",
       "      <td>3.37</td>\n",
       "      <td>9.38</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201711</th>\n",
       "      <td>4.15</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.30</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.55</td>\n",
       "      <td>7.94</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.21</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201712</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.31</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.77</td>\n",
       "      <td>...</td>\n",
       "      <td>3.08</td>\n",
       "      <td>9.25</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4.84</td>\n",
       "      <td>11.37</td>\n",
       "      <td>3.12</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Food  Beer  Smoke  Games  Books  Hshld  Clths   Hlth  Chems  Txtls  \\\n",
       "yyyymm                                                                        \n",
       "195912   2.01  0.35  -3.02   1.64   7.29   0.67   1.87  -1.97   3.08   0.74   \n",
       "196001  -4.49 -5.71  -2.05   1.21  -5.47  -7.84  -8.53  -6.68 -10.03  -4.77   \n",
       "196002   3.35 -2.14   2.27   4.23   2.39   9.31   1.44  -0.02  -0.74   0.32   \n",
       "196003  -1.67 -2.94  -0.18  -0.65   2.18  -0.56  -2.59   1.26  -2.75  -6.79   \n",
       "196004   1.17 -2.16   1.35   6.46  -1.17  -1.27   0.21   1.49  -5.53  -1.10   \n",
       "196005   8.20 -0.52   2.44   7.28  11.67   7.74   1.74  13.50   3.40   2.10   \n",
       "196006   5.39  0.47   4.73   2.24   0.02   6.38  -1.59  -0.40   0.45   4.04   \n",
       "196007  -2.11 -0.79   4.60  -4.72   0.23  -0.60  -1.10  -3.99  -6.80  -3.14   \n",
       "196008   4.57  3.24   5.20   7.16   3.63   5.09   3.34   2.29   1.17  -0.84   \n",
       "196009  -3.88 -5.00  -2.09  -2.33  -6.20  -9.18  -4.23  -8.87  -6.70  -5.25   \n",
       "196010   1.02  0.54   3.87   0.11   2.38   6.48  -3.50  -3.71  -1.59  -3.06   \n",
       "196011   9.46  6.57   5.44  13.91  10.11   9.13   3.15   3.91   4.25   2.04   \n",
       "196012   4.51 -0.31   3.54   7.77   7.41   1.76   3.28   6.06   2.85   0.52   \n",
       "196101   4.70  5.23   8.77   0.56   9.47   4.36   5.94   5.86   6.46  11.21   \n",
       "196102   4.21  8.16   5.41  22.33   2.15   5.90   7.84   5.05   2.13   6.81   \n",
       "196103   4.64  2.55   5.60   7.18   4.77   6.34   3.08   3.60   0.92   5.92   \n",
       "196104  -1.39  1.40  -0.23  -2.21  -6.37   2.66   2.60  -0.47  -1.47  -5.31   \n",
       "196105   4.20  5.38   3.39  -3.91   2.71  -0.02   6.80   2.10   5.50   5.47   \n",
       "196106  -2.17 -3.12   3.97  -5.87  -3.85   3.43  -5.50  -3.58  -1.32  -3.36   \n",
       "196107   2.72  0.88   5.95  -1.21  -2.55   1.97   2.03   3.27   2.95   1.53   \n",
       "196108   4.92  3.20   7.74   0.89   0.89  10.45   5.21   3.70   2.35   5.77   \n",
       "196109  -0.62 -1.48  -0.07   1.24   0.75  -3.05  -1.14  -1.48  -4.45  -4.25   \n",
       "196110   3.73 -0.84   7.05  -5.26   0.99  -0.67   8.28   3.33   0.05   3.11   \n",
       "196111   5.28  4.47   8.03   0.25   3.75   4.51   5.30   3.12   2.49   7.37   \n",
       "196112  -3.69  1.41  -6.12   1.97  -3.66  -3.78   0.32  -2.21  -0.16  -1.17   \n",
       "196201  -6.67 -3.45  -4.28 -13.23  -3.44  -7.37  -5.89  -4.86  -4.76   0.57   \n",
       "196202  -0.25  0.28   0.68  -2.02  -0.52  -0.90   2.01   3.56   3.30   1.93   \n",
       "196203   0.98 -0.34  -6.67  -5.34   0.41   4.31  -1.18   0.34  -2.72  -0.74   \n",
       "196204  -4.59 -3.59 -12.99 -11.04  -8.74  -7.03  -8.01 -11.23  -6.23  -7.53   \n",
       "196205 -11.25 -9.05 -14.14 -11.39 -14.87 -10.19 -10.01 -11.14  -8.25  -7.50   \n",
       "...       ...   ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "201507   4.03  3.51   9.59   6.09  -2.90   0.71   5.96   3.66  -4.90  -0.72   \n",
       "201508  -4.37 -3.12  -4.06  -7.35  -8.61  -6.94  -3.86  -8.37  -7.15  -3.11   \n",
       "201509  -1.19  2.58   2.37  -9.94  -5.32  -0.53   1.18  -7.28  -8.38  -5.92   \n",
       "201510   5.81  8.06  10.90  14.61  12.21   5.81   0.98   7.74  16.62   7.96   \n",
       "201511   0.11 -0.71  -3.00  -0.41  -1.17  -1.10  -1.08   0.71   1.68  -2.59   \n",
       "201512   1.96  0.30   1.59  -1.70  -6.18   1.86  -4.38   0.39  -4.82  -2.65   \n",
       "201601  -1.67 -0.23   4.28  -8.15  -5.28   0.16   1.52  -9.43 -11.10  -5.33   \n",
       "201602   0.95 -2.34   0.93   4.25  -0.96   0.34   0.81  -1.09   6.79   0.63   \n",
       "201603   4.69  5.61   5.04   8.61   6.88   4.65   2.30   2.90   8.33   3.58   \n",
       "201604   0.63  0.31  -0.25  -6.30   1.82  -0.42  -2.27   3.55   3.77   1.55   \n",
       "201605   2.06 -0.91   0.83   5.42  -0.53  -0.09  -4.96   2.46  -1.42  -1.70   \n",
       "201606   4.75  5.31   6.87  -4.43  -0.34   3.16   1.63   0.11  -1.14  -4.67   \n",
       "201607  -0.51  1.82  -2.79   6.15   7.38   2.58   1.62   6.00   4.29   8.39   \n",
       "201608  -0.52 -0.90  -1.22   0.94   0.29   1.24   1.37  -3.24   2.48   1.03   \n",
       "201609  -2.92  1.63  -2.78   4.62  -3.95   0.00  -6.92   0.35  -1.76  -4.87   \n",
       "201610  -0.33 -1.65   4.59   5.59 -10.28  -2.96  -5.76  -7.45  -1.95  -4.17   \n",
       "201611  -4.41 -5.76  -5.12   3.87   8.15  -4.18   1.80   1.37   7.55   1.58   \n",
       "201612   4.43  3.00   5.39  -3.36   1.98   1.43  -0.44   0.82   0.32  -1.27   \n",
       "201701   0.95 -1.02   5.61   4.84   1.60   2.72  -0.01   2.17   3.79   6.90   \n",
       "201702   1.71  6.13   7.93   0.65  -0.39   4.83   3.81   7.03   3.10  -2.12   \n",
       "201703   0.52  0.89   1.03   5.79  -0.92  -0.36   0.32  -0.19   2.17   1.94   \n",
       "201704   0.76  1.89  -0.14   2.74  -0.83  -0.36  -1.24   1.06   1.28   3.24   \n",
       "201705   1.63  4.28   6.12   2.80  -1.71   1.77  -2.17  -0.50  -0.63  -0.14   \n",
       "201706  -2.65 -1.20  -1.00  -0.20   2.69  -0.19   9.38   5.49   1.93   4.39   \n",
       "201707   1.52  1.18  -6.06   4.98   2.34   1.79   1.25   0.45   1.03   1.22   \n",
       "201708  -2.77  0.94  -0.96  -1.78  -6.37  -0.04  -5.79   1.69   1.52   2.62   \n",
       "201709   0.43 -3.00  -2.13   3.66   3.54  -0.21   1.01   1.40   6.58   0.45   \n",
       "201710   0.71  1.24  -2.90   2.02  -1.90  -3.09   3.47  -2.35   5.52   1.23   \n",
       "201711   4.15  4.33   1.34   3.30  10.00   4.55   7.94   2.28   2.17   3.80   \n",
       "201712  -0.10  4.31   4.87   0.80   1.06   2.31   4.65  -0.32   0.49  -1.77   \n",
       "\n",
       "           ...      Telcm.lead  Servs.lead  BusEq.lead  Paper.lead  \\\n",
       "yyyymm     ...                                                       \n",
       "195912     ...            0.62       -6.18       -7.93       -9.41   \n",
       "196001     ...            8.07        9.13        5.09        3.00   \n",
       "196002     ...           -0.21       -0.31        3.34       -2.43   \n",
       "196003     ...           -1.24        7.14        1.77        0.41   \n",
       "196004     ...            3.05       -1.75       11.90        2.85   \n",
       "196005     ...           -0.58       -8.07        2.39        3.50   \n",
       "196006     ...           -0.03        2.84       -2.02       -4.10   \n",
       "196007     ...            6.94        5.69        2.71        1.18   \n",
       "196008     ...           -6.07       -3.53       -7.61       -7.37   \n",
       "196009     ...           -0.08        4.62       -3.40       -1.85   \n",
       "196010     ...            4.06        9.49        8.19        5.31   \n",
       "196011     ...           12.29        8.18        4.29        5.57   \n",
       "196012     ...            7.70        4.29        5.08        4.56   \n",
       "196101     ...            0.61        0.20        4.54        6.83   \n",
       "196102     ...            7.23       -0.20        2.31       -0.69   \n",
       "196103     ...            0.63       -0.12        2.19       -0.37   \n",
       "196104     ...           -1.22       -0.70        1.57        1.39   \n",
       "196105     ...           -4.19        0.13       -3.31       -4.46   \n",
       "196106     ...            6.25       -8.25        0.56       -0.50   \n",
       "196107     ...            0.12        8.99        5.11        5.37   \n",
       "196108     ...           -2.94       -6.04        1.01       -2.74   \n",
       "196109     ...            0.00        2.24        6.53        1.74   \n",
       "196110     ...            8.34        8.27        0.99        2.05   \n",
       "196111     ...            3.14       -0.68       -0.55       -2.65   \n",
       "196112     ...           -6.32       -4.88       -6.91       -5.22   \n",
       "196201     ...            3.89       -1.94       -0.07        3.87   \n",
       "196202     ...           -3.14       -1.41       -0.76        1.13   \n",
       "196203     ...           -4.93       -3.11      -12.01       -7.93   \n",
       "196204     ...           -7.35      -10.97      -13.19      -11.03   \n",
       "196205     ...           -8.72      -13.11      -12.59       -9.62   \n",
       "...        ...             ...         ...         ...         ...   \n",
       "201507     ...           -8.42       -5.29       -6.50       -5.69   \n",
       "201508     ...           -2.63       -1.47       -1.72       -2.66   \n",
       "201509     ...            8.84       11.26        8.16       10.19   \n",
       "201510     ...           -1.92        1.99        0.12       -0.02   \n",
       "201511     ...           -3.03       -1.19       -4.64       -3.75   \n",
       "201512     ...           -0.36       -5.09       -7.95       -5.27   \n",
       "201601     ...            1.15       -2.45        1.45        2.99   \n",
       "201602     ...            6.00        7.76        8.86        8.18   \n",
       "201603     ...            0.59       -2.55       -5.46        0.80   \n",
       "201604     ...            0.30        4.67        5.64        1.79   \n",
       "201605     ...            3.10       -2.12       -1.63        2.06   \n",
       "201606     ...            2.27        7.33        8.20        2.52   \n",
       "201607     ...           -3.56        1.19        2.38        2.46   \n",
       "201608     ...            0.52        0.82        4.33       -0.64   \n",
       "201609     ...           -2.85       -0.55       -2.24       -5.49   \n",
       "201610     ...            6.25       -0.01        2.39        4.21   \n",
       "201611     ...            4.65       -0.19        2.07        1.86   \n",
       "201612     ...            3.36        5.45        3.20        2.28   \n",
       "201701     ...           -0.11        3.23        6.99        3.21   \n",
       "201702     ...            0.81        1.81        2.45        0.26   \n",
       "201703     ...           -0.16        3.83        0.99        2.06   \n",
       "201704     ...           -2.00        3.60        4.57        2.60   \n",
       "201705     ...           -2.33       -0.82       -3.44        1.87   \n",
       "201706     ...            5.25        3.98        3.05       -2.42   \n",
       "201707     ...           -2.58        1.36        4.98        0.72   \n",
       "201708     ...           -1.68        0.60        0.80        2.44   \n",
       "201709     ...           -5.75        5.93        7.12        4.28   \n",
       "201710     ...            3.91        0.05        2.41        4.21   \n",
       "201711     ...            4.41        0.39       -0.89       -0.95   \n",
       "201712     ...            3.08        9.25        5.02        4.28   \n",
       "\n",
       "        Trans.lead  Whlsl.lead  Rtail.lead  Meals.lead  Fin.lead  Other.lead  \n",
       "yyyymm                                                                        \n",
       "195912       -4.31       -5.33       -6.09      -10.08     -4.68       -3.98  \n",
       "196001       -0.94        1.42        4.00        1.81     -0.98        6.32  \n",
       "196002       -4.99       -1.37       -0.13       -3.88      0.05       -2.43  \n",
       "196003       -2.13        0.45       -0.53        8.86     -0.64        0.55  \n",
       "196004        0.90        1.65        3.11        0.80     -0.45        1.02  \n",
       "196005        2.17        5.96        3.41        1.03      3.72        6.41  \n",
       "196006       -3.11       -6.16       -2.99       -1.25      0.09       -5.95  \n",
       "196007        1.98        4.51        2.85        2.05      3.47        3.48  \n",
       "196008       -7.07       -8.44       -8.57       -1.90     -5.78       -4.21  \n",
       "196009       -1.02       -4.22        0.31       -4.54     -0.40        0.38  \n",
       "196010        5.35        9.72        6.50        4.40      7.71        4.01  \n",
       "196011        2.27        2.06        2.05        2.08      5.56        3.80  \n",
       "196012        8.35        7.93        2.28        4.08      7.12        8.23  \n",
       "196101        4.22        3.31        4.82        8.23      7.00        6.00  \n",
       "196102        0.86        4.45        5.76        4.06      4.34        7.08  \n",
       "196103       -1.62        3.08        0.22        4.23      1.38       -3.67  \n",
       "196104        4.74       -0.04        4.31       -1.90      4.00        3.32  \n",
       "196105       -4.57       -4.90        0.80       -5.63     -2.88        0.37  \n",
       "196106       -0.32       -0.01        2.45        2.69      3.35        5.37  \n",
       "196107        3.52        3.09        3.03        0.46      8.65        1.64  \n",
       "196108       -1.16       -4.22        0.66       -6.21     -0.40        3.14  \n",
       "196109        2.16        4.30        9.35        0.71      2.02        0.39  \n",
       "196110        0.47        5.65        4.90        1.08      7.22        1.69  \n",
       "196111       -0.24        0.46       -0.63       -2.21     -4.44       -0.77  \n",
       "196112        2.52       -0.79       -9.56       -3.90     -4.99       -3.62  \n",
       "196201        0.32       -0.09        1.58       -0.59      3.59        4.20  \n",
       "196202       -1.68       -2.30        0.90       -4.07     -2.13       -1.83  \n",
       "196203       -6.27       -5.78       -4.61       -9.09     -7.69       -2.12  \n",
       "196204       -5.17      -11.34       -9.09       -7.46    -10.02      -11.83  \n",
       "196205       -7.81      -11.11      -10.43      -12.90    -11.01      -14.25  \n",
       "...            ...         ...         ...         ...       ...         ...  \n",
       "201507       -6.37       -4.13       -5.44       -6.48     -6.54       -5.20  \n",
       "201508       -0.71       -6.04       -1.75        0.44     -3.14       -1.87  \n",
       "201509        6.48        5.07        4.56        5.05      5.90        6.98  \n",
       "201510       -1.10        2.67        0.61       -1.01      2.16        0.05  \n",
       "201511       -5.02       -1.88        0.82       -0.95     -2.92        0.25  \n",
       "201512       -8.53       -8.68       -4.45       -0.94     -9.63       -3.20  \n",
       "201601        6.89        3.85       -0.36        1.03     -2.85        2.71  \n",
       "201602        6.86        6.18        5.99        5.36      6.65        6.68  \n",
       "201603       -1.08        0.49       -0.38       -2.38      3.96        0.67  \n",
       "201604       -2.18        1.78        1.19       -1.48      2.15       -2.02  \n",
       "201605       -2.53        1.81        0.71        1.16     -5.30        3.61  \n",
       "201606        5.39        3.65        3.78        2.19      4.04       -0.21  \n",
       "201607        1.09       -1.03       -1.69       -0.24      4.88        2.24  \n",
       "201608        2.86       -2.56       -0.18       -2.25     -1.45       -3.48  \n",
       "201609       -0.64       -8.18       -3.59       -1.96      1.40       -0.53  \n",
       "201610       12.75        9.29        2.99        8.47     12.84        8.29  \n",
       "201611        0.84        2.34       -0.98        0.58      3.80        2.57  \n",
       "201612        1.70        1.69        0.93        0.71      0.56       -0.87  \n",
       "201701        2.56        2.73        2.79        2.35      4.58        3.71  \n",
       "201702       -2.80       -1.29        0.80        2.73     -2.26       -1.83  \n",
       "201703        2.14       -2.96        3.21        4.30      0.18       -1.06  \n",
       "201704        2.68        2.56        0.45        6.93     -1.03       -0.74  \n",
       "201705        3.38        0.15       -2.45       -2.54      5.70        1.31  \n",
       "201706       -4.49       -0.99        0.93       -2.63      1.86        1.44  \n",
       "201707        0.60       -4.55       -1.81        1.26     -1.31        1.53  \n",
       "201708        6.20        4.46        2.71        0.79      5.07        0.99  \n",
       "201709       -1.99       -1.18        3.22        3.64      3.22       -1.20  \n",
       "201710        6.66        3.37        9.38        5.45      3.76        1.60  \n",
       "201711        2.73        4.21        2.45        1.18      0.88        1.14  \n",
       "201712        2.56        4.84       11.37        3.12      6.00        5.41  \n",
       "\n",
       "[697 rows x 90 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude 2017 and later to tie to paper\n",
    "#data = data.loc[data.index[data.index < 201701]]\n",
    "data = data.loc[data.index[data.index > 195911]]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Beer</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Games</th>\n",
       "      <th>Books</th>\n",
       "      <th>Hshld</th>\n",
       "      <th>Clths</th>\n",
       "      <th>Hlth</th>\n",
       "      <th>Chems</th>\n",
       "      <th>Txtls</th>\n",
       "      <th>...</th>\n",
       "      <th>Telcm.lead</th>\n",
       "      <th>Servs.lead</th>\n",
       "      <th>BusEq.lead</th>\n",
       "      <th>Paper.lead</th>\n",
       "      <th>Trans.lead</th>\n",
       "      <th>Whlsl.lead</th>\n",
       "      <th>Rtail.lead</th>\n",
       "      <th>Meals.lead</th>\n",
       "      <th>Fin.lead</th>\n",
       "      <th>Other.lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.688666</td>\n",
       "      <td>0.727030</td>\n",
       "      <td>0.985079</td>\n",
       "      <td>0.732095</td>\n",
       "      <td>0.532253</td>\n",
       "      <td>0.564333</td>\n",
       "      <td>0.690387</td>\n",
       "      <td>0.665825</td>\n",
       "      <td>0.552367</td>\n",
       "      <td>0.687145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515968</td>\n",
       "      <td>0.729928</td>\n",
       "      <td>0.622970</td>\n",
       "      <td>0.534806</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.631076</td>\n",
       "      <td>0.698235</td>\n",
       "      <td>0.728766</td>\n",
       "      <td>0.637547</td>\n",
       "      <td>0.396628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.308660</td>\n",
       "      <td>5.058992</td>\n",
       "      <td>6.032324</td>\n",
       "      <td>7.128170</td>\n",
       "      <td>5.780362</td>\n",
       "      <td>4.728000</td>\n",
       "      <td>6.355251</td>\n",
       "      <td>4.897557</td>\n",
       "      <td>5.482363</td>\n",
       "      <td>6.970961</td>\n",
       "      <td>...</td>\n",
       "      <td>4.607931</td>\n",
       "      <td>6.486956</td>\n",
       "      <td>6.698787</td>\n",
       "      <td>5.021876</td>\n",
       "      <td>5.707154</td>\n",
       "      <td>5.571040</td>\n",
       "      <td>5.334178</td>\n",
       "      <td>6.065564</td>\n",
       "      <td>5.381389</td>\n",
       "      <td>5.771655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.150000</td>\n",
       "      <td>-20.190000</td>\n",
       "      <td>-25.320000</td>\n",
       "      <td>-33.400000</td>\n",
       "      <td>-26.560000</td>\n",
       "      <td>-22.240000</td>\n",
       "      <td>-31.500000</td>\n",
       "      <td>-21.060000</td>\n",
       "      <td>-28.600000</td>\n",
       "      <td>-33.110000</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.440000</td>\n",
       "      <td>-28.670000</td>\n",
       "      <td>-32.070000</td>\n",
       "      <td>-27.740000</td>\n",
       "      <td>-28.500000</td>\n",
       "      <td>-29.250000</td>\n",
       "      <td>-29.740000</td>\n",
       "      <td>-31.890000</td>\n",
       "      <td>-22.530000</td>\n",
       "      <td>-28.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.630000</td>\n",
       "      <td>-2.080000</td>\n",
       "      <td>-2.740000</td>\n",
       "      <td>-3.390000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>-2.030000</td>\n",
       "      <td>-2.800000</td>\n",
       "      <td>-2.230000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-3.170000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.110000</td>\n",
       "      <td>-3.050000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-2.780000</td>\n",
       "      <td>-2.560000</td>\n",
       "      <td>-2.380000</td>\n",
       "      <td>-2.840000</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.070000</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>4.660000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>3.640000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>4.310000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>4.480000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>4.260000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>3.460000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.890000</td>\n",
       "      <td>25.510000</td>\n",
       "      <td>32.380000</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>33.130000</td>\n",
       "      <td>18.220000</td>\n",
       "      <td>31.790000</td>\n",
       "      <td>29.010000</td>\n",
       "      <td>21.680000</td>\n",
       "      <td>59.030000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.220000</td>\n",
       "      <td>23.380000</td>\n",
       "      <td>24.660000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>17.530000</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>27.380000</td>\n",
       "      <td>20.590000</td>\n",
       "      <td>19.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Food        Beer       Smoke       Games       Books       Hshld  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000  697.000000  697.000000   \n",
       "mean     0.688666    0.727030    0.985079    0.732095    0.532253    0.564333   \n",
       "std      4.308660    5.058992    6.032324    7.128170    5.780362    4.728000   \n",
       "min    -18.150000  -20.190000  -25.320000  -33.400000  -26.560000  -22.240000   \n",
       "25%     -1.630000   -2.080000   -2.740000   -3.390000   -2.600000   -2.030000   \n",
       "50%      0.740000    0.750000    1.270000    0.940000    0.510000    0.750000   \n",
       "75%      3.070000    3.690000    4.660000    5.260000    3.640000    3.540000   \n",
       "max     19.890000   25.510000   32.380000   34.520000   33.130000   18.220000   \n",
       "\n",
       "            Clths        Hlth       Chems       Txtls     ...      Telcm.lead  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000     ...      697.000000   \n",
       "mean     0.690387    0.665825    0.552367    0.687145     ...        0.515968   \n",
       "std      6.355251    4.897557    5.482363    6.970961     ...        4.607931   \n",
       "min    -31.500000  -21.060000  -28.600000  -33.110000     ...      -16.440000   \n",
       "25%     -2.800000   -2.230000   -2.750000   -3.170000     ...       -2.110000   \n",
       "50%      0.700000    0.760000    0.720000    0.640000     ...        0.590000   \n",
       "75%      4.310000    3.550000    3.760000    4.480000     ...        3.360000   \n",
       "max     31.790000   29.010000   21.680000   59.030000     ...       21.220000   \n",
       "\n",
       "       Servs.lead  BusEq.lead  Paper.lead  Trans.lead  Whlsl.lead  Rtail.lead  \\\n",
       "count  697.000000  697.000000  697.000000  697.000000  697.000000  697.000000   \n",
       "mean     0.729928    0.622970    0.534806    0.601090    0.631076    0.698235   \n",
       "std      6.486956    6.698787    5.021876    5.707154    5.571040    5.334178   \n",
       "min    -28.670000  -32.070000  -27.740000  -28.500000  -29.250000  -29.740000   \n",
       "25%     -3.050000   -3.220000   -2.400000   -2.780000   -2.560000   -2.380000   \n",
       "50%      1.010000    0.670000    0.710000    0.900000    0.940000    0.540000   \n",
       "75%      4.260000    4.630000    3.460000    4.040000    3.880000    3.980000   \n",
       "max     23.380000   24.660000   21.000000   18.500000   17.530000   26.490000   \n",
       "\n",
       "       Meals.lead    Fin.lead  Other.lead  \n",
       "count  697.000000  697.000000  697.000000  \n",
       "mean     0.728766    0.637547    0.396628  \n",
       "std      6.065564    5.381389    5.771655  \n",
       "min    -31.890000  -22.530000  -28.090000  \n",
       "25%     -2.840000   -2.400000   -2.930000  \n",
       "50%      1.080000    0.870000    0.540000  \n",
       "75%      4.300000    4.000000    4.200000  \n",
       "max     27.380000   20.590000   19.960000  \n",
       "\n",
       "[8 rows x 90 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = data.describe()\n",
    "desc\n",
    "# min, max line up with Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Ann. Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.073929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beer</th>\n",
       "      <td>0.074309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <td>0.100741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Games</th>\n",
       "      <td>0.058342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books</th>\n",
       "      <td>0.044662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hshld</th>\n",
       "      <td>0.055568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clths</th>\n",
       "      <td>0.060067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hlth</th>\n",
       "      <td>0.067552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chems</th>\n",
       "      <td>0.049242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Txtls</th>\n",
       "      <td>0.054817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnstr</th>\n",
       "      <td>0.046665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steel</th>\n",
       "      <td>0.006028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FabPr</th>\n",
       "      <td>0.050970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElcEq</th>\n",
       "      <td>0.065001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autos</th>\n",
       "      <td>0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carry</th>\n",
       "      <td>0.069454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mines</th>\n",
       "      <td>0.034906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coal</th>\n",
       "      <td>0.027115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oil</th>\n",
       "      <td>0.061626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Util</th>\n",
       "      <td>0.050316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Telcm</th>\n",
       "      <td>0.050423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Servs</th>\n",
       "      <td>0.062007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusEq</th>\n",
       "      <td>0.047654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper</th>\n",
       "      <td>0.049591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trans</th>\n",
       "      <td>0.053606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whlsl</th>\n",
       "      <td>0.057449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rtail</th>\n",
       "      <td>0.067139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meals</th>\n",
       "      <td>0.066807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fin</th>\n",
       "      <td>0.059503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.025495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mean Ann. Return\n",
       "Food           0.073929\n",
       "Beer           0.074309\n",
       "Smoke          0.100741\n",
       "Games          0.058342\n",
       "Books          0.044662\n",
       "Hshld          0.055568\n",
       "Clths          0.060067\n",
       "Hlth           0.067552\n",
       "Chems          0.049242\n",
       "Txtls          0.054817\n",
       "Cnstr          0.046665\n",
       "Steel          0.006028\n",
       "FabPr          0.050970\n",
       "ElcEq          0.065001\n",
       "Autos          0.030769\n",
       "Carry          0.069454\n",
       "Mines          0.034906\n",
       "Coal           0.027115\n",
       "Oil            0.061626\n",
       "Util           0.050316\n",
       "Telcm          0.050423\n",
       "Servs          0.062007\n",
       "BusEq          0.047654\n",
       "Paper          0.049591\n",
       "Trans          0.053606\n",
       "Whlsl          0.057449\n",
       "Rtail          0.067139\n",
       "Meals          0.066807\n",
       "Fin            0.059503\n",
       "Other          0.025495"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annualized returns don't match Table 1, oddly\n",
    "# geometric mean, annualized\n",
    "pd.DataFrame((np.prod(data/100 + 1)**(12.0/len(data))-1)[:30], columns=['Mean Ann. Return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Ann. Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.085843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beer</th>\n",
       "      <td>0.090818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <td>0.124829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Games</th>\n",
       "      <td>0.091476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books</th>\n",
       "      <td>0.065774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hshld</th>\n",
       "      <td>0.069862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clths</th>\n",
       "      <td>0.086066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hlth</th>\n",
       "      <td>0.082891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chems</th>\n",
       "      <td>0.068335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Txtls</th>\n",
       "      <td>0.085646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnstr</th>\n",
       "      <td>0.069331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steel</th>\n",
       "      <td>0.038837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FabPr</th>\n",
       "      <td>0.074827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElcEq</th>\n",
       "      <td>0.089624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autos</th>\n",
       "      <td>0.058334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carry</th>\n",
       "      <td>0.094998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mines</th>\n",
       "      <td>0.070076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coal</th>\n",
       "      <td>0.092509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oil</th>\n",
       "      <td>0.079734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Util</th>\n",
       "      <td>0.060258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Telcm</th>\n",
       "      <td>0.063866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Servs</th>\n",
       "      <td>0.089106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusEq</th>\n",
       "      <td>0.076570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper</th>\n",
       "      <td>0.065584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trans</th>\n",
       "      <td>0.074466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whlsl</th>\n",
       "      <td>0.077466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rtail</th>\n",
       "      <td>0.085418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meals</th>\n",
       "      <td>0.090967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fin</th>\n",
       "      <td>0.078166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.046508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mean Ann. Return\n",
       "Food           0.085843\n",
       "Beer           0.090818\n",
       "Smoke          0.124829\n",
       "Games          0.091476\n",
       "Books          0.065774\n",
       "Hshld          0.069862\n",
       "Clths          0.086066\n",
       "Hlth           0.082891\n",
       "Chems          0.068335\n",
       "Txtls          0.085646\n",
       "Cnstr          0.069331\n",
       "Steel          0.038837\n",
       "FabPr          0.074827\n",
       "ElcEq          0.089624\n",
       "Autos          0.058334\n",
       "Carry          0.094998\n",
       "Mines          0.070076\n",
       "Coal           0.092509\n",
       "Oil            0.079734\n",
       "Util           0.060258\n",
       "Telcm          0.063866\n",
       "Servs          0.089106\n",
       "BusEq          0.076570\n",
       "Paper          0.065584\n",
       "Trans          0.074466\n",
       "Whlsl          0.077466\n",
       "Rtail          0.085418\n",
       "Meals          0.090967\n",
       "Fin            0.078166\n",
       "Other          0.046508"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try this way, arithmetic mean then annualize (not very correct)\n",
    "#print(pd.DataFrame(((desc.loc['mean']/100+1)**12-1)[:30]))\n",
    "#nope\n",
    "\n",
    "# same\n",
    "pd.DataFrame(((1 + np.mean(data, axis=0)/100)**12 -1)[:30], columns=['Mean Ann. Return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>14.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beer</th>\n",
       "      <td>17.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoke</th>\n",
       "      <td>20.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Games</th>\n",
       "      <td>24.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books</th>\n",
       "      <td>20.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hshld</th>\n",
       "      <td>16.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clths</th>\n",
       "      <td>22.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hlth</th>\n",
       "      <td>16.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chems</th>\n",
       "      <td>18.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Txtls</th>\n",
       "      <td>24.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnstr</th>\n",
       "      <td>20.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steel</th>\n",
       "      <td>25.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FabPr</th>\n",
       "      <td>21.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElcEq</th>\n",
       "      <td>21.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autos</th>\n",
       "      <td>23.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carry</th>\n",
       "      <td>21.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mines</th>\n",
       "      <td>25.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coal</th>\n",
       "      <td>35.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oil</th>\n",
       "      <td>18.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Util</th>\n",
       "      <td>13.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Telcm</th>\n",
       "      <td>15.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Servs</th>\n",
       "      <td>22.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusEq</th>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paper</th>\n",
       "      <td>17.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trans</th>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whlsl</th>\n",
       "      <td>19.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rtail</th>\n",
       "      <td>18.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meals</th>\n",
       "      <td>21.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fin</th>\n",
       "      <td>18.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         std\n",
       "Food   14.93\n",
       "Beer   17.52\n",
       "Smoke  20.90\n",
       "Games  24.69\n",
       "Books  20.02\n",
       "Hshld  16.38\n",
       "Clths  22.02\n",
       "Hlth   16.97\n",
       "Chems  18.99\n",
       "Txtls  24.15\n",
       "Cnstr  20.64\n",
       "Steel  25.15\n",
       "FabPr  21.06\n",
       "ElcEq  21.37\n",
       "Autos  23.01\n",
       "Carry  21.66\n",
       "Mines  25.74\n",
       "Coal   35.18\n",
       "Oil    18.48\n",
       "Util   13.76\n",
       "Telcm  15.97\n",
       "Servs  22.45\n",
       "BusEq  23.20\n",
       "Paper  17.39\n",
       "Trans  19.77\n",
       "Whlsl  19.29\n",
       "Rtail  18.43\n",
       "Meals  21.01\n",
       "Fin    18.63\n",
       "Other  20.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#annualized volatility \n",
    "pd.DataFrame((desc.loc['std']*np.sqrt(12))[:30].round(2))\n",
    "# lines up with table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run LASSO, then OLS on selected variables\n",
    "\n",
    "# skip last row to better match published r-squared\n",
    "# looks like they forecast actuals 1960-2016 using 1959m12 to 2016m11\n",
    "# not exact matches to Table 2 R-squared but almost within rounding error \n",
    "X = data.values[:-1,:npredictors]\n",
    "Y = data.values[:-1,-nresponses:]\n",
    "nrows = X.shape[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO variables selected for Food.lead: \n",
      "['Clths', 'Coal', 'Oil', 'Util', 'Rtail', 'Meals', 'Beer.3m', 'Mines.3m', 'Util.3m']\n",
      "Running OLS for Food.lead against ['Clths', 'Coal', 'Oil', 'Util', 'Rtail', 'Meals', 'Beer.3m', 'Mines.3m', 'Util.3m']\n",
      "In-sample OLS R-squared: 4.38\n",
      "---\n",
      "LASSO variables selected for Beer.lead: \n",
      "['Food', 'Clths', 'Coal', 'Beer.3m', 'Hlth.3m', 'Util.3m']\n",
      "Running OLS for Beer.lead against ['Food', 'Clths', 'Coal', 'Beer.3m', 'Hlth.3m', 'Util.3m']\n",
      "In-sample OLS R-squared: 4.16\n",
      "---\n",
      "LASSO variables selected for Smoke.lead: \n",
      "['Txtls', 'Carry', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'Trans', 'Food.3m', 'Beer.3m', 'Hshld.3m', 'Clths.3m', 'Chems.3m', 'ElcEq.3m', 'Mines.3m', 'Oil.3m', 'Util.3m', 'Servs.3m', 'Paper.3m', 'Other.3m']\n",
      "Running OLS for Smoke.lead against ['Txtls', 'Carry', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'Trans', 'Food.3m', 'Beer.3m', 'Hshld.3m', 'Clths.3m', 'Chems.3m', 'ElcEq.3m', 'Mines.3m', 'Oil.3m', 'Util.3m', 'Servs.3m', 'Paper.3m', 'Other.3m']\n",
      "In-sample OLS R-squared: 11.23\n",
      "---\n",
      "LASSO variables selected for Games.lead: \n",
      "['Books', 'Clths', 'Coal', 'Fin', 'Steel.3m']\n",
      "Running OLS for Games.lead against ['Books', 'Clths', 'Coal', 'Fin', 'Steel.3m']\n",
      "In-sample OLS R-squared: 5.53\n",
      "---\n",
      "LASSO variables selected for Books.lead: \n",
      "['Games', 'Books', 'Clths', 'Autos', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Books.3m', 'Chems.3m', 'Steel.3m', 'ElcEq.3m', 'Autos.3m', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'Paper.3m']\n",
      "Running OLS for Books.lead against ['Games', 'Books', 'Clths', 'Autos', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Books.3m', 'Chems.3m', 'Steel.3m', 'ElcEq.3m', 'Autos.3m', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 10.55\n",
      "---\n",
      "LASSO variables selected for Hshld.lead: \n",
      "['Clths', 'Coal', 'Rtail', 'Steel.3m', 'Util.3m']\n",
      "Running OLS for Hshld.lead against ['Clths', 'Coal', 'Rtail', 'Steel.3m', 'Util.3m']\n",
      "In-sample OLS R-squared: 3.83\n",
      "---\n",
      "LASSO variables selected for Clths.lead: \n",
      "['Clths', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Steel.3m', 'Util.3m']\n",
      "Running OLS for Clths.lead against ['Clths', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Steel.3m', 'Util.3m']\n",
      "In-sample OLS R-squared: 6.91\n",
      "---\n",
      "LASSO variables selected for Hlth.lead: \n",
      "['Books', 'Mines', 'Coal', 'Util', 'Mines.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Hlth.lead against ['Books', 'Mines', 'Coal', 'Util', 'Mines.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 3.65\n",
      "---\n",
      "LASSO variables selected for Chems.lead: \n",
      "['Clths', 'Autos', 'Coal', 'Oil', 'Rtail', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Chems.lead against ['Clths', 'Autos', 'Coal', 'Oil', 'Rtail', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 5.76\n",
      "---\n",
      "LASSO variables selected for Txtls.lead: \n",
      "['Clths', 'Autos', 'Coal', 'Oil', 'BusEq', 'Rtail', 'Fin', 'Chems.3m', 'Txtls.3m', 'ElcEq.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Txtls.lead against ['Clths', 'Autos', 'Coal', 'Oil', 'BusEq', 'Rtail', 'Fin', 'Chems.3m', 'Txtls.3m', 'ElcEq.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 13.00\n",
      "---\n",
      "LASSO variables selected for Cnstr.lead: \n",
      "['Clths', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Trans', 'Fin', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Cnstr.lead against ['Clths', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Trans', 'Fin', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 8.33\n",
      "---\n",
      "LASSO variables selected for Steel.lead: \n",
      "['Fin']\n",
      "Running OLS for Steel.lead against ['Fin']\n",
      "In-sample OLS R-squared: 1.29\n",
      "---\n",
      "LASSO variables selected for FabPr.lead: \n",
      "['Smoke', 'Games', 'Books', 'Hshld', 'Coal', 'Oil', 'Util', 'Telcm', 'BusEq', 'Trans', 'Fin', 'Games.3m', 'Hlth.3m', 'Chems.3m', 'Steel.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Paper.3m', 'Rtail.3m']\n",
      "Running OLS for FabPr.lead against ['Smoke', 'Games', 'Books', 'Hshld', 'Coal', 'Oil', 'Util', 'Telcm', 'BusEq', 'Trans', 'Fin', 'Games.3m', 'Hlth.3m', 'Chems.3m', 'Steel.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Paper.3m', 'Rtail.3m']\n",
      "In-sample OLS R-squared: 9.04\n",
      "---\n",
      "LASSO variables selected for ElcEq.lead: \n",
      "['Coal', 'Oil', 'Util', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Smoke.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "Running OLS for ElcEq.lead against ['Coal', 'Oil', 'Util', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Smoke.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 4.33\n",
      "---\n",
      "LASSO variables selected for Autos.lead: \n",
      "['Smoke', 'Hshld', 'Clths', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m']\n",
      "Running OLS for Autos.lead against ['Smoke', 'Hshld', 'Clths', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m']\n",
      "In-sample OLS R-squared: 8.36\n",
      "---\n",
      "LASSO variables selected for Carry.lead: \n",
      "['Beer', 'Hlth', 'Txtls', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Trans', 'Fin', 'Smoke.3m', 'Books.3m', 'Clths.3m', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'BusEq.3m', 'Paper.3m', 'Rtail.3m']\n",
      "Running OLS for Carry.lead against ['Beer', 'Hlth', 'Txtls', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Trans', 'Fin', 'Smoke.3m', 'Books.3m', 'Clths.3m', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'BusEq.3m', 'Paper.3m', 'Rtail.3m']\n",
      "In-sample OLS R-squared: 9.96\n",
      "---\n",
      "LASSO variables selected for Mines.lead: \n",
      "[]\n",
      "No coefs selected for Mines.lead, using all\n",
      "---\n",
      "Running OLS for Mines.lead against ['Food', 'Beer', 'Smoke', 'Games', 'Books', 'Hshld', 'Clths', 'Hlth', 'Chems', 'Txtls', 'Cnstr', 'Steel', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Mines', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'BusEq', 'Paper', 'Trans', 'Whlsl', 'Rtail', 'Meals', 'Fin', 'Other', 'Food.3m', 'Beer.3m', 'Smoke.3m', 'Games.3m', 'Books.3m', 'Hshld.3m', 'Clths.3m', 'Hlth.3m', 'Chems.3m', 'Txtls.3m', 'Cnstr.3m', 'Steel.3m', 'FabPr.3m', 'ElcEq.3m', 'Autos.3m', 'Carry.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Whlsl.3m', 'Rtail.3m', 'Meals.3m', 'Fin.3m', 'Other.3m']\n",
      "In-sample OLS R-squared: 8.45\n",
      "---\n",
      "LASSO variables selected for Coal.lead: \n",
      "['Beer', 'Smoke', 'Books', 'Oil', 'Paper', 'Food.3m', 'Smoke.3m', 'Clths.3m', 'Steel.3m', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Telcm.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Rtail.3m', 'Other.3m']\n",
      "Running OLS for Coal.lead against ['Beer', 'Smoke', 'Books', 'Oil', 'Paper', 'Food.3m', 'Smoke.3m', 'Clths.3m', 'Steel.3m', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Telcm.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Rtail.3m', 'Other.3m']\n",
      "In-sample OLS R-squared: 6.91\n",
      "---\n",
      "LASSO variables selected for Oil.lead: \n",
      "['Beer', 'Smoke', 'Hlth', 'Carry', 'Hshld.3m', 'Autos.3m', 'Coal.3m']\n",
      "Running OLS for Oil.lead against ['Beer', 'Smoke', 'Hlth', 'Carry', 'Hshld.3m', 'Autos.3m', 'Coal.3m']\n",
      "In-sample OLS R-squared: 3.35\n",
      "---\n",
      "LASSO variables selected for Util.lead: \n",
      "['Beer', 'Hlth', 'Cnstr', 'FabPr', 'Carry', 'Mines', 'Oil', 'Util', 'Telcm', 'Whlsl', 'Fin', 'Other', 'Beer.3m', 'Cnstr.3m', 'ElcEq.3m', 'Mines.3m', 'Telcm.3m', 'Paper.3m', 'Fin.3m', 'Other.3m']\n",
      "Running OLS for Util.lead against ['Beer', 'Hlth', 'Cnstr', 'FabPr', 'Carry', 'Mines', 'Oil', 'Util', 'Telcm', 'Whlsl', 'Fin', 'Other', 'Beer.3m', 'Cnstr.3m', 'ElcEq.3m', 'Mines.3m', 'Telcm.3m', 'Paper.3m', 'Fin.3m', 'Other.3m']\n",
      "In-sample OLS R-squared: 9.37\n",
      "---\n",
      "LASSO variables selected for Telcm.lead: \n",
      "['Smoke', 'Books', 'Hshld', 'Mines', 'Coal', 'Oil', 'Util', 'Rtail', 'Meals', 'Fin', 'Beer.3m', 'Smoke.3m', 'Books.3m', 'Txtls.3m', 'Carry.3m', 'Telcm.3m']\n",
      "Running OLS for Telcm.lead against ['Smoke', 'Books', 'Hshld', 'Mines', 'Coal', 'Oil', 'Util', 'Rtail', 'Meals', 'Fin', 'Beer.3m', 'Smoke.3m', 'Books.3m', 'Txtls.3m', 'Carry.3m', 'Telcm.3m']\n",
      "In-sample OLS R-squared: 6.69\n",
      "---\n",
      "LASSO variables selected for Servs.lead: \n",
      "['Smoke', 'Books', 'Oil', 'Fin', 'Steel.3m', 'Mines.3m', 'Telcm.3m', 'Servs.3m']\n",
      "Running OLS for Servs.lead against ['Smoke', 'Books', 'Oil', 'Fin', 'Steel.3m', 'Mines.3m', 'Telcm.3m', 'Servs.3m']\n",
      "In-sample OLS R-squared: 5.09\n",
      "---\n",
      "LASSO variables selected for BusEq.lead: \n",
      "['Smoke', 'Books', 'Hlth', 'Util', 'Txtls.3m', 'Telcm.3m']\n",
      "Running OLS for BusEq.lead against ['Smoke', 'Books', 'Hlth', 'Util', 'Txtls.3m', 'Telcm.3m']\n",
      "In-sample OLS R-squared: 4.04\n",
      "---\n",
      "LASSO variables selected for Paper.lead: \n",
      "['Clths', 'Coal', 'Oil', 'Rtail', 'Fin', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Paper.lead against ['Clths', 'Coal', 'Oil', 'Rtail', 'Fin', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 5.80\n",
      "---\n",
      "LASSO variables selected for Trans.lead: \n",
      "['Books', 'Clths', 'Coal', 'Oil', 'Servs', 'Fin', 'Other', 'Beer.3m', 'Steel.3m', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Trans.lead against ['Books', 'Clths', 'Coal', 'Oil', 'Servs', 'Fin', 'Other', 'Beer.3m', 'Steel.3m', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 6.66\n",
      "---\n",
      "LASSO variables selected for Whlsl.lead: \n",
      "['Smoke', 'Books', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'Fin', 'Other', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Whlsl.lead against ['Smoke', 'Books', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'Fin', 'Other', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 7.08\n",
      "---\n",
      "LASSO variables selected for Rtail.lead: \n",
      "['Steel', 'Mines', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Steel.3m', 'Util.3m', 'Paper.3m']\n",
      "Running OLS for Rtail.lead against ['Steel', 'Mines', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Steel.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 6.28\n",
      "---\n",
      "LASSO variables selected for Meals.lead: \n",
      "['Smoke', 'Books', 'Clths', 'Steel', 'Carry', 'Coal', 'Oil', 'Servs', 'BusEq', 'Meals', 'Fin', 'Beer.3m', 'Games.3m', 'Steel.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "Running OLS for Meals.lead against ['Smoke', 'Books', 'Clths', 'Steel', 'Carry', 'Coal', 'Oil', 'Servs', 'BusEq', 'Meals', 'Fin', 'Beer.3m', 'Games.3m', 'Steel.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 9.90\n",
      "---\n",
      "LASSO variables selected for Fin.lead: \n",
      "['Fin']\n",
      "Running OLS for Fin.lead against ['Fin']\n",
      "In-sample OLS R-squared: 1.68\n",
      "---\n",
      "LASSO variables selected for Other.lead: \n",
      "['Smoke', 'Clths', 'Coal', 'Oil', 'Telcm', 'Servs', 'Paper', 'Trans', 'Fin', 'Books.3m', 'Hshld.3m', 'Steel.3m', 'Coal.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "Running OLS for Other.lead against ['Smoke', 'Clths', 'Coal', 'Oil', 'Telcm', 'Servs', 'Paper', 'Trans', 'Fin', 'Books.3m', 'Hshld.3m', 'Steel.3m', 'Coal.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample OLS R-squared: 8.83\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def subset_selection(X, Y, model_aic, verbose=False):\n",
    "    \n",
    "    global responses\n",
    "    global response_reverse_dict\n",
    "    global predictors\n",
    "    global predictor_reverse_dict\n",
    "    \n",
    "    coef_dict = {}\n",
    "    for response_index, response in enumerate(responses):\n",
    "        y = Y[:,response_reverse_dict[response]]\n",
    "        \n",
    "        model_aic.fit(X, y)\n",
    "\n",
    "        coef_dict[response] = [predstr for i, predstr in enumerate(predictors) if model_aic.coef_[i] !=0]\n",
    "        #y_response = model_aic.responseict(X)\n",
    "        # print (\"In-sample LASSO R-squared: %.6f\" % r2_score(y, y_response))\n",
    "        if verbose:\n",
    "            print(\"LASSO variables selected for %s: \" % response)\n",
    "            print(coef_dict[response])\n",
    "        \n",
    "        if not coef_dict[response]:\n",
    "            if verbose:\n",
    "                print(\"No coefs selected for \" + response + \", using all\")\n",
    "                print(\"---\")\n",
    "            coef_dict[response] = predictors            \n",
    "        # fit OLS vs. selected vars, better fit w/o LASSO penalties\n",
    "        # in-sample R-squared using LASSO coeffs\n",
    "        if verbose:\n",
    "            print(\"Running OLS for \" + response + \" against \" + str(coef_dict[response]))\n",
    "            # col nums of selected responses\n",
    "            predcols = [predictor_reverse_dict[predstr] for predstr in coef_dict[response]]\n",
    "            model_ols = LinearRegression()\n",
    "            model_ols.fit(X[:, predcols], y)\n",
    "            y_pred = model_ols.predict(X[:, predcols])\n",
    "            print (\"In-sample OLS R-squared: %.2f\" % (100 * r2_score(y, y_pred)))\n",
    "            print(\"---\")\n",
    "            \n",
    "    return coef_dict\n",
    "\n",
    "coef_dict = subset_selection(X, Y, LassoLarsIC(criterion='aic'), verbose=True)\n",
    "\n",
    "# These subsets line up closely with Table 2\n",
    "# except Clths, Whlsl, we get different responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same predictors selected for all but 2 response vars\n",
    "# use predictors from paper to match results\n",
    "if False: # turn off/on\n",
    "    coef_dict = {}\n",
    "    coef_dict['Food.lead'] = ['Clths', 'Coal', 'Util', 'Rtail']\n",
    "    coef_dict['Beer.lead'] = ['Food', 'Clths', 'Coal']\n",
    "    coef_dict['Smoke.lead'] = ['Txtls', 'Carry', 'Mines', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'Paper', 'Trans', 'Fin']\n",
    "    coef_dict['Games.lead'] = ['Books', 'Clths', 'Coal', 'Fin']\n",
    "    coef_dict['Books.lead'] = ['Games', 'Books', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Rtail', 'Fin']\n",
    "    coef_dict['Hshld.lead'] = ['Clths', 'Coal', 'Rtail']\n",
    "    coef_dict['Clths.lead'] = ['Books', 'Clths', 'Chems', 'Steel', 'ElcEq', 'Carry',  'Coal', 'Oil', 'Util','Telcm', 'Servs', 'BusEq', 'Rtail']\n",
    "    # Running OLS for Clths against ['Clths', 'Coal', 'Oil', 'Servs', 'Rtail']\n",
    "    coef_dict['Hlth.lead'] = ['Books', 'Mines', 'Coal', 'Util']\n",
    "    coef_dict['Chems.lead'] = ['Clths']\n",
    "    coef_dict['Txtls.lead'] = ['Clths', 'Autos', 'Coal', 'Oil', 'Rtail', 'Fin']\n",
    "    coef_dict['Cnstr.lead'] = ['Clths', 'Coal', 'Oil', 'Util', 'Trans', 'Rtail', 'Fin']\n",
    "    coef_dict['Steel.lead'] = ['Fin']\n",
    "    coef_dict['FabPr.lead'] = ['Trans', 'Fin']\n",
    "    coef_dict['ElcEq.lead'] = ['Fin']\n",
    "    coef_dict['Autos.lead'] = ['Hshld', 'Clths', 'Coal', 'Oil', 'Util', 'BusEq', 'Rtail', 'Fin']\n",
    "    coef_dict['Carry.lead'] = ['Trans']\n",
    "    coef_dict['Mines.lead'] = []\n",
    "    coef_dict['Coal.lead'] = ['Beer', 'Smoke', 'Books', 'Autos', 'Coal', 'Oil', 'Paper', 'Rtail']\n",
    "    coef_dict['Oil.lead'] = ['Beer', 'Hlth', 'Carry']\n",
    "    coef_dict['Util.lead'] = ['Food', 'Beer', 'Smoke', 'Hshld', 'Hlth', 'Cnstr', 'FabPr', 'Carry', 'Mines', 'Oil', 'Util', 'Telcm', 'BusEq', 'Whlsl', 'Fin', 'Other']\n",
    "    coef_dict['Telcm.lead'] = ['Beer', 'Smoke', 'Books', 'Hshld', 'Cnstr', 'Autos', 'Carry', 'Mines', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin']\n",
    "    coef_dict['Servs.lead'] = ['Smoke', 'Books', 'Steel', 'Oil', 'Util', 'Fin']\n",
    "    coef_dict['BusEq.lead'] = ['Smoke', 'Books', 'Util']\n",
    "    coef_dict['Paper.lead'] = ['Clths', 'Coal', 'Oil', 'Rtail', 'Fin']\n",
    "    coef_dict['Trans.lead'] = ['Fin']\n",
    "    coef_dict['Whlsl.lead'] = ['Food', 'Beer', 'Smoke', 'Books', 'Hlth', 'Carry', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'BusEq', 'Fin', 'Other']\n",
    "    # Running OLS for Whlsl against ['Food', 'Smoke', 'Books', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'Fin', 'Other']\n",
    "    coef_dict['Rtail.lead'] = ['Rtail']\n",
    "    coef_dict['Meals.lead'] = ['Smoke', 'Books', 'Clths', 'Steel', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Meals', 'Fin']\n",
    "    coef_dict['Fin.lead'] = ['Fin']\n",
    "    coef_dict['Other.lead'] = ['Clths', 'Fin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R-squared: 0.0438 for Food.lead against ['Clths', 'Coal', 'Oil', 'Util', 'Rtail', 'Meals', 'Beer.3m', 'Mines.3m', 'Util.3m']\n",
      "In-sample R-squared: 0.0416 for Beer.lead against ['Food', 'Clths', 'Coal', 'Beer.3m', 'Hlth.3m', 'Util.3m']\n",
      "In-sample R-squared: 0.1123 for Smoke.lead against ['Txtls', 'Carry', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'Trans', 'Food.3m', 'Beer.3m', 'Hshld.3m', 'Clths.3m', 'Chems.3m', 'ElcEq.3m', 'Mines.3m', 'Oil.3m', 'Util.3m', 'Servs.3m', 'Paper.3m', 'Other.3m']\n",
      "In-sample R-squared: 0.0553 for Games.lead against ['Books', 'Clths', 'Coal', 'Fin', 'Steel.3m']\n",
      "In-sample R-squared: 0.1055 for Books.lead against ['Games', 'Books', 'Clths', 'Autos', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Books.3m', 'Chems.3m', 'Steel.3m', 'ElcEq.3m', 'Autos.3m', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0383 for Hshld.lead against ['Clths', 'Coal', 'Rtail', 'Steel.3m', 'Util.3m']\n",
      "In-sample R-squared: 0.0691 for Clths.lead against ['Clths', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Steel.3m', 'Util.3m']\n",
      "In-sample R-squared: 0.0365 for Hlth.lead against ['Books', 'Mines', 'Coal', 'Util', 'Mines.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0576 for Chems.lead against ['Clths', 'Autos', 'Coal', 'Oil', 'Rtail', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.1300 for Txtls.lead against ['Clths', 'Autos', 'Coal', 'Oil', 'BusEq', 'Rtail', 'Fin', 'Chems.3m', 'Txtls.3m', 'ElcEq.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0833 for Cnstr.lead against ['Clths', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Trans', 'Fin', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0129 for Steel.lead against ['Fin']\n",
      "In-sample R-squared: 0.0904 for FabPr.lead against ['Smoke', 'Games', 'Books', 'Hshld', 'Coal', 'Oil', 'Util', 'Telcm', 'BusEq', 'Trans', 'Fin', 'Games.3m', 'Hlth.3m', 'Chems.3m', 'Steel.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Paper.3m', 'Rtail.3m']\n",
      "In-sample R-squared: 0.0433 for ElcEq.lead against ['Coal', 'Oil', 'Util', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Smoke.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0836 for Autos.lead against ['Smoke', 'Hshld', 'Clths', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m']\n",
      "In-sample R-squared: 0.0996 for Carry.lead against ['Beer', 'Hlth', 'Txtls', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'BusEq', 'Trans', 'Fin', 'Smoke.3m', 'Books.3m', 'Clths.3m', 'Steel.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'BusEq.3m', 'Paper.3m', 'Rtail.3m']\n",
      "In-sample R-squared: 0.0845 for Mines.lead against ['Food', 'Beer', 'Smoke', 'Games', 'Books', 'Hshld', 'Clths', 'Hlth', 'Chems', 'Txtls', 'Cnstr', 'Steel', 'FabPr', 'ElcEq', 'Autos', 'Carry', 'Mines', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'BusEq', 'Paper', 'Trans', 'Whlsl', 'Rtail', 'Meals', 'Fin', 'Other', 'Food.3m', 'Beer.3m', 'Smoke.3m', 'Games.3m', 'Books.3m', 'Hshld.3m', 'Clths.3m', 'Hlth.3m', 'Chems.3m', 'Txtls.3m', 'Cnstr.3m', 'Steel.3m', 'FabPr.3m', 'ElcEq.3m', 'Autos.3m', 'Carry.3m', 'Mines.3m', 'Coal.3m', 'Oil.3m', 'Util.3m', 'Telcm.3m', 'Servs.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Whlsl.3m', 'Rtail.3m', 'Meals.3m', 'Fin.3m', 'Other.3m']\n",
      "In-sample R-squared: 0.0691 for Coal.lead against ['Beer', 'Smoke', 'Books', 'Oil', 'Paper', 'Food.3m', 'Smoke.3m', 'Clths.3m', 'Steel.3m', 'Mines.3m', 'Coal.3m', 'Util.3m', 'Telcm.3m', 'BusEq.3m', 'Paper.3m', 'Trans.3m', 'Rtail.3m', 'Other.3m']\n",
      "In-sample R-squared: 0.0335 for Oil.lead against ['Beer', 'Smoke', 'Hlth', 'Carry', 'Hshld.3m', 'Autos.3m', 'Coal.3m']\n",
      "In-sample R-squared: 0.0937 for Util.lead against ['Beer', 'Hlth', 'Cnstr', 'FabPr', 'Carry', 'Mines', 'Oil', 'Util', 'Telcm', 'Whlsl', 'Fin', 'Other', 'Beer.3m', 'Cnstr.3m', 'ElcEq.3m', 'Mines.3m', 'Telcm.3m', 'Paper.3m', 'Fin.3m', 'Other.3m']\n",
      "In-sample R-squared: 0.0669 for Telcm.lead against ['Smoke', 'Books', 'Hshld', 'Mines', 'Coal', 'Oil', 'Util', 'Rtail', 'Meals', 'Fin', 'Beer.3m', 'Smoke.3m', 'Books.3m', 'Txtls.3m', 'Carry.3m', 'Telcm.3m']\n",
      "In-sample R-squared: 0.0509 for Servs.lead against ['Smoke', 'Books', 'Oil', 'Fin', 'Steel.3m', 'Mines.3m', 'Telcm.3m', 'Servs.3m']\n",
      "In-sample R-squared: 0.0404 for BusEq.lead against ['Smoke', 'Books', 'Hlth', 'Util', 'Txtls.3m', 'Telcm.3m']\n",
      "In-sample R-squared: 0.0580 for Paper.lead against ['Clths', 'Coal', 'Oil', 'Rtail', 'Fin', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0666 for Trans.lead against ['Books', 'Clths', 'Coal', 'Oil', 'Servs', 'Fin', 'Other', 'Beer.3m', 'Steel.3m', 'Coal.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0708 for Whlsl.lead against ['Smoke', 'Books', 'Carry', 'Coal', 'Oil', 'Util', 'Servs', 'Fin', 'Other', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0628 for Rtail.lead against ['Steel', 'Mines', 'Coal', 'Oil', 'Servs', 'BusEq', 'Rtail', 'Meals', 'Fin', 'Steel.3m', 'Util.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0990 for Meals.lead against ['Smoke', 'Books', 'Clths', 'Steel', 'Carry', 'Coal', 'Oil', 'Servs', 'BusEq', 'Meals', 'Fin', 'Beer.3m', 'Games.3m', 'Steel.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "In-sample R-squared: 0.0168 for Fin.lead against ['Fin']\n",
      "In-sample R-squared: 0.0883 for Other.lead against ['Smoke', 'Clths', 'Coal', 'Oil', 'Telcm', 'Servs', 'Paper', 'Trans', 'Fin', 'Books.3m', 'Hshld.3m', 'Steel.3m', 'Coal.3m', 'Util.3m', 'Servs.3m', 'Paper.3m']\n",
      "Mean R-squared: 0.0668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.066816985873582"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_with_subsets(X, Y, model, coef_dict, verbose=False):\n",
    "\n",
    "    global responses\n",
    "    global response_reverse_dict\n",
    "    \n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        y = Y[:,response_reverse_dict[response]]\n",
    "\n",
    "#        print(\"LASSO variables selected for %s: \" % pred)\n",
    "#        print(coef_dict[pred])\n",
    "        \n",
    "        if not coef_dict[response]:\n",
    "            if verbose:\n",
    "                print(\"No coefs selected for \" + response)\n",
    " #           print(\"---\")\n",
    "            continue\n",
    "        # fit model vs. selected vars, better fit w/o LASSO penalties\n",
    "        # in-sample R-squared using LASSO coeffs\n",
    "        #print(\"Running model for \" + pred + \" against \" + str(coef_dict[pred]))\n",
    "        # col nums of selected predictors\n",
    "        predcols = [predictor_reverse_dict[predstr] for predstr in coef_dict[response]]\n",
    "        model.fit(X[:, predcols], y)\n",
    "        y_pred = model.predict(X[:, predcols])\n",
    "        score = r2_score(y, y_pred)\n",
    "        scores.append(score)\n",
    "        if verbose:\n",
    "            print (\"In-sample R-squared: %.4f for %s against %s\" % (score, response, str(coef_dict[response])))\n",
    "#        print(\"---\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Mean R-squared: %.4f\" % np.mean(np.array(scores)))\n",
    "    return np.mean(np.array(scores))\n",
    "    \n",
    "\n",
    "predict_with_subsets(X, Y, LinearRegression(), coef_dict, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.8536473674516295,\n",
       " -0.7708285578179694,\n",
       " -2.4280971602836754,\n",
       " -0.07882299101500101,\n",
       " -2.023396225788131,\n",
       " 0.06315081701329256,\n",
       " -0.8533691403459845,\n",
       " -1.079652578340466,\n",
       " -0.25344454364601343,\n",
       " -0.5460795367539997,\n",
       " -1.3130952344726021,\n",
       " 0.35380818707447625,\n",
       " -2.6600182569991935,\n",
       " -1.2373953026559863,\n",
       " -1.3830369632222037,\n",
       " -1.807316312559728,\n",
       " -4.170719450208045,\n",
       " 0.6587990973550054,\n",
       " 0.8733823800768382,\n",
       " -1.4330584604736534,\n",
       " -0.9183593929203996,\n",
       " -0.6698044320601328,\n",
       " -0.7549309496940442,\n",
       " -0.2139496959653665,\n",
       " -0.8270259584809956,\n",
       " -1.4676526087862514,\n",
       " -1.8447633904066465,\n",
       " -2.1956037974064464,\n",
       " 0.6624203768057081,\n",
       " -1.5039208535103081]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_predict(X, Y, model, coef_dict=None):\n",
    "    \"\"\"for backtest, fit Ys v. X using n-1 rows\n",
    "    predict Ys on X using nth row\n",
    "    return a prediction for month n+1 using X for final month\"\"\"\n",
    "    \n",
    "    global responses\n",
    "    global response_reverse_dict\n",
    "    \n",
    "    # keep last row to predict against\n",
    "    X_predict = X[-1]\n",
    "    X_predict = X_predict.reshape(1,X.shape[1])\n",
    "    # fit on remaining rows\n",
    "    X_fit = X[:-1]\n",
    "    Y_fit = Y[:-1]\n",
    "\n",
    "    # if no coef_dict select predictors into coef_dict\n",
    "    if coef_dict is None:\n",
    "        coef_dict = subset_selection(X_fit, Y_fit, LassoLarsIC(criterion='aic'))\n",
    "\n",
    "    predictions = []\n",
    "    for response in responses:\n",
    "        if not coef_dict[response]:\n",
    "            predictions.append(0.0)\n",
    "            continue\n",
    "        # column predexes to fit against each other\n",
    "        predcols = [predictor_reverse_dict[predstr] for predstr in coef_dict[response]]\n",
    "        responsecol = response_reverse_dict[response]\n",
    "        model.fit(X_fit[:, predcols], Y_fit[:,responsecol])\n",
    "        y_pred = model.predict(X_predict[:,predcols])        \n",
    "        predictions.append(y_pred[0])\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "#    return np.argsort(predictions)\n",
    "\n",
    "X = data.values[:,:npredictors]\n",
    "Y = data.values[:, -nresponses:]\n",
    "model = LinearRegression()\n",
    "predictions = fit_predict(X, Y, model, coef_dict)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.34        -1.95        -7.59        -7.76       -12.05\n",
      "  -7.5         -5.69        -7.71        -7.37        -5.26\n",
      "  -9.84        -6.31        -7.15        -6.89        -9.35\n",
      " -12.49        -2.34        -0.77       -12.16        -4.83\n",
      "  -3.16       -11.17        -9.73        -8.89        -8.17\n",
      "  -8.28        -6.31       -13.12        -9.78        -6.2\n",
      "  -2.28666667  -2.18        -3.36        -7.00333333  -6.82\n",
      "  -3.37666667  -5.32666667  -1.41        -5.58666667  -5.43333333\n",
      "  -6.02666667  -4.45        -4.68333333  -4.67666667  -5.93666667\n",
      "  -9.37        -2.48         2.38666667  -6.98666667  -3.96666667\n",
      "  -2.88333333  -4.91666667  -4.43        -4.87666667  -8.43\n",
      "  -6.69666667  -4.78333333  -7.30333333  -6.68333333  -6.49      ]\n",
      "Food     -3.34\n",
      "Beer     -1.95\n",
      "Smoke    -7.59\n",
      "Games    -7.76\n",
      "Books   -12.05\n",
      "Hshld    -7.50\n",
      "Clths    -5.69\n",
      "Hlth     -7.71\n",
      "Chems    -7.37\n",
      "Txtls    -5.26\n",
      "Cnstr    -9.84\n",
      "Steel    -6.31\n",
      "FabPr    -7.15\n",
      "ElcEq    -6.89\n",
      "Autos    -9.35\n",
      "Carry   -12.49\n",
      "Mines    -2.34\n",
      "Coal     -0.77\n",
      "Oil     -12.16\n",
      "Util     -4.83\n",
      "Telcm    -3.16\n",
      "Servs   -11.17\n",
      "BusEq    -9.73\n",
      "Paper    -8.89\n",
      "Trans    -8.17\n",
      "Whlsl    -8.28\n",
      "Rtail    -6.31\n",
      "Meals   -13.12\n",
      "Fin      -9.78\n",
      "Other    -6.20\n",
      "Name: 197001, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 197001 = 121\n",
    "STARTMONTH = 121\n",
    "print(X[STARTMONTH])\n",
    "print(data.iloc[STARTMONTH][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all months starting STARTMONTH\n",
    "# initialize predictions matrix P\n",
    "\n",
    "def run_backtest(X, Y, model, coef_dict=None, startmonth=0, minmaxscale=False):\n",
    "    global P\n",
    "    global R \n",
    "\n",
    "    P = np.zeros_like(Y)\n",
    "    count = 0\n",
    "    for month_index in range(startmonth, X.shape[0]+1):\n",
    "        # 0 to month_index-1\n",
    "        Xscale = X.copy()\n",
    "        Yscale = Y.copy()\n",
    "\n",
    "        if minmaxscale:\n",
    "            # minmaxscale each row\n",
    "            for i in range(Xscale.shape[0]):\n",
    "                Xscale[i] = Xscale[i] - np.min(Xscale[i])\n",
    "                Xscale[i] = Xscale[i]/np.max(Xscale[i])\n",
    "                \n",
    "            for i in range(Yscale.shape[0]):\n",
    "                Yscale[i] = Yscale[i] - np.min(Yscale[i])\n",
    "                Yscale[i] = Yscale[i]/np.max(Yscale[i])\n",
    "        \n",
    "        predictions = fit_predict(Xscale[:month_index, :], \n",
    "                                  Yscale[:month_index], \n",
    "                                  model,\n",
    "                                  coef_dict)\n",
    "        try:\n",
    "            P[month_index]= predictions\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "            sys.stdout.flush()\n",
    "        except IndexError:\n",
    "            # I want to run the fit and see the R-squared on full dataset\n",
    "            # but we are storing the predictions in row of the month predicted\n",
    "            # so we have no row to store the last prediction (2017-01)\n",
    "            print(\"\\nlast prediction not stored\")\n",
    "\n",
    "    mse = np.mean((P[startmonth:]-Xscale[startmonth:,-nresponses:])**2)\n",
    "    print(\"MSE across all predictions: %.4f\" % mse)\n",
    "    print(\"Variance: %.4f\" % (np.mean(Yscale[startmonth:,-nresponses]**2)))\n",
    "    print(\"R-squared: %.4f\" % (1- mse/np.mean(Yscale[startmonth:,-nresponses]**2)))\n",
    "\n",
    "    R = np.zeros(P.shape[0])\n",
    "    numstocks = 6 # top quintile (and bottom)\n",
    "\n",
    "    for month_index in range(startmonth, X.shape[0]):\n",
    "        # get indexes of sorted smallest to largest\n",
    "        select_array = np.argsort(P[month_index])\n",
    "        # leftmost 6\n",
    "        short_indexes = select_array[:numstocks]\n",
    "        # rightmost 6\n",
    "        long_indexes = select_array[-numstocks:]\n",
    "        # compute equal weighted long/short return\n",
    "        R[month_index] = np.mean(X[month_index, long_indexes])/2 - np.mean(X[month_index, short_indexes])/2\n",
    "\n",
    "    results = R[startmonth:]\n",
    "\n",
    "    index = pd.date_range('01/01/1970',periods=results.shape[0], freq='M')\n",
    "    perfdata = pd.DataFrame(results,index=index,columns=['Returns'])\n",
    "    perfdata['Equity'] = 100 * np.cumprod(1 + results / 100)\n",
    "\n",
    "    stats = perfdata['Equity'].calc_stats()\n",
    "\n",
    "    retframe = pd.DataFrame([stats.stats.loc['start'],\n",
    "                             stats.stats.loc['end'],\n",
    "                             stats.stats.loc['cagr'],\n",
    "                             stats.stats.loc['yearly_vol'],\n",
    "                             stats.stats.loc['yearly_sharpe'],\n",
    "                             stats.stats.loc['max_drawdown'],\n",
    "                             ffn.core.calc_sortino_ratio(perfdata.Returns, rf=0, nperiods=564, annualize=False),\n",
    "                            ],\n",
    "                            index = ['start',\n",
    "                                     'end',\n",
    "                                     'cagr',\n",
    "                                     'yearly_vol',\n",
    "                                     'yearly_sharpe',\n",
    "                                     'max_drawdown',\n",
    "                                     'sortino',\n",
    "                                    ],\n",
    "                            columns=['Value'])   \n",
    "    return retframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:38:56 Starting\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................\n",
      "last prediction not stored\n",
      "MSE across all predictions: 0.0530\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>0.0505986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0586134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>0.871087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.101667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>0.48858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-12-31 00:00:00\n",
       "cagr                     0.0505986\n",
       "yearly_vol               0.0586134\n",
       "yearly_sharpe             0.871087\n",
       "max_drawdown             -0.101667\n",
       "sortino                    0.48858"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "model = LinearRegression()\n",
    "run_backtest(X, Y, model, coef_dict, startmonth=STARTMONTH, minmaxscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:32:45 Starting\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................\n",
      "last prediction not stored\n",
      "MSE across all predictions: 14.2270\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>0.072308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0901095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>0.840289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.101805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>0.719113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-12-31 00:00:00\n",
       "cagr                      0.072308\n",
       "yearly_vol               0.0901095\n",
       "yearly_sharpe             0.840289\n",
       "max_drawdown             -0.101805\n",
       "sortino                   0.719113"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without minmaxscale\n",
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "model = LinearRegression()\n",
    "run_backtest(X, Y, model, coef_dict, startmonth=STARTMONTH, minmaxscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "0.422214988425926\n",
      "5.433597396370643\n",
      "10.560532923520368\n",
      "0.05033246212836251\n"
     ]
    }
   ],
   "source": [
    "# double check results\n",
    "#model = LinearRegression()\n",
    "#R = run_backtest(X, Y, model, coef_dict_paper, startmonth=STARTMONTH, summary=False)\n",
    "results = R[STARTMONTH:]\n",
    "print(len(results))\n",
    "#print(results)\n",
    "print(np.mean(results))\n",
    "print(np.std(results) * np.sqrt(12))\n",
    "print(np.prod(1 + results / 100))\n",
    "print(np.prod(1 + results / 100) ** (12.0/results.shape[0]))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Growth of $1",
         "type": "scatter",
         "x": [
          1970,
          1970.08,
          1970.16,
          1970.24,
          1970.32,
          1970.4,
          1970.48,
          1970.56,
          1970.64,
          1970.72,
          1970.8,
          1970.88,
          1970.96,
          1971.04,
          1971.12,
          1971.2,
          1971.28,
          1971.36,
          1971.44,
          1971.52,
          1971.6,
          1971.68,
          1971.76,
          1971.84,
          1971.92,
          1972,
          1972.08,
          1972.16,
          1972.24,
          1972.32,
          1972.4,
          1972.48,
          1972.56,
          1972.64,
          1972.72,
          1972.8,
          1972.88,
          1972.96,
          1973.04,
          1973.12,
          1973.2,
          1973.28,
          1973.36,
          1973.44,
          1973.52,
          1973.6,
          1973.68,
          1973.76,
          1973.84,
          1973.92,
          1974,
          1974.08,
          1974.16,
          1974.24,
          1974.32,
          1974.4,
          1974.48,
          1974.56,
          1974.64,
          1974.72,
          1974.8,
          1974.88,
          1974.96,
          1975.04,
          1975.12,
          1975.2,
          1975.28,
          1975.36,
          1975.44,
          1975.52,
          1975.6,
          1975.68,
          1975.76,
          1975.84,
          1975.92,
          1976,
          1976.08,
          1976.16,
          1976.24,
          1976.32,
          1976.4,
          1976.48,
          1976.56,
          1976.64,
          1976.72,
          1976.8,
          1976.88,
          1976.96,
          1977.04,
          1977.12,
          1977.2,
          1977.28,
          1977.36,
          1977.44,
          1977.52,
          1977.6,
          1977.68,
          1977.76,
          1977.84,
          1977.92,
          1978,
          1978.08,
          1978.16,
          1978.24,
          1978.32,
          1978.4,
          1978.48,
          1978.56,
          1978.64,
          1978.72,
          1978.8,
          1978.88,
          1978.96,
          1979.04,
          1979.12,
          1979.2,
          1979.28,
          1979.36,
          1979.44,
          1979.52,
          1979.6,
          1979.68,
          1979.76,
          1979.84,
          1979.92,
          1980,
          1980.08,
          1980.16,
          1980.24,
          1980.32,
          1980.4,
          1980.48,
          1980.56,
          1980.64,
          1980.72,
          1980.8,
          1980.88,
          1980.96,
          1981.04,
          1981.12,
          1981.2,
          1981.28,
          1981.36,
          1981.44,
          1981.52,
          1981.6,
          1981.68,
          1981.76,
          1981.84,
          1981.92,
          1982,
          1982.08,
          1982.16,
          1982.24,
          1982.32,
          1982.4,
          1982.48,
          1982.56,
          1982.64,
          1982.72,
          1982.8,
          1982.88,
          1982.96,
          1983.04,
          1983.12,
          1983.2,
          1983.28,
          1983.36,
          1983.44,
          1983.52,
          1983.6,
          1983.68,
          1983.76,
          1983.84,
          1983.92,
          1984,
          1984.08,
          1984.16,
          1984.24,
          1984.32,
          1984.4,
          1984.48,
          1984.56,
          1984.64,
          1984.72,
          1984.8,
          1984.88,
          1984.96,
          1985.04,
          1985.12,
          1985.2,
          1985.28,
          1985.36,
          1985.44,
          1985.52,
          1985.6,
          1985.68,
          1985.76,
          1985.84,
          1985.92,
          1986,
          1986.08,
          1986.16,
          1986.24,
          1986.32,
          1986.4,
          1986.48,
          1986.56,
          1986.64,
          1986.72,
          1986.8,
          1986.88,
          1986.96,
          1987.04,
          1987.12,
          1987.2,
          1987.28,
          1987.36,
          1987.44,
          1987.52,
          1987.6,
          1987.68,
          1987.76,
          1987.84,
          1987.92,
          1988,
          1988.08,
          1988.16,
          1988.24,
          1988.32,
          1988.4,
          1988.48,
          1988.56,
          1988.64,
          1988.72,
          1988.8,
          1988.88,
          1988.96,
          1989.04,
          1989.12,
          1989.2,
          1989.28,
          1989.36,
          1989.44,
          1989.52,
          1989.6,
          1989.68,
          1989.76,
          1989.84,
          1989.92,
          1990,
          1990.08,
          1990.16,
          1990.24,
          1990.32,
          1990.4,
          1990.48,
          1990.56,
          1990.64,
          1990.72,
          1990.8,
          1990.88,
          1990.96,
          1991.04,
          1991.12,
          1991.2,
          1991.28,
          1991.36,
          1991.44,
          1991.52,
          1991.6,
          1991.68,
          1991.76,
          1991.84,
          1991.92,
          1992,
          1992.08,
          1992.16,
          1992.24,
          1992.32,
          1992.4,
          1992.48,
          1992.56,
          1992.64,
          1992.72,
          1992.8,
          1992.88,
          1992.96,
          1993.04,
          1993.12,
          1993.2,
          1993.28,
          1993.36,
          1993.44,
          1993.52,
          1993.6,
          1993.68,
          1993.76,
          1993.84,
          1993.92,
          1994,
          1994.08,
          1994.16,
          1994.24,
          1994.32,
          1994.4,
          1994.48,
          1994.56,
          1994.64,
          1994.72,
          1994.8,
          1994.88,
          1994.96,
          1995.04,
          1995.12,
          1995.2,
          1995.28,
          1995.36,
          1995.44,
          1995.52,
          1995.6,
          1995.68,
          1995.76,
          1995.84,
          1995.92,
          1996,
          1996.08,
          1996.16,
          1996.24,
          1996.32,
          1996.4,
          1996.48,
          1996.56,
          1996.64,
          1996.72,
          1996.8,
          1996.88,
          1996.96,
          1997.04,
          1997.12,
          1997.2,
          1997.28,
          1997.36,
          1997.44,
          1997.52,
          1997.6,
          1997.68,
          1997.76,
          1997.84,
          1997.92,
          1998,
          1998.08,
          1998.16,
          1998.24,
          1998.32,
          1998.4,
          1998.48,
          1998.56,
          1998.64,
          1998.72,
          1998.8,
          1998.88,
          1998.96,
          1999.04,
          1999.12,
          1999.2,
          1999.28,
          1999.36,
          1999.44,
          1999.52,
          1999.6,
          1999.68,
          1999.76,
          1999.84,
          1999.92,
          2000,
          2000.08,
          2000.16,
          2000.24,
          2000.32,
          2000.4,
          2000.48,
          2000.56,
          2000.64,
          2000.72,
          2000.8,
          2000.88,
          2000.96,
          2001.04,
          2001.12,
          2001.2,
          2001.28,
          2001.36,
          2001.44,
          2001.52,
          2001.6,
          2001.68,
          2001.76,
          2001.84,
          2001.92,
          2002,
          2002.08,
          2002.16,
          2002.24,
          2002.32,
          2002.4,
          2002.48,
          2002.56,
          2002.64,
          2002.72,
          2002.8,
          2002.88,
          2002.96,
          2003.04,
          2003.12,
          2003.2,
          2003.28,
          2003.36,
          2003.44,
          2003.52,
          2003.6,
          2003.68,
          2003.76,
          2003.84,
          2003.92,
          2004,
          2004.08,
          2004.16,
          2004.24,
          2004.32,
          2004.4,
          2004.48,
          2004.56,
          2004.64,
          2004.72,
          2004.8,
          2004.88,
          2004.96,
          2005.04,
          2005.12,
          2005.2,
          2005.28,
          2005.36,
          2005.44,
          2005.52,
          2005.6,
          2005.68,
          2005.76,
          2005.84,
          2005.92,
          2006,
          2006.08,
          2006.16,
          2006.24,
          2006.32,
          2006.4,
          2006.48,
          2006.56,
          2006.64,
          2006.72,
          2006.8,
          2006.88,
          2006.96,
          2007.04,
          2007.12,
          2007.2,
          2007.28,
          2007.36,
          2007.44,
          2007.52,
          2007.6,
          2007.68,
          2007.76,
          2007.84,
          2007.92,
          2008,
          2008.08,
          2008.16,
          2008.24,
          2008.32,
          2008.4,
          2008.48,
          2008.56,
          2008.64,
          2008.72,
          2008.8,
          2008.88,
          2008.96,
          2009.04,
          2009.12,
          2009.2,
          2009.28,
          2009.36,
          2009.44,
          2009.52,
          2009.6,
          2009.68,
          2009.76,
          2009.84,
          2009.92,
          2010,
          2010.08,
          2010.16,
          2010.24,
          2010.32,
          2010.4,
          2010.48,
          2010.56,
          2010.64,
          2010.72,
          2010.8,
          2010.88,
          2010.96,
          2011.04,
          2011.12,
          2011.2,
          2011.28,
          2011.36,
          2011.44,
          2011.52,
          2011.6,
          2011.68,
          2011.76,
          2011.84,
          2011.92,
          2012,
          2012.08,
          2012.16,
          2012.24,
          2012.32,
          2012.4,
          2012.48,
          2012.56,
          2012.64,
          2012.72,
          2012.8,
          2012.88,
          2012.96,
          2013.04,
          2013.12,
          2013.2,
          2013.28,
          2013.36,
          2013.44,
          2013.52,
          2013.6,
          2013.68,
          2013.76,
          2013.84,
          2013.92,
          2014,
          2014.08,
          2014.16,
          2014.24,
          2014.32,
          2014.4,
          2014.48,
          2014.56,
          2014.64,
          2014.72,
          2014.8,
          2014.88,
          2014.96,
          2015.04,
          2015.12,
          2015.2,
          2015.28,
          2015.36,
          2015.44,
          2015.52,
          2015.6,
          2015.68,
          2015.76,
          2015.84,
          2015.92,
          2016
         ],
         "y": [
          99.20583333333333,
          99.02560940277777,
          98.45868778894688,
          100.853695369413,
          103.51959471701116,
          102.72508182755809,
          105.21873318892206,
          106.66812123859945,
          109.82727542928264,
          108.08376743184279,
          108.51159901126049,
          108.9628264104823,
          110.80157410615918,
          111.54486799912134,
          113.28961564274094,
          110.67168144126327,
          111.15402551954476,
          111.4587728061775,
          111.6835479980033,
          110.45782105872523,
          111.86063538617104,
          111.51852827628166,
          112.82980030459693,
          112.29197825647834,
          112.7074585760273,
          113.62884204988634,
          114.89390982470842,
          115.21657022146614,
          117.10324155884267,
          117.93565043425676,
          118.12533027203852,
          119.36859937315172,
          120.43097990757276,
          120.37377519211667,
          120.76298373190453,
          120.80323805981517,
          121.79684469285715,
          126.54184676734972,
          126.92041779226204,
          128.01933707631338,
          130.93177699479952,
          133.94429896382152,
          128.69591484942245,
          130.20380198507485,
          130.6692805771715,
          127.4809501310885,
          133.95273303274345,
          137.64759591889663,
          136.95935793930215,
          135.78607277295546,
          133.86130519139883,
          136.1213302273803,
          138.04177532800492,
          139.45555317698927,
          139.69030335817052,
          139.9731762224708,
          141.74267045854987,
          136.38125394845522,
          135.35271199159396,
          135.48355294651915,
          130.59937086279714,
          128.98538030455106,
          128.0523860536815,
          127.33209138212953,
          129.542364268371,
          127.49883347203746,
          128.76850935536314,
          130.00790625790853,
          132.06094777756465,
          130.8734997554647,
          132.58685198976335,
          132.3813423691792,
          130.29082033759926,
          132.71422959587858,
          134.25813846684397,
          133.85872050490514,
          135.574343106043,
          136.90184188228966,
          136.1488817519371,
          137.4774679230331,
          138.87171857688585,
          139.31610807633191,
          140.05448344913646,
          139.5047696015986,
          141.2078903304848,
          141.9762966003665,
          142.11235721794188,
          142.828840352249,
          142.9930935186541,
          143.3195944155217,
          145.5780723575196,
          145.60597482138812,
          145.5101175546307,
          146.04971757389583,
          145.0225012269594,
          146.95734309749577,
          146.5201450017807,
          145.48717797951815,
          147.6149279574686,
          150.0862478763566,
          151.37824032682553,
          151.7276717649133,
          150.35706512997024,
          152.25031117506512,
          148.70034141949986,
          153.26172439254302,
          152.07139166642762,
          152.32104220108,
          152.0087840645678,
          151.2968762591987,
          150.6538645350971,
          150.2056692881052,
          148.89262139574504,
          150.87041171661852,
          151.8950732628606,
          151.75710190464682,
          151.9607093497022,
          152.05061943606745,
          153.65095220563205,
          155.37312329493685,
          159.63034687321814,
          162.07801219194081,
          159.6265822575377,
          160.37682719414815,
          159.14326209831316,
          158.18707633187245,
          158.1158921475231,
          159.2437855115088,
          162.46316404193314,
          163.8996091840039,
          168.58304051643682,
          168.12224687235855,
          169.38036168645337,
          169.05148148417882,
          167.4088645890909,
          169.61726652779535,
          169.92399108476647,
          168.71186661502847,
          170.7546861332921,
          170.4074849381544,
          169.74999605876803,
          173.29494180979532,
          171.31360297510335,
          170.70258445782548,
          174.83358700170484,
          176.13464027830918,
          183.19323598746246,
          186.06021013066623,
          188.41697279232133,
          190.89936640886015,
          196.12523656430267,
          191.29401823693536,
          194.0279285809049,
          197.2083697102269,
          201.86413063846916,
          195.28672438183236,
          194.90754265865763,
          188.82805155722968,
          186.7289130507518,
          190.1833979421907,
          189.99004482094946,
          188.28646741905496,
          188.8293600667799,
          193.49187168242878,
          189.9558077274324,
          193.17555866841238,
          190.65944701675633,
          192.93623857988143,
          196.97342937216547,
          198.9661438993139,
          200.430203108173,
          199.1107042710442,
          201.3689515086516,
          203.12925175975641,
          205.9273572027471,
          200.6968023297973,
          200.18669795720908,
          201.7081168616839,
          200.90968889910638,
          204.67674556596464,
          201.91019822173138,
          199.52765788271495,
          206.09876874898566,
          206.5916883042436,
          210.05898547294984,
          212.07205075039894,
          206.9399071222393,
          205.78621714003282,
          204.44346207319413,
          206.17271302322985,
          208.4010964298226,
          211.72683059368183,
          212.1326403523197,
          213.4107395104424,
          215.994787881348,
          214.19123140253876,
          217.95207244058167,
          222.13311969690017,
          221.10760512763278,
          219.19133921652664,
          217.75563594465837,
          211.64940498671024,
          211.9157304879852,
          213.32850202457175,
          213.89204481741996,
          218.93989707511108,
          218.35788184871973,
          219.717159663228,
          220.9548996626642,
          222.56050526687957,
          225.45564650622626,
          224.08412465664668,
          223.23820708606786,
          231.728700228908,
          228.1330432303561,
          230.8364197926358,
          234.9664680700924,
          234.29289752829146,
          235.96418686399326,
          239.11430875862754,
          238.18176295446892,
          239.27739906405944,
          237.19967364885318,
          239.39179396615796,
          241.78770683743593,
          244.74557645108055,
          244.2275316475924,
          245.24921682165152,
          243.63465947757567,
          244.46707789745736,
          244.5139340873877,
          242.88587880958917,
          244.87147086885756,
          246.612098907617,
          249.64953792582918,
          244.97276991535196,
          244.10719946165108,
          245.1405866060387,
          247.79831913249254,
          246.68735666838185,
          247.77278103772272,
          249.10868928215112,
          246.68195546739415,
          243.429865021149,
          244.31026969964216,
          246.63732501853124,
          247.2436417758684,
          250.92757203832883,
          258.3633924230646,
          268.4158814165921,
          273.74841026073506,
          277.9709794890069,
          285.13336506050695,
          289.68837056734856,
          297.39166715468525,
          295.3520559707827,
          297.00602748421915,
          303.52778483772676,
          303.44937349331036,
          303.8590301475263,
          301.5775552628353,
          300.4692577472444,
          301.9440610206871,
          301.47101532508805,
          312.2812634829535,
          312.95787288716656,
          314.04018553090134,
          313.4906152062223,
          318.09370240616704,
          318.3481773680919,
          318.96895631395967,
          322.35534340015954,
          323.7683343220636,
          329.37762071419337,
          336.09417936325696,
          333.0749333186437,
          333.11934330975276,
          336.5865604747018,
          336.45753562651987,
          339.26134842340747,
          335.5068561675218,
          332.9877588557973,
          337.87435421700616,
          338.0827100687733,
          337.9108513578216,
          345.32517862136456,
          350.07915524705203,
          346.3128870018525,
          347.31719437415785,
          342.7528675780908,
          343.73256952458485,
          349.82809375748747,
          353.0610883906296,
          353.9761050447087,
          356.9288557209566,
          358.50826590752183,
          364.7044837699568,
          358.10941102178344,
          360.5773817127419,
          359.6188468396889,
          362.4508452585515,
          364.1090578756093,
          363.4081479391988,
          362.92966054441223,
          365.40967989146566,
          364.62100399903323,
          358.4558705230829,
          355.8451169327731,
          358.98545008970484,
          368.22932542951474,
          366.96813998991865,
          356.1701024707153,
          351.7506251158911,
          352.7150080797505,
          356.9975561361855,
          360.9513040703937,
          363.1170118948161,
          357.845763272143,
          355.62413749182844,
          359.1211081771648,
          363.4066200680789,
          363.05229861351256,
          365.7298093157872,
          365.79990752923936,
          364.5714295064537,
          369.27136285184105,
          372.1978384024419,
          372.3529208351096,
          374.9562883399484,
          372.46595365822395,
          369.8121337384091,
          375.16824614205376,
          374.14278626926546,
          386.50820535546467,
          400.1422822993787,
          403.3934383430611,
          408.27113733502597,
          408.2167011833813,
          409.0909652850824,
          411.8796020317757,
          416.58875881500575,
          414.0614536781947,
          404.2067910806536,
          410.2934716763432,
          397.21194815439577,
          382.6541302545372,
          387.9283796832122,
          404.56404169862725,
          405.3158565427839,
          415.55008192048916,
          435.84277758760635,
          434.20110312535974,
          433.29651749384857,
          432.63935110898296,
          427.0583034796771,
          438.31128977636666,
          433.1136483984353,
          447.9586186972916,
          445.83454824696867,
          463.5304645244713,
          486.74947804327627,
          495.4947436654538,
          479.9073048543114,
          478.7955195980656,
          480.3715548500759,
          491.30801391549596,
          482.1860617904649,
          472.97228979308545,
          472.5426732965235,
          485.46277755557253,
          509.1088603456752,
          486.9498971991296,
          503.8835798742293,
          526.7934866391777,
          538.791208297385,
          535.5898905347514,
          538.4062007091466,
          538.5138819492884,
          555.1898618269846,
          559.8580832485133,
          583.773354371279,
          580.7620568183138,
          605.6622300043989,
          615.2367404237186,
          615.1342009669813,
          627.1703268325685,
          618.9230370347203,
          609.7526607026558,
          619.0310636896812,
          638.6852999618285,
          648.0952633812661,
          662.8934385618049,
          660.0374726640011,
          670.455064107548,
          672.6061074382262,
          684.6681769649517,
          674.6035547635669,
          683.1372897313261,
          694.1585713389915,
          689.8374342324063,
          684.8706047059329,
          687.3247243727959,
          687.9032226824763,
          690.4713947138242,
          693.1642331532082,
          679.9305726685916,
          671.3181187481226,
          669.1810894034411,
          671.2332447442784,
          667.6813021575066,
          665.3611096325093,
          678.2746598352935,
          675.0189414680841,
          679.7046979534416,
          687.3627042170505,
          697.1232546169325,
          693.4633575301937,
          690.1462911366742,
          666.4167611597583,
          662.2794237675581,
          670.8338663245557,
          684.5021063509187,
          681.6214933200251,
          688.8126000745515,
          685.5177798041948,
          677.6457506327768,
          673.303170780805,
          679.7051617629791,
          699.5412240670953,
          691.7063623575439,
          686.1957683374288,
          679.722654922779,
          694.3310276481611,
          690.6915758449054,
          681.7356084114497,
          700.9719148287928,
          691.3452338651441,
          696.8529508949364,
          712.5147209663,
          722.561178531925,
          735.2782552740869,
          735.9216237474517,
          734.1063504088746,
          721.736658404485,
          714.2847274064587,
          714.8323456974703,
          719.3298325391501,
          724.1313591713489,
          712.0926753251252,
          725.0705643329255,
          730.8348753193723,
          734.8727380055118,
          723.9721257250967,
          715.9119027253572,
          707.4104488804936,
          712.0675676689568,
          709.2074296054865,
          701.559809489574,
          704.5823630021249,
          702.6565045432524,
          701.8543050338989,
          728.9049397070803,
          718.9614614879096,
          727.7927047731861,
          759.6639603030452,
          759.3854168509341,
          785.4766341315707,
          786.8119444095944,
          781.2518066690999,
          814.5917275187038,
          799.9222881589712,
          897.379486933006,
          883.3205416377222,
          875.7166239751241,
          858.5087923140129,
          868.3172552662006,
          863.2665432314022,
          869.8489506235417,
          879.6564975418221,
          888.2184874512293,
          905.0132186847861,
          902.2152194836856,
          905.0421605047345,
          922.1323733022656,
          913.003262806573,
          920.7866156219993,
          924.8764428397203,
          916.6219205873758,
          912.9019632929921,
          921.0800433808253,
          924.0889048558694,
          939.5673940122053,
          956.5735638438263,
          956.589506736557,
          956.4619614689923,
          963.404281205988,
          986.3734449437409,
          986.061093352842,
          1000.8602269289125,
          1028.2837971467648,
          1056.0731667646562,
          1019.3306211709693,
          1009.0693595845149,
          1013.7026697272739,
          990.7845418691899,
          1004.9279912043726,
          1000.3639432443194,
          995.63722361249,
          1018.2547825422203,
          993.1378312395121,
          997.8386836407126,
          999.5682706923566,
          1008.5477256574096,
          1012.1448792122544,
          1000.9522437562989,
          996.0892841053828,
          1017.837233475017,
          1023.4014103513472,
          1025.4993832425675,
          1054.803028118724,
          1035.333122224699,
          1046.7217865691707,
          1048.4401548354551,
          1030.9049932458322,
          1017.0650937115068,
          1017.5397240885723,
          1014.7499693450294,
          1001.8372759851139,
          1014.2767554952625,
          1013.7442601986276,
          1010.8719847947315,
          1009.5410033480852,
          1017.146212239974,
          1014.0269638557716,
          1014.2804705967355,
          1013.5958312790829,
          1010.9942686454664,
          1011.1880425469567,
          1032.313446069167,
          1049.96600599695,
          1078.4638333430503,
          1083.5775493528188,
          1106.856407038082,
          1098.5549839852963,
          1070.825625264534,
          1071.1468729521134,
          1071.236135191526,
          1058.2295431167422,
          1072.938933766065,
          1062.9695428398218,
          1047.2553097648397,
          1074.1959526085402,
          1098.8935078855982,
          1077.629918508012,
          1072.969169110465,
          1083.8150991282232,
          1086.1091744213782,
          1092.2637930764326,
          1073.3130162665566,
          1102.2924677057536,
          1112.2314714562337,
          1115.5959716573889,
          1097.2072313912363,
          1102.1080903581171,
          1098.2690805100365,
          1076.7338542897023,
          1081.7945034048637,
          1080.8749780769697,
          1087.864636268534,
          1097.564762608595,
          1081.2933650029227,
          1067.083368031176,
          1071.565118176907,
          1076.0478322546137,
          1069.4391051515165,
          1056.0532923520368
         ]
        }
       ],
       "layout": {
        "yaxis": {
         "autorange": true,
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div id=\"ebe3136f-9b34-4932-8fcf-05303fde3f43\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ebe3136f-9b34-4932-8fcf-05303fde3f43\", [{\"y\": [99.20583333333333, 99.02560940277777, 98.45868778894688, 100.853695369413, 103.51959471701116, 102.72508182755809, 105.21873318892206, 106.66812123859945, 109.82727542928264, 108.08376743184279, 108.51159901126049, 108.9628264104823, 110.80157410615918, 111.54486799912134, 113.28961564274094, 110.67168144126327, 111.15402551954476, 111.4587728061775, 111.6835479980033, 110.45782105872523, 111.86063538617104, 111.51852827628166, 112.82980030459693, 112.29197825647834, 112.7074585760273, 113.62884204988634, 114.89390982470842, 115.21657022146614, 117.10324155884267, 117.93565043425676, 118.12533027203852, 119.36859937315172, 120.43097990757276, 120.37377519211667, 120.76298373190453, 120.80323805981517, 121.79684469285715, 126.54184676734972, 126.92041779226204, 128.01933707631338, 130.93177699479952, 133.94429896382152, 128.69591484942245, 130.20380198507485, 130.6692805771715, 127.4809501310885, 133.95273303274345, 137.64759591889663, 136.95935793930215, 135.78607277295546, 133.86130519139883, 136.1213302273803, 138.04177532800492, 139.45555317698927, 139.69030335817052, 139.9731762224708, 141.74267045854987, 136.38125394845522, 135.35271199159396, 135.48355294651915, 130.59937086279714, 128.98538030455106, 128.0523860536815, 127.33209138212953, 129.542364268371, 127.49883347203746, 128.76850935536314, 130.00790625790853, 132.06094777756465, 130.8734997554647, 132.58685198976335, 132.3813423691792, 130.29082033759926, 132.71422959587858, 134.25813846684397, 133.85872050490514, 135.574343106043, 136.90184188228966, 136.1488817519371, 137.4774679230331, 138.87171857688585, 139.31610807633191, 140.05448344913646, 139.5047696015986, 141.2078903304848, 141.9762966003665, 142.11235721794188, 142.828840352249, 142.9930935186541, 143.3195944155217, 145.5780723575196, 145.60597482138812, 145.5101175546307, 146.04971757389583, 145.0225012269594, 146.95734309749577, 146.5201450017807, 145.48717797951815, 147.6149279574686, 150.0862478763566, 151.37824032682553, 151.7276717649133, 150.35706512997024, 152.25031117506512, 148.70034141949986, 153.26172439254302, 152.07139166642762, 152.32104220108, 152.0087840645678, 151.2968762591987, 150.6538645350971, 150.2056692881052, 148.89262139574504, 150.87041171661852, 151.8950732628606, 151.75710190464682, 151.9607093497022, 152.05061943606745, 153.65095220563205, 155.37312329493685, 159.63034687321814, 162.07801219194081, 159.6265822575377, 160.37682719414815, 159.14326209831316, 158.18707633187245, 158.1158921475231, 159.2437855115088, 162.46316404193314, 163.8996091840039, 168.58304051643682, 168.12224687235855, 169.38036168645337, 169.05148148417882, 167.4088645890909, 169.61726652779535, 169.92399108476647, 168.71186661502847, 170.7546861332921, 170.4074849381544, 169.74999605876803, 173.29494180979532, 171.31360297510335, 170.70258445782548, 174.83358700170484, 176.13464027830918, 183.19323598746246, 186.06021013066623, 188.41697279232133, 190.89936640886015, 196.12523656430267, 191.29401823693536, 194.0279285809049, 197.2083697102269, 201.86413063846916, 195.28672438183236, 194.90754265865763, 188.82805155722968, 186.7289130507518, 190.1833979421907, 189.99004482094946, 188.28646741905496, 188.8293600667799, 193.49187168242878, 189.9558077274324, 193.17555866841238, 190.65944701675633, 192.93623857988143, 196.97342937216547, 198.9661438993139, 200.430203108173, 199.1107042710442, 201.3689515086516, 203.12925175975641, 205.9273572027471, 200.6968023297973, 200.18669795720908, 201.7081168616839, 200.90968889910638, 204.67674556596464, 201.91019822173138, 199.52765788271495, 206.09876874898566, 206.5916883042436, 210.05898547294984, 212.07205075039894, 206.9399071222393, 205.78621714003282, 204.44346207319413, 206.17271302322985, 208.4010964298226, 211.72683059368183, 212.1326403523197, 213.4107395104424, 215.994787881348, 214.19123140253876, 217.95207244058167, 222.13311969690017, 221.10760512763278, 219.19133921652664, 217.75563594465837, 211.64940498671024, 211.9157304879852, 213.32850202457175, 213.89204481741996, 218.93989707511108, 218.35788184871973, 219.717159663228, 220.9548996626642, 222.56050526687957, 225.45564650622626, 224.08412465664668, 223.23820708606786, 231.728700228908, 228.1330432303561, 230.8364197926358, 234.9664680700924, 234.29289752829146, 235.96418686399326, 239.11430875862754, 238.18176295446892, 239.27739906405944, 237.19967364885318, 239.39179396615796, 241.78770683743593, 244.74557645108055, 244.2275316475924, 245.24921682165152, 243.63465947757567, 244.46707789745736, 244.5139340873877, 242.88587880958917, 244.87147086885756, 246.612098907617, 249.64953792582918, 244.97276991535196, 244.10719946165108, 245.1405866060387, 247.79831913249254, 246.68735666838185, 247.77278103772272, 249.10868928215112, 246.68195546739415, 243.429865021149, 244.31026969964216, 246.63732501853124, 247.2436417758684, 250.92757203832883, 258.3633924230646, 268.4158814165921, 273.74841026073506, 277.9709794890069, 285.13336506050695, 289.68837056734856, 297.39166715468525, 295.3520559707827, 297.00602748421915, 303.52778483772676, 303.44937349331036, 303.8590301475263, 301.5775552628353, 300.4692577472444, 301.9440610206871, 301.47101532508805, 312.2812634829535, 312.95787288716656, 314.04018553090134, 313.4906152062223, 318.09370240616704, 318.3481773680919, 318.96895631395967, 322.35534340015954, 323.7683343220636, 329.37762071419337, 336.09417936325696, 333.0749333186437, 333.11934330975276, 336.5865604747018, 336.45753562651987, 339.26134842340747, 335.5068561675218, 332.9877588557973, 337.87435421700616, 338.0827100687733, 337.9108513578216, 345.32517862136456, 350.07915524705203, 346.3128870018525, 347.31719437415785, 342.7528675780908, 343.73256952458485, 349.82809375748747, 353.0610883906296, 353.9761050447087, 356.9288557209566, 358.50826590752183, 364.7044837699568, 358.10941102178344, 360.5773817127419, 359.6188468396889, 362.4508452585515, 364.1090578756093, 363.4081479391988, 362.92966054441223, 365.40967989146566, 364.62100399903323, 358.4558705230829, 355.8451169327731, 358.98545008970484, 368.22932542951474, 366.96813998991865, 356.1701024707153, 351.7506251158911, 352.7150080797505, 356.9975561361855, 360.9513040703937, 363.1170118948161, 357.845763272143, 355.62413749182844, 359.1211081771648, 363.4066200680789, 363.05229861351256, 365.7298093157872, 365.79990752923936, 364.5714295064537, 369.27136285184105, 372.1978384024419, 372.3529208351096, 374.9562883399484, 372.46595365822395, 369.8121337384091, 375.16824614205376, 374.14278626926546, 386.50820535546467, 400.1422822993787, 403.3934383430611, 408.27113733502597, 408.2167011833813, 409.0909652850824, 411.8796020317757, 416.58875881500575, 414.0614536781947, 404.2067910806536, 410.2934716763432, 397.21194815439577, 382.6541302545372, 387.9283796832122, 404.56404169862725, 405.3158565427839, 415.55008192048916, 435.84277758760635, 434.20110312535974, 433.29651749384857, 432.63935110898296, 427.0583034796771, 438.31128977636666, 433.1136483984353, 447.9586186972916, 445.83454824696867, 463.5304645244713, 486.74947804327627, 495.4947436654538, 479.9073048543114, 478.7955195980656, 480.3715548500759, 491.30801391549596, 482.1860617904649, 472.97228979308545, 472.5426732965235, 485.46277755557253, 509.1088603456752, 486.9498971991296, 503.8835798742293, 526.7934866391777, 538.791208297385, 535.5898905347514, 538.4062007091466, 538.5138819492884, 555.1898618269846, 559.8580832485133, 583.773354371279, 580.7620568183138, 605.6622300043989, 615.2367404237186, 615.1342009669813, 627.1703268325685, 618.9230370347203, 609.7526607026558, 619.0310636896812, 638.6852999618285, 648.0952633812661, 662.8934385618049, 660.0374726640011, 670.455064107548, 672.6061074382262, 684.6681769649517, 674.6035547635669, 683.1372897313261, 694.1585713389915, 689.8374342324063, 684.8706047059329, 687.3247243727959, 687.9032226824763, 690.4713947138242, 693.1642331532082, 679.9305726685916, 671.3181187481226, 669.1810894034411, 671.2332447442784, 667.6813021575066, 665.3611096325093, 678.2746598352935, 675.0189414680841, 679.7046979534416, 687.3627042170505, 697.1232546169325, 693.4633575301937, 690.1462911366742, 666.4167611597583, 662.2794237675581, 670.8338663245557, 684.5021063509187, 681.6214933200251, 688.8126000745515, 685.5177798041948, 677.6457506327768, 673.303170780805, 679.7051617629791, 699.5412240670953, 691.7063623575439, 686.1957683374288, 679.722654922779, 694.3310276481611, 690.6915758449054, 681.7356084114497, 700.9719148287928, 691.3452338651441, 696.8529508949364, 712.5147209663, 722.561178531925, 735.2782552740869, 735.9216237474517, 734.1063504088746, 721.736658404485, 714.2847274064587, 714.8323456974703, 719.3298325391501, 724.1313591713489, 712.0926753251252, 725.0705643329255, 730.8348753193723, 734.8727380055118, 723.9721257250967, 715.9119027253572, 707.4104488804936, 712.0675676689568, 709.2074296054865, 701.559809489574, 704.5823630021249, 702.6565045432524, 701.8543050338989, 728.9049397070803, 718.9614614879096, 727.7927047731861, 759.6639603030452, 759.3854168509341, 785.4766341315707, 786.8119444095944, 781.2518066690999, 814.5917275187038, 799.9222881589712, 897.379486933006, 883.3205416377222, 875.7166239751241, 858.5087923140129, 868.3172552662006, 863.2665432314022, 869.8489506235417, 879.6564975418221, 888.2184874512293, 905.0132186847861, 902.2152194836856, 905.0421605047345, 922.1323733022656, 913.003262806573, 920.7866156219993, 924.8764428397203, 916.6219205873758, 912.9019632929921, 921.0800433808253, 924.0889048558694, 939.5673940122053, 956.5735638438263, 956.589506736557, 956.4619614689923, 963.404281205988, 986.3734449437409, 986.061093352842, 1000.8602269289125, 1028.2837971467648, 1056.0731667646562, 1019.3306211709693, 1009.0693595845149, 1013.7026697272739, 990.7845418691899, 1004.9279912043726, 1000.3639432443194, 995.63722361249, 1018.2547825422203, 993.1378312395121, 997.8386836407126, 999.5682706923566, 1008.5477256574096, 1012.1448792122544, 1000.9522437562989, 996.0892841053828, 1017.837233475017, 1023.4014103513472, 1025.4993832425675, 1054.803028118724, 1035.333122224699, 1046.7217865691707, 1048.4401548354551, 1030.9049932458322, 1017.0650937115068, 1017.5397240885723, 1014.7499693450294, 1001.8372759851139, 1014.2767554952625, 1013.7442601986276, 1010.8719847947315, 1009.5410033480852, 1017.146212239974, 1014.0269638557716, 1014.2804705967355, 1013.5958312790829, 1010.9942686454664, 1011.1880425469567, 1032.313446069167, 1049.96600599695, 1078.4638333430503, 1083.5775493528188, 1106.856407038082, 1098.5549839852963, 1070.825625264534, 1071.1468729521134, 1071.236135191526, 1058.2295431167422, 1072.938933766065, 1062.9695428398218, 1047.2553097648397, 1074.1959526085402, 1098.8935078855982, 1077.629918508012, 1072.969169110465, 1083.8150991282232, 1086.1091744213782, 1092.2637930764326, 1073.3130162665566, 1102.2924677057536, 1112.2314714562337, 1115.5959716573889, 1097.2072313912363, 1102.1080903581171, 1098.2690805100365, 1076.7338542897023, 1081.7945034048637, 1080.8749780769697, 1087.864636268534, 1097.564762608595, 1081.2933650029227, 1067.083368031176, 1071.565118176907, 1076.0478322546137, 1069.4391051515165, 1056.0532923520368], \"x\": [1970.0, 1970.08, 1970.16, 1970.24, 1970.32, 1970.4, 1970.48, 1970.56, 1970.64, 1970.72, 1970.8, 1970.88, 1970.96, 1971.04, 1971.12, 1971.2, 1971.28, 1971.36, 1971.44, 1971.52, 1971.6, 1971.68, 1971.76, 1971.84, 1971.92, 1972.0, 1972.08, 1972.16, 1972.24, 1972.32, 1972.4, 1972.48, 1972.56, 1972.64, 1972.72, 1972.8, 1972.88, 1972.96, 1973.04, 1973.12, 1973.2, 1973.28, 1973.36, 1973.44, 1973.52, 1973.6, 1973.68, 1973.76, 1973.84, 1973.92, 1974.0, 1974.08, 1974.16, 1974.24, 1974.32, 1974.4, 1974.48, 1974.56, 1974.64, 1974.72, 1974.8, 1974.88, 1974.96, 1975.04, 1975.12, 1975.2, 1975.28, 1975.36, 1975.44, 1975.52, 1975.6, 1975.68, 1975.76, 1975.84, 1975.92, 1976.0, 1976.08, 1976.16, 1976.24, 1976.32, 1976.4, 1976.48, 1976.56, 1976.64, 1976.72, 1976.8, 1976.88, 1976.96, 1977.04, 1977.12, 1977.2, 1977.28, 1977.36, 1977.44, 1977.52, 1977.6, 1977.68, 1977.76, 1977.84, 1977.92, 1978.0, 1978.08, 1978.16, 1978.24, 1978.32, 1978.4, 1978.48, 1978.56, 1978.64, 1978.72, 1978.8, 1978.88, 1978.96, 1979.04, 1979.12, 1979.2, 1979.28, 1979.36, 1979.44, 1979.52, 1979.6, 1979.68, 1979.76, 1979.84, 1979.92, 1980.0, 1980.08, 1980.16, 1980.24, 1980.32, 1980.4, 1980.48, 1980.56, 1980.64, 1980.72, 1980.8, 1980.88, 1980.96, 1981.04, 1981.12, 1981.2, 1981.28, 1981.36, 1981.44, 1981.52, 1981.6, 1981.68, 1981.76, 1981.84, 1981.92, 1982.0, 1982.08, 1982.16, 1982.24, 1982.32, 1982.4, 1982.48, 1982.56, 1982.64, 1982.72, 1982.8, 1982.88, 1982.96, 1983.04, 1983.12, 1983.2, 1983.28, 1983.36, 1983.44, 1983.52, 1983.6, 1983.68, 1983.76, 1983.84, 1983.92, 1984.0, 1984.08, 1984.16, 1984.24, 1984.32, 1984.4, 1984.48, 1984.56, 1984.64, 1984.72, 1984.8, 1984.88, 1984.96, 1985.04, 1985.12, 1985.2, 1985.28, 1985.36, 1985.44, 1985.52, 1985.6, 1985.68, 1985.76, 1985.84, 1985.92, 1986.0, 1986.08, 1986.16, 1986.24, 1986.32, 1986.4, 1986.48, 1986.56, 1986.64, 1986.72, 1986.8, 1986.88, 1986.96, 1987.04, 1987.12, 1987.2, 1987.28, 1987.36, 1987.44, 1987.52, 1987.6, 1987.68, 1987.76, 1987.84, 1987.92, 1988.0, 1988.08, 1988.16, 1988.24, 1988.32, 1988.4, 1988.48, 1988.56, 1988.64, 1988.72, 1988.8, 1988.88, 1988.96, 1989.04, 1989.12, 1989.2, 1989.28, 1989.36, 1989.44, 1989.52, 1989.6, 1989.68, 1989.76, 1989.84, 1989.92, 1990.0, 1990.08, 1990.16, 1990.24, 1990.32, 1990.4, 1990.48, 1990.56, 1990.64, 1990.72, 1990.8, 1990.88, 1990.96, 1991.04, 1991.12, 1991.2, 1991.28, 1991.36, 1991.44, 1991.52, 1991.6, 1991.68, 1991.76, 1991.84, 1991.92, 1992.0, 1992.08, 1992.16, 1992.24, 1992.32, 1992.4, 1992.48, 1992.56, 1992.64, 1992.72, 1992.8, 1992.88, 1992.96, 1993.04, 1993.12, 1993.2, 1993.28, 1993.36, 1993.44, 1993.52, 1993.6, 1993.68, 1993.76, 1993.84, 1993.92, 1994.0, 1994.08, 1994.16, 1994.24, 1994.32, 1994.4, 1994.48, 1994.56, 1994.64, 1994.72, 1994.8, 1994.88, 1994.96, 1995.04, 1995.12, 1995.2, 1995.28, 1995.36, 1995.44, 1995.52, 1995.6, 1995.68, 1995.76, 1995.84, 1995.92, 1996.0, 1996.08, 1996.16, 1996.24, 1996.32, 1996.4, 1996.48, 1996.56, 1996.64, 1996.72, 1996.8, 1996.88, 1996.96, 1997.04, 1997.12, 1997.2, 1997.28, 1997.36, 1997.44, 1997.52, 1997.6, 1997.68, 1997.76, 1997.84, 1997.92, 1998.0, 1998.08, 1998.16, 1998.24, 1998.32, 1998.4, 1998.48, 1998.56, 1998.64, 1998.72, 1998.8, 1998.88, 1998.96, 1999.04, 1999.12, 1999.2, 1999.28, 1999.36, 1999.44, 1999.52, 1999.6, 1999.68, 1999.76, 1999.84, 1999.92, 2000.0, 2000.08, 2000.16, 2000.24, 2000.32, 2000.4, 2000.48, 2000.56, 2000.64, 2000.72, 2000.8, 2000.88, 2000.96, 2001.04, 2001.12, 2001.2, 2001.28, 2001.36, 2001.44, 2001.52, 2001.6, 2001.68, 2001.76, 2001.84, 2001.92, 2002.0, 2002.08, 2002.16, 2002.24, 2002.32, 2002.4, 2002.48, 2002.56, 2002.64, 2002.72, 2002.8, 2002.88, 2002.96, 2003.04, 2003.12, 2003.2, 2003.28, 2003.36, 2003.44, 2003.52, 2003.6, 2003.68, 2003.76, 2003.84, 2003.92, 2004.0, 2004.08, 2004.16, 2004.24, 2004.32, 2004.4, 2004.48, 2004.56, 2004.64, 2004.72, 2004.8, 2004.88, 2004.96, 2005.04, 2005.12, 2005.2, 2005.28, 2005.36, 2005.44, 2005.52, 2005.6, 2005.68, 2005.76, 2005.84, 2005.92, 2006.0, 2006.08, 2006.16, 2006.24, 2006.32, 2006.4, 2006.48, 2006.56, 2006.64, 2006.72, 2006.8, 2006.88, 2006.96, 2007.04, 2007.12, 2007.2, 2007.28, 2007.36, 2007.44, 2007.52, 2007.6, 2007.68, 2007.76, 2007.84, 2007.92, 2008.0, 2008.08, 2008.16, 2008.24, 2008.32, 2008.4, 2008.48, 2008.56, 2008.64, 2008.72, 2008.8, 2008.88, 2008.96, 2009.04, 2009.12, 2009.2, 2009.28, 2009.36, 2009.44, 2009.52, 2009.6, 2009.68, 2009.76, 2009.84, 2009.92, 2010.0, 2010.08, 2010.16, 2010.24, 2010.32, 2010.4, 2010.48, 2010.56, 2010.64, 2010.72, 2010.8, 2010.88, 2010.96, 2011.04, 2011.12, 2011.2, 2011.28, 2011.36, 2011.44, 2011.52, 2011.6, 2011.68, 2011.76, 2011.84, 2011.92, 2012.0, 2012.08, 2012.16, 2012.24, 2012.32, 2012.4, 2012.48, 2012.56, 2012.64, 2012.72, 2012.8, 2012.88, 2012.96, 2013.04, 2013.12, 2013.2, 2013.28, 2013.36, 2013.44, 2013.52, 2013.6, 2013.68, 2013.76, 2013.84, 2013.92, 2014.0, 2014.08, 2014.16, 2014.24, 2014.32, 2014.4, 2014.48, 2014.56, 2014.64, 2014.72, 2014.8, 2014.88, 2014.96, 2015.04, 2015.12, 2015.2, 2015.28, 2015.36, 2015.44, 2015.52, 2015.6, 2015.68, 2015.76, 2015.84, 2015.92, 2016.0], \"type\": \"scatter\", \"name\": \"Growth of $1\"}], {\"yaxis\": {\"type\": \"log\", \"autorange\": true}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"ebe3136f-9b34-4932-8fcf-05303fde3f43\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ebe3136f-9b34-4932-8fcf-05303fde3f43\", [{\"y\": [99.20583333333333, 99.02560940277777, 98.45868778894688, 100.853695369413, 103.51959471701116, 102.72508182755809, 105.21873318892206, 106.66812123859945, 109.82727542928264, 108.08376743184279, 108.51159901126049, 108.9628264104823, 110.80157410615918, 111.54486799912134, 113.28961564274094, 110.67168144126327, 111.15402551954476, 111.4587728061775, 111.6835479980033, 110.45782105872523, 111.86063538617104, 111.51852827628166, 112.82980030459693, 112.29197825647834, 112.7074585760273, 113.62884204988634, 114.89390982470842, 115.21657022146614, 117.10324155884267, 117.93565043425676, 118.12533027203852, 119.36859937315172, 120.43097990757276, 120.37377519211667, 120.76298373190453, 120.80323805981517, 121.79684469285715, 126.54184676734972, 126.92041779226204, 128.01933707631338, 130.93177699479952, 133.94429896382152, 128.69591484942245, 130.20380198507485, 130.6692805771715, 127.4809501310885, 133.95273303274345, 137.64759591889663, 136.95935793930215, 135.78607277295546, 133.86130519139883, 136.1213302273803, 138.04177532800492, 139.45555317698927, 139.69030335817052, 139.9731762224708, 141.74267045854987, 136.38125394845522, 135.35271199159396, 135.48355294651915, 130.59937086279714, 128.98538030455106, 128.0523860536815, 127.33209138212953, 129.542364268371, 127.49883347203746, 128.76850935536314, 130.00790625790853, 132.06094777756465, 130.8734997554647, 132.58685198976335, 132.3813423691792, 130.29082033759926, 132.71422959587858, 134.25813846684397, 133.85872050490514, 135.574343106043, 136.90184188228966, 136.1488817519371, 137.4774679230331, 138.87171857688585, 139.31610807633191, 140.05448344913646, 139.5047696015986, 141.2078903304848, 141.9762966003665, 142.11235721794188, 142.828840352249, 142.9930935186541, 143.3195944155217, 145.5780723575196, 145.60597482138812, 145.5101175546307, 146.04971757389583, 145.0225012269594, 146.95734309749577, 146.5201450017807, 145.48717797951815, 147.6149279574686, 150.0862478763566, 151.37824032682553, 151.7276717649133, 150.35706512997024, 152.25031117506512, 148.70034141949986, 153.26172439254302, 152.07139166642762, 152.32104220108, 152.0087840645678, 151.2968762591987, 150.6538645350971, 150.2056692881052, 148.89262139574504, 150.87041171661852, 151.8950732628606, 151.75710190464682, 151.9607093497022, 152.05061943606745, 153.65095220563205, 155.37312329493685, 159.63034687321814, 162.07801219194081, 159.6265822575377, 160.37682719414815, 159.14326209831316, 158.18707633187245, 158.1158921475231, 159.2437855115088, 162.46316404193314, 163.8996091840039, 168.58304051643682, 168.12224687235855, 169.38036168645337, 169.05148148417882, 167.4088645890909, 169.61726652779535, 169.92399108476647, 168.71186661502847, 170.7546861332921, 170.4074849381544, 169.74999605876803, 173.29494180979532, 171.31360297510335, 170.70258445782548, 174.83358700170484, 176.13464027830918, 183.19323598746246, 186.06021013066623, 188.41697279232133, 190.89936640886015, 196.12523656430267, 191.29401823693536, 194.0279285809049, 197.2083697102269, 201.86413063846916, 195.28672438183236, 194.90754265865763, 188.82805155722968, 186.7289130507518, 190.1833979421907, 189.99004482094946, 188.28646741905496, 188.8293600667799, 193.49187168242878, 189.9558077274324, 193.17555866841238, 190.65944701675633, 192.93623857988143, 196.97342937216547, 198.9661438993139, 200.430203108173, 199.1107042710442, 201.3689515086516, 203.12925175975641, 205.9273572027471, 200.6968023297973, 200.18669795720908, 201.7081168616839, 200.90968889910638, 204.67674556596464, 201.91019822173138, 199.52765788271495, 206.09876874898566, 206.5916883042436, 210.05898547294984, 212.07205075039894, 206.9399071222393, 205.78621714003282, 204.44346207319413, 206.17271302322985, 208.4010964298226, 211.72683059368183, 212.1326403523197, 213.4107395104424, 215.994787881348, 214.19123140253876, 217.95207244058167, 222.13311969690017, 221.10760512763278, 219.19133921652664, 217.75563594465837, 211.64940498671024, 211.9157304879852, 213.32850202457175, 213.89204481741996, 218.93989707511108, 218.35788184871973, 219.717159663228, 220.9548996626642, 222.56050526687957, 225.45564650622626, 224.08412465664668, 223.23820708606786, 231.728700228908, 228.1330432303561, 230.8364197926358, 234.9664680700924, 234.29289752829146, 235.96418686399326, 239.11430875862754, 238.18176295446892, 239.27739906405944, 237.19967364885318, 239.39179396615796, 241.78770683743593, 244.74557645108055, 244.2275316475924, 245.24921682165152, 243.63465947757567, 244.46707789745736, 244.5139340873877, 242.88587880958917, 244.87147086885756, 246.612098907617, 249.64953792582918, 244.97276991535196, 244.10719946165108, 245.1405866060387, 247.79831913249254, 246.68735666838185, 247.77278103772272, 249.10868928215112, 246.68195546739415, 243.429865021149, 244.31026969964216, 246.63732501853124, 247.2436417758684, 250.92757203832883, 258.3633924230646, 268.4158814165921, 273.74841026073506, 277.9709794890069, 285.13336506050695, 289.68837056734856, 297.39166715468525, 295.3520559707827, 297.00602748421915, 303.52778483772676, 303.44937349331036, 303.8590301475263, 301.5775552628353, 300.4692577472444, 301.9440610206871, 301.47101532508805, 312.2812634829535, 312.95787288716656, 314.04018553090134, 313.4906152062223, 318.09370240616704, 318.3481773680919, 318.96895631395967, 322.35534340015954, 323.7683343220636, 329.37762071419337, 336.09417936325696, 333.0749333186437, 333.11934330975276, 336.5865604747018, 336.45753562651987, 339.26134842340747, 335.5068561675218, 332.9877588557973, 337.87435421700616, 338.0827100687733, 337.9108513578216, 345.32517862136456, 350.07915524705203, 346.3128870018525, 347.31719437415785, 342.7528675780908, 343.73256952458485, 349.82809375748747, 353.0610883906296, 353.9761050447087, 356.9288557209566, 358.50826590752183, 364.7044837699568, 358.10941102178344, 360.5773817127419, 359.6188468396889, 362.4508452585515, 364.1090578756093, 363.4081479391988, 362.92966054441223, 365.40967989146566, 364.62100399903323, 358.4558705230829, 355.8451169327731, 358.98545008970484, 368.22932542951474, 366.96813998991865, 356.1701024707153, 351.7506251158911, 352.7150080797505, 356.9975561361855, 360.9513040703937, 363.1170118948161, 357.845763272143, 355.62413749182844, 359.1211081771648, 363.4066200680789, 363.05229861351256, 365.7298093157872, 365.79990752923936, 364.5714295064537, 369.27136285184105, 372.1978384024419, 372.3529208351096, 374.9562883399484, 372.46595365822395, 369.8121337384091, 375.16824614205376, 374.14278626926546, 386.50820535546467, 400.1422822993787, 403.3934383430611, 408.27113733502597, 408.2167011833813, 409.0909652850824, 411.8796020317757, 416.58875881500575, 414.0614536781947, 404.2067910806536, 410.2934716763432, 397.21194815439577, 382.6541302545372, 387.9283796832122, 404.56404169862725, 405.3158565427839, 415.55008192048916, 435.84277758760635, 434.20110312535974, 433.29651749384857, 432.63935110898296, 427.0583034796771, 438.31128977636666, 433.1136483984353, 447.9586186972916, 445.83454824696867, 463.5304645244713, 486.74947804327627, 495.4947436654538, 479.9073048543114, 478.7955195980656, 480.3715548500759, 491.30801391549596, 482.1860617904649, 472.97228979308545, 472.5426732965235, 485.46277755557253, 509.1088603456752, 486.9498971991296, 503.8835798742293, 526.7934866391777, 538.791208297385, 535.5898905347514, 538.4062007091466, 538.5138819492884, 555.1898618269846, 559.8580832485133, 583.773354371279, 580.7620568183138, 605.6622300043989, 615.2367404237186, 615.1342009669813, 627.1703268325685, 618.9230370347203, 609.7526607026558, 619.0310636896812, 638.6852999618285, 648.0952633812661, 662.8934385618049, 660.0374726640011, 670.455064107548, 672.6061074382262, 684.6681769649517, 674.6035547635669, 683.1372897313261, 694.1585713389915, 689.8374342324063, 684.8706047059329, 687.3247243727959, 687.9032226824763, 690.4713947138242, 693.1642331532082, 679.9305726685916, 671.3181187481226, 669.1810894034411, 671.2332447442784, 667.6813021575066, 665.3611096325093, 678.2746598352935, 675.0189414680841, 679.7046979534416, 687.3627042170505, 697.1232546169325, 693.4633575301937, 690.1462911366742, 666.4167611597583, 662.2794237675581, 670.8338663245557, 684.5021063509187, 681.6214933200251, 688.8126000745515, 685.5177798041948, 677.6457506327768, 673.303170780805, 679.7051617629791, 699.5412240670953, 691.7063623575439, 686.1957683374288, 679.722654922779, 694.3310276481611, 690.6915758449054, 681.7356084114497, 700.9719148287928, 691.3452338651441, 696.8529508949364, 712.5147209663, 722.561178531925, 735.2782552740869, 735.9216237474517, 734.1063504088746, 721.736658404485, 714.2847274064587, 714.8323456974703, 719.3298325391501, 724.1313591713489, 712.0926753251252, 725.0705643329255, 730.8348753193723, 734.8727380055118, 723.9721257250967, 715.9119027253572, 707.4104488804936, 712.0675676689568, 709.2074296054865, 701.559809489574, 704.5823630021249, 702.6565045432524, 701.8543050338989, 728.9049397070803, 718.9614614879096, 727.7927047731861, 759.6639603030452, 759.3854168509341, 785.4766341315707, 786.8119444095944, 781.2518066690999, 814.5917275187038, 799.9222881589712, 897.379486933006, 883.3205416377222, 875.7166239751241, 858.5087923140129, 868.3172552662006, 863.2665432314022, 869.8489506235417, 879.6564975418221, 888.2184874512293, 905.0132186847861, 902.2152194836856, 905.0421605047345, 922.1323733022656, 913.003262806573, 920.7866156219993, 924.8764428397203, 916.6219205873758, 912.9019632929921, 921.0800433808253, 924.0889048558694, 939.5673940122053, 956.5735638438263, 956.589506736557, 956.4619614689923, 963.404281205988, 986.3734449437409, 986.061093352842, 1000.8602269289125, 1028.2837971467648, 1056.0731667646562, 1019.3306211709693, 1009.0693595845149, 1013.7026697272739, 990.7845418691899, 1004.9279912043726, 1000.3639432443194, 995.63722361249, 1018.2547825422203, 993.1378312395121, 997.8386836407126, 999.5682706923566, 1008.5477256574096, 1012.1448792122544, 1000.9522437562989, 996.0892841053828, 1017.837233475017, 1023.4014103513472, 1025.4993832425675, 1054.803028118724, 1035.333122224699, 1046.7217865691707, 1048.4401548354551, 1030.9049932458322, 1017.0650937115068, 1017.5397240885723, 1014.7499693450294, 1001.8372759851139, 1014.2767554952625, 1013.7442601986276, 1010.8719847947315, 1009.5410033480852, 1017.146212239974, 1014.0269638557716, 1014.2804705967355, 1013.5958312790829, 1010.9942686454664, 1011.1880425469567, 1032.313446069167, 1049.96600599695, 1078.4638333430503, 1083.5775493528188, 1106.856407038082, 1098.5549839852963, 1070.825625264534, 1071.1468729521134, 1071.236135191526, 1058.2295431167422, 1072.938933766065, 1062.9695428398218, 1047.2553097648397, 1074.1959526085402, 1098.8935078855982, 1077.629918508012, 1072.969169110465, 1083.8150991282232, 1086.1091744213782, 1092.2637930764326, 1073.3130162665566, 1102.2924677057536, 1112.2314714562337, 1115.5959716573889, 1097.2072313912363, 1102.1080903581171, 1098.2690805100365, 1076.7338542897023, 1081.7945034048637, 1080.8749780769697, 1087.864636268534, 1097.564762608595, 1081.2933650029227, 1067.083368031176, 1071.565118176907, 1076.0478322546137, 1069.4391051515165, 1056.0532923520368], \"x\": [1970.0, 1970.08, 1970.16, 1970.24, 1970.32, 1970.4, 1970.48, 1970.56, 1970.64, 1970.72, 1970.8, 1970.88, 1970.96, 1971.04, 1971.12, 1971.2, 1971.28, 1971.36, 1971.44, 1971.52, 1971.6, 1971.68, 1971.76, 1971.84, 1971.92, 1972.0, 1972.08, 1972.16, 1972.24, 1972.32, 1972.4, 1972.48, 1972.56, 1972.64, 1972.72, 1972.8, 1972.88, 1972.96, 1973.04, 1973.12, 1973.2, 1973.28, 1973.36, 1973.44, 1973.52, 1973.6, 1973.68, 1973.76, 1973.84, 1973.92, 1974.0, 1974.08, 1974.16, 1974.24, 1974.32, 1974.4, 1974.48, 1974.56, 1974.64, 1974.72, 1974.8, 1974.88, 1974.96, 1975.04, 1975.12, 1975.2, 1975.28, 1975.36, 1975.44, 1975.52, 1975.6, 1975.68, 1975.76, 1975.84, 1975.92, 1976.0, 1976.08, 1976.16, 1976.24, 1976.32, 1976.4, 1976.48, 1976.56, 1976.64, 1976.72, 1976.8, 1976.88, 1976.96, 1977.04, 1977.12, 1977.2, 1977.28, 1977.36, 1977.44, 1977.52, 1977.6, 1977.68, 1977.76, 1977.84, 1977.92, 1978.0, 1978.08, 1978.16, 1978.24, 1978.32, 1978.4, 1978.48, 1978.56, 1978.64, 1978.72, 1978.8, 1978.88, 1978.96, 1979.04, 1979.12, 1979.2, 1979.28, 1979.36, 1979.44, 1979.52, 1979.6, 1979.68, 1979.76, 1979.84, 1979.92, 1980.0, 1980.08, 1980.16, 1980.24, 1980.32, 1980.4, 1980.48, 1980.56, 1980.64, 1980.72, 1980.8, 1980.88, 1980.96, 1981.04, 1981.12, 1981.2, 1981.28, 1981.36, 1981.44, 1981.52, 1981.6, 1981.68, 1981.76, 1981.84, 1981.92, 1982.0, 1982.08, 1982.16, 1982.24, 1982.32, 1982.4, 1982.48, 1982.56, 1982.64, 1982.72, 1982.8, 1982.88, 1982.96, 1983.04, 1983.12, 1983.2, 1983.28, 1983.36, 1983.44, 1983.52, 1983.6, 1983.68, 1983.76, 1983.84, 1983.92, 1984.0, 1984.08, 1984.16, 1984.24, 1984.32, 1984.4, 1984.48, 1984.56, 1984.64, 1984.72, 1984.8, 1984.88, 1984.96, 1985.04, 1985.12, 1985.2, 1985.28, 1985.36, 1985.44, 1985.52, 1985.6, 1985.68, 1985.76, 1985.84, 1985.92, 1986.0, 1986.08, 1986.16, 1986.24, 1986.32, 1986.4, 1986.48, 1986.56, 1986.64, 1986.72, 1986.8, 1986.88, 1986.96, 1987.04, 1987.12, 1987.2, 1987.28, 1987.36, 1987.44, 1987.52, 1987.6, 1987.68, 1987.76, 1987.84, 1987.92, 1988.0, 1988.08, 1988.16, 1988.24, 1988.32, 1988.4, 1988.48, 1988.56, 1988.64, 1988.72, 1988.8, 1988.88, 1988.96, 1989.04, 1989.12, 1989.2, 1989.28, 1989.36, 1989.44, 1989.52, 1989.6, 1989.68, 1989.76, 1989.84, 1989.92, 1990.0, 1990.08, 1990.16, 1990.24, 1990.32, 1990.4, 1990.48, 1990.56, 1990.64, 1990.72, 1990.8, 1990.88, 1990.96, 1991.04, 1991.12, 1991.2, 1991.28, 1991.36, 1991.44, 1991.52, 1991.6, 1991.68, 1991.76, 1991.84, 1991.92, 1992.0, 1992.08, 1992.16, 1992.24, 1992.32, 1992.4, 1992.48, 1992.56, 1992.64, 1992.72, 1992.8, 1992.88, 1992.96, 1993.04, 1993.12, 1993.2, 1993.28, 1993.36, 1993.44, 1993.52, 1993.6, 1993.68, 1993.76, 1993.84, 1993.92, 1994.0, 1994.08, 1994.16, 1994.24, 1994.32, 1994.4, 1994.48, 1994.56, 1994.64, 1994.72, 1994.8, 1994.88, 1994.96, 1995.04, 1995.12, 1995.2, 1995.28, 1995.36, 1995.44, 1995.52, 1995.6, 1995.68, 1995.76, 1995.84, 1995.92, 1996.0, 1996.08, 1996.16, 1996.24, 1996.32, 1996.4, 1996.48, 1996.56, 1996.64, 1996.72, 1996.8, 1996.88, 1996.96, 1997.04, 1997.12, 1997.2, 1997.28, 1997.36, 1997.44, 1997.52, 1997.6, 1997.68, 1997.76, 1997.84, 1997.92, 1998.0, 1998.08, 1998.16, 1998.24, 1998.32, 1998.4, 1998.48, 1998.56, 1998.64, 1998.72, 1998.8, 1998.88, 1998.96, 1999.04, 1999.12, 1999.2, 1999.28, 1999.36, 1999.44, 1999.52, 1999.6, 1999.68, 1999.76, 1999.84, 1999.92, 2000.0, 2000.08, 2000.16, 2000.24, 2000.32, 2000.4, 2000.48, 2000.56, 2000.64, 2000.72, 2000.8, 2000.88, 2000.96, 2001.04, 2001.12, 2001.2, 2001.28, 2001.36, 2001.44, 2001.52, 2001.6, 2001.68, 2001.76, 2001.84, 2001.92, 2002.0, 2002.08, 2002.16, 2002.24, 2002.32, 2002.4, 2002.48, 2002.56, 2002.64, 2002.72, 2002.8, 2002.88, 2002.96, 2003.04, 2003.12, 2003.2, 2003.28, 2003.36, 2003.44, 2003.52, 2003.6, 2003.68, 2003.76, 2003.84, 2003.92, 2004.0, 2004.08, 2004.16, 2004.24, 2004.32, 2004.4, 2004.48, 2004.56, 2004.64, 2004.72, 2004.8, 2004.88, 2004.96, 2005.04, 2005.12, 2005.2, 2005.28, 2005.36, 2005.44, 2005.52, 2005.6, 2005.68, 2005.76, 2005.84, 2005.92, 2006.0, 2006.08, 2006.16, 2006.24, 2006.32, 2006.4, 2006.48, 2006.56, 2006.64, 2006.72, 2006.8, 2006.88, 2006.96, 2007.04, 2007.12, 2007.2, 2007.28, 2007.36, 2007.44, 2007.52, 2007.6, 2007.68, 2007.76, 2007.84, 2007.92, 2008.0, 2008.08, 2008.16, 2008.24, 2008.32, 2008.4, 2008.48, 2008.56, 2008.64, 2008.72, 2008.8, 2008.88, 2008.96, 2009.04, 2009.12, 2009.2, 2009.28, 2009.36, 2009.44, 2009.52, 2009.6, 2009.68, 2009.76, 2009.84, 2009.92, 2010.0, 2010.08, 2010.16, 2010.24, 2010.32, 2010.4, 2010.48, 2010.56, 2010.64, 2010.72, 2010.8, 2010.88, 2010.96, 2011.04, 2011.12, 2011.2, 2011.28, 2011.36, 2011.44, 2011.52, 2011.6, 2011.68, 2011.76, 2011.84, 2011.92, 2012.0, 2012.08, 2012.16, 2012.24, 2012.32, 2012.4, 2012.48, 2012.56, 2012.64, 2012.72, 2012.8, 2012.88, 2012.96, 2013.04, 2013.12, 2013.2, 2013.28, 2013.36, 2013.44, 2013.52, 2013.6, 2013.68, 2013.76, 2013.84, 2013.92, 2014.0, 2014.08, 2014.16, 2014.24, 2014.32, 2014.4, 2014.48, 2014.56, 2014.64, 2014.72, 2014.8, 2014.88, 2014.96, 2015.04, 2015.12, 2015.2, 2015.28, 2015.36, 2015.44, 2015.52, 2015.6, 2015.68, 2015.76, 2015.84, 2015.92, 2016.0], \"type\": \"scatter\", \"name\": \"Growth of $1\"}], {\"yaxis\": {\"type\": \"log\", \"autorange\": true}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run performance chart\n",
    "perf = 100 * np.cumprod(1 + results / 100)\n",
    "\n",
    "def mychart(perf):\n",
    "    x_coords = np.linspace(1970, 2016, perf.shape[0])\n",
    "    \n",
    "    trace1 = Scatter(\n",
    "        x = x_coords,\n",
    "        y = perf,\n",
    "        name = 'Growth of $1',    \n",
    "    )\n",
    "\n",
    "    layout = Layout(\n",
    "        yaxis=dict(\n",
    "            type='log',\n",
    "            autorange=True\n",
    "        )\n",
    "    )\n",
    "    plotdata = [trace1]\n",
    "    \n",
    "    fig = Figure(data=plotdata, layout=layout)\n",
    "    \n",
    "    iplot(fig)\n",
    "    \n",
    "mychart(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:39:23 Starting\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................\n",
      "last prediction not stored\n",
      "MSE across all predictions: 0.0543\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>0.0260419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0494673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>0.535379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.173108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>0.231912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-12-31 00:00:00\n",
       "cagr                     0.0260419\n",
       "yearly_vol               0.0494673\n",
       "yearly_sharpe             0.535379\n",
       "max_drawdown             -0.173108\n",
       "sortino                   0.231912"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass coef_dict as None\n",
    "# fit_predict will do subset selection at each timestep using data it trains on\n",
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "model = LinearRegression()\n",
    "run_backtest(X, Y, model, coef_dict=None, startmonth=STARTMONTH, minmaxscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Growth of $1",
         "type": "scatter",
         "x": [
          1970,
          1970.08,
          1970.16,
          1970.24,
          1970.32,
          1970.4,
          1970.48,
          1970.56,
          1970.64,
          1970.72,
          1970.8,
          1970.88,
          1970.96,
          1971.04,
          1971.12,
          1971.2,
          1971.28,
          1971.36,
          1971.44,
          1971.52,
          1971.6,
          1971.68,
          1971.76,
          1971.84,
          1971.92,
          1972,
          1972.08,
          1972.16,
          1972.24,
          1972.32,
          1972.4,
          1972.48,
          1972.56,
          1972.64,
          1972.72,
          1972.8,
          1972.88,
          1972.96,
          1973.04,
          1973.12,
          1973.2,
          1973.28,
          1973.36,
          1973.44,
          1973.52,
          1973.6,
          1973.68,
          1973.76,
          1973.84,
          1973.92,
          1974,
          1974.08,
          1974.16,
          1974.24,
          1974.32,
          1974.4,
          1974.48,
          1974.56,
          1974.64,
          1974.72,
          1974.8,
          1974.88,
          1974.96,
          1975.04,
          1975.12,
          1975.2,
          1975.28,
          1975.36,
          1975.44,
          1975.52,
          1975.6,
          1975.68,
          1975.76,
          1975.84,
          1975.92,
          1976,
          1976.08,
          1976.16,
          1976.24,
          1976.32,
          1976.4,
          1976.48,
          1976.56,
          1976.64,
          1976.72,
          1976.8,
          1976.88,
          1976.96,
          1977.04,
          1977.12,
          1977.2,
          1977.28,
          1977.36,
          1977.44,
          1977.52,
          1977.6,
          1977.68,
          1977.76,
          1977.84,
          1977.92,
          1978,
          1978.08,
          1978.16,
          1978.24,
          1978.32,
          1978.4,
          1978.48,
          1978.56,
          1978.64,
          1978.72,
          1978.8,
          1978.88,
          1978.96,
          1979.04,
          1979.12,
          1979.2,
          1979.28,
          1979.36,
          1979.44,
          1979.52,
          1979.6,
          1979.68,
          1979.76,
          1979.84,
          1979.92,
          1980,
          1980.08,
          1980.16,
          1980.24,
          1980.32,
          1980.4,
          1980.48,
          1980.56,
          1980.64,
          1980.72,
          1980.8,
          1980.88,
          1980.96,
          1981.04,
          1981.12,
          1981.2,
          1981.28,
          1981.36,
          1981.44,
          1981.52,
          1981.6,
          1981.68,
          1981.76,
          1981.84,
          1981.92,
          1982,
          1982.08,
          1982.16,
          1982.24,
          1982.32,
          1982.4,
          1982.48,
          1982.56,
          1982.64,
          1982.72,
          1982.8,
          1982.88,
          1982.96,
          1983.04,
          1983.12,
          1983.2,
          1983.28,
          1983.36,
          1983.44,
          1983.52,
          1983.6,
          1983.68,
          1983.76,
          1983.84,
          1983.92,
          1984,
          1984.08,
          1984.16,
          1984.24,
          1984.32,
          1984.4,
          1984.48,
          1984.56,
          1984.64,
          1984.72,
          1984.8,
          1984.88,
          1984.96,
          1985.04,
          1985.12,
          1985.2,
          1985.28,
          1985.36,
          1985.44,
          1985.52,
          1985.6,
          1985.68,
          1985.76,
          1985.84,
          1985.92,
          1986,
          1986.08,
          1986.16,
          1986.24,
          1986.32,
          1986.4,
          1986.48,
          1986.56,
          1986.64,
          1986.72,
          1986.8,
          1986.88,
          1986.96,
          1987.04,
          1987.12,
          1987.2,
          1987.28,
          1987.36,
          1987.44,
          1987.52,
          1987.6,
          1987.68,
          1987.76,
          1987.84,
          1987.92,
          1988,
          1988.08,
          1988.16,
          1988.24,
          1988.32,
          1988.4,
          1988.48,
          1988.56,
          1988.64,
          1988.72,
          1988.8,
          1988.88,
          1988.96,
          1989.04,
          1989.12,
          1989.2,
          1989.28,
          1989.36,
          1989.44,
          1989.52,
          1989.6,
          1989.68,
          1989.76,
          1989.84,
          1989.92,
          1990,
          1990.08,
          1990.16,
          1990.24,
          1990.32,
          1990.4,
          1990.48,
          1990.56,
          1990.64,
          1990.72,
          1990.8,
          1990.88,
          1990.96,
          1991.04,
          1991.12,
          1991.2,
          1991.28,
          1991.36,
          1991.44,
          1991.52,
          1991.6,
          1991.68,
          1991.76,
          1991.84,
          1991.92,
          1992,
          1992.08,
          1992.16,
          1992.24,
          1992.32,
          1992.4,
          1992.48,
          1992.56,
          1992.64,
          1992.72,
          1992.8,
          1992.88,
          1992.96,
          1993.04,
          1993.12,
          1993.2,
          1993.28,
          1993.36,
          1993.44,
          1993.52,
          1993.6,
          1993.68,
          1993.76,
          1993.84,
          1993.92,
          1994,
          1994.08,
          1994.16,
          1994.24,
          1994.32,
          1994.4,
          1994.48,
          1994.56,
          1994.64,
          1994.72,
          1994.8,
          1994.88,
          1994.96,
          1995.04,
          1995.12,
          1995.2,
          1995.28,
          1995.36,
          1995.44,
          1995.52,
          1995.6,
          1995.68,
          1995.76,
          1995.84,
          1995.92,
          1996,
          1996.08,
          1996.16,
          1996.24,
          1996.32,
          1996.4,
          1996.48,
          1996.56,
          1996.64,
          1996.72,
          1996.8,
          1996.88,
          1996.96,
          1997.04,
          1997.12,
          1997.2,
          1997.28,
          1997.36,
          1997.44,
          1997.52,
          1997.6,
          1997.68,
          1997.76,
          1997.84,
          1997.92,
          1998,
          1998.08,
          1998.16,
          1998.24,
          1998.32,
          1998.4,
          1998.48,
          1998.56,
          1998.64,
          1998.72,
          1998.8,
          1998.88,
          1998.96,
          1999.04,
          1999.12,
          1999.2,
          1999.28,
          1999.36,
          1999.44,
          1999.52,
          1999.6,
          1999.68,
          1999.76,
          1999.84,
          1999.92,
          2000,
          2000.08,
          2000.16,
          2000.24,
          2000.32,
          2000.4,
          2000.48,
          2000.56,
          2000.64,
          2000.72,
          2000.8,
          2000.88,
          2000.96,
          2001.04,
          2001.12,
          2001.2,
          2001.28,
          2001.36,
          2001.44,
          2001.52,
          2001.6,
          2001.68,
          2001.76,
          2001.84,
          2001.92,
          2002,
          2002.08,
          2002.16,
          2002.24,
          2002.32,
          2002.4,
          2002.48,
          2002.56,
          2002.64,
          2002.72,
          2002.8,
          2002.88,
          2002.96,
          2003.04,
          2003.12,
          2003.2,
          2003.28,
          2003.36,
          2003.44,
          2003.52,
          2003.6,
          2003.68,
          2003.76,
          2003.84,
          2003.92,
          2004,
          2004.08,
          2004.16,
          2004.24,
          2004.32,
          2004.4,
          2004.48,
          2004.56,
          2004.64,
          2004.72,
          2004.8,
          2004.88,
          2004.96,
          2005.04,
          2005.12,
          2005.2,
          2005.28,
          2005.36,
          2005.44,
          2005.52,
          2005.6,
          2005.68,
          2005.76,
          2005.84,
          2005.92,
          2006,
          2006.08,
          2006.16,
          2006.24,
          2006.32,
          2006.4,
          2006.48,
          2006.56,
          2006.64,
          2006.72,
          2006.8,
          2006.88,
          2006.96,
          2007.04,
          2007.12,
          2007.2,
          2007.28,
          2007.36,
          2007.44,
          2007.52,
          2007.6,
          2007.68,
          2007.76,
          2007.84,
          2007.92,
          2008,
          2008.08,
          2008.16,
          2008.24,
          2008.32,
          2008.4,
          2008.48,
          2008.56,
          2008.64,
          2008.72,
          2008.8,
          2008.88,
          2008.96,
          2009.04,
          2009.12,
          2009.2,
          2009.28,
          2009.36,
          2009.44,
          2009.52,
          2009.6,
          2009.68,
          2009.76,
          2009.84,
          2009.92,
          2010,
          2010.08,
          2010.16,
          2010.24,
          2010.32,
          2010.4,
          2010.48,
          2010.56,
          2010.64,
          2010.72,
          2010.8,
          2010.88,
          2010.96,
          2011.04,
          2011.12,
          2011.2,
          2011.28,
          2011.36,
          2011.44,
          2011.52,
          2011.6,
          2011.68,
          2011.76,
          2011.84,
          2011.92,
          2012,
          2012.08,
          2012.16,
          2012.24,
          2012.32,
          2012.4,
          2012.48,
          2012.56,
          2012.64,
          2012.72,
          2012.8,
          2012.88,
          2012.96,
          2013.04,
          2013.12,
          2013.2,
          2013.28,
          2013.36,
          2013.44,
          2013.52,
          2013.6,
          2013.68,
          2013.76,
          2013.84,
          2013.92,
          2014,
          2014.08,
          2014.16,
          2014.24,
          2014.32,
          2014.4,
          2014.48,
          2014.56,
          2014.64,
          2014.72,
          2014.8,
          2014.88,
          2014.96,
          2015.04,
          2015.12,
          2015.2,
          2015.28,
          2015.36,
          2015.44,
          2015.52,
          2015.6,
          2015.68,
          2015.76,
          2015.84,
          2015.92,
          2016
         ],
         "y": [
          98.68916666666667,
          100.5404109513889,
          100.24633024935609,
          101.68152354409273,
          103.13133260062557,
          102.52629544936855,
          103.77967941123707,
          103.39655942807727,
          106.71903553769948,
          103.73623849442077,
          103.88579157158355,
          104.40695195930098,
          106.20536170679995,
          107.40017202600146,
          108.52518882797382,
          107.69858863973407,
          108.58979446072787,
          108.88841639549487,
          109.78583842728776,
          108.20400747194792,
          109.44384505756399,
          109.26052661709258,
          110.54615881362038,
          108.37208435695251,
          109.37723543936325,
          111.18104834715072,
          110.51952110948517,
          110.53425704563311,
          111.15785447913224,
          109.76097077451115,
          109.40973566803271,
          110.339718421211,
          109.1278205138847,
          108.93320923396827,
          108.2977655134368,
          108.07395013137568,
          106.32855583675396,
          108.8972765315102,
          110.92276587499627,
          109.19884122202237,
          110.17981081233354,
          112.676301692323,
          108.56925049563782,
          108.59820229576998,
          110.43532188460676,
          108.20544867688673,
          110.62925072724899,
          113.29264993850752,
          115.47258934440762,
          114.88367913875115,
          113.91482677801434,
          114.06101747237946,
          112.81204933105691,
          114.08494528767565,
          114.387270392688,
          115.69033204791137,
          116.07114605756908,
          115.65232267221135,
          115.20320615250093,
          114.06269441159115,
          111.4592134116466,
          111.74529205940316,
          111.14745474688534,
          111.55870032944884,
          110.88841847163606,
          109.28977710533665,
          109.61855718479521,
          110.02505933435549,
          109.4813521661449,
          109.73498396532982,
          110.52873368267903,
          109.98622181485322,
          109.69200867149848,
          110.77521725712953,
          110.62105507978002,
          112.00105274190027,
          113.2358643483797,
          112.88860769771134,
          113.08428128438737,
          115.62113866120045,
          117.04424217622207,
          117.1710401052463,
          117.36437232141996,
          117.42109843470865,
          119.61002341136233,
          119.19936233098332,
          118.25272072847144,
          118.6971538705426,
          119.49440308737309,
          120.68735554486203,
          123.19966399612092,
          125.11336544352733,
          124.75470712925589,
          124.64970525075545,
          123.91011699960097,
          125.17296760868857,
          125.60168502274833,
          124.99879693463913,
          126.71440542256704,
          127.74818378013948,
          129.6995372873811,
          131.36725717100134,
          129.6485355563474,
          131.77693234839748,
          130.00672889051734,
          132.6588661598839,
          132.28963231573886,
          132.36459644071778,
          132.13516447355386,
          132.31354694559317,
          132.55171133009523,
          132.61688258816585,
          131.17909455277248,
          132.64392777527846,
          133.47184695780916,
          133.93010029903098,
          132.7950426989967,
          131.5722216808101,
          132.30354394631925,
          135.1535827888295,
          137.0006817536102,
          138.56248952560136,
          136.59490217433782,
          135.89371500984288,
          135.87446340021648,
          136.24585360017707,
          136.54332371387082,
          136.63093901325388,
          137.7182935695677,
          138.75347607623226,
          139.35589741819655,
          137.1482677432633,
          136.6442478593068,
          136.7660889803147,
          138.46312820107875,
          138.12389353698612,
          139.44988291494118,
          139.308108867311,
          141.81913752964428,
          140.8535855682966,
          138.5846690607673,
          140.37241129165122,
          139.0061198217458,
          139.38722826692376,
          140.23168255817419,
          142.84116045111088,
          147.13115663665926,
          148.3008493319207,
          149.84070648415047,
          150.87585603144515,
          153.73118160684027,
          152.79342139903855,
          151.63855778896416,
          152.67349094587385,
          156.83766041142255,
          152.22794017583016,
          153.99885854654232,
          150.15145373052118,
          150.19900169086918,
          151.50072637219006,
          155.0407933450869,
          155.22555029048982,
          152.77686723465735,
          154.1658635859324,
          151.63882813865303,
          154.93823630756987,
          150.74328355954242,
          149.3061975896081,
          153.67962496067037,
          157.26932486704337,
          157.3387854855263,
          156.06303016654783,
          157.01501465056378,
          155.61758102017376,
          158.0439184709133,
          154.73026431363982,
          154.01334742231992,
          153.67451805799084,
          154.71438229684992,
          157.79061993151893,
          157.6144204059287,
          156.026455120339,
          160.30027977017696,
          161.55863696637283,
          159.04774648352046,
          159.2160720152155,
          156.07420819411524,
          156.18606137665435,
          156.37478620081782,
          157.98414337546788,
          160.13536079443048,
          161.25097047463169,
          162.6740102890703,
          161.74947966392745,
          162.9221633914909,
          163.27380372747754,
          164.15276103754377,
          166.4837302442769,
          170.0894903684842,
          168.7925580044245,
          172.2260799551645,
          169.04850877999172,
          171.97445671945871,
          172.20948847697534,
          171.0355937971906,
          173.94034829851287,
          172.51403744246508,
          173.7906413195393,
          174.37139171261543,
          174.7942423375185,
          174.41260824174825,
          171.7237471980213,
          173.0016580834199,
          178.2493750452836,
          175.30677494557773,
          177.44405671012257,
          179.6975962303411,
          177.73140503158746,
          178.69559790388382,
          180.69698860040734,
          181.53271217268426,
          181.04106107721657,
          179.51277278662306,
          180.84116730524408,
          182.53956726818583,
          180.93169791316524,
          181.48806288424822,
          180.96023510135984,
          179.41000908732488,
          180.50889539298473,
          179.67705023338206,
          178.15728185015806,
          180.231329539697,
          182.57734067920538,
          186.44037291240954,
          181.49348835113364,
          182.1317404518351,
          185.68179162614214,
          187.99352993188762,
          188.147057981332,
          188.9482508699025,
          190.3149765511948,
          191.17139394567516,
          189.8825801314914,
          189.9316331313587,
          190.3779724692174,
          191.50278898988972,
          196.56005847579775,
          205.49207513303546,
          210.8485685581699,
          215.3115299259845,
          218.21644131723593,
          221.57515604317703,
          224.04571903305848,
          226.9471110945366,
          223.5807289466343,
          220.9238446176518,
          221.84067857281508,
          222.11982809335257,
          223.15638729112152,
          219.41479853087372,
          219.1844129924163,
          220.37896804322497,
          218.2541474930082,
          223.21397299478684,
          228.12840063355537,
          228.0219407132597,
          225.8196288025375,
          227.5019850371164,
          231.0813496017004,
          233.07250056410172,
          238.05248299282135,
          235.72750374225814,
          237.51117518724124,
          243.78542873177088,
          242.97281063599834,
          241.92600277684156,
          245.3331273159487,
          246.8562371480352,
          246.81920871246302,
          245.45553258432668,
          243.21779631226624,
          244.4987433728442,
          241.79091978998994,
          243.84614260820484,
          246.10578352970757,
          246.6451653719435,
          242.98248466617017,
          246.09468532393603,
          247.98961440093038,
          248.92164203505388,
          251.87551218720316,
          251.74537650590642,
          248.2356263817866,
          250.62075702527164,
          248.86850023240328,
          250.9631434426927,
          246.42907598449472,
          249.0165812823319,
          247.9748619173008,
          246.97056372653577,
          247.63326807253532,
          246.10000542105288,
          245.34529873776165,
          244.57859467920616,
          243.7898287113657,
          245.74624208677437,
          244.6403839973839,
          239.73534429823636,
          244.6379320891353,
          243.05390147885814,
          238.51284441956147,
          239.71733428388026,
          239.60346855009544,
          243.27539170562568,
          243.27133711576388,
          247.01974296848928,
          241.99289119908053,
          243.90463503955323,
          242.1525867445191,
          243.51065916851132,
          242.74765910311666,
          243.23720021564122,
          238.45556225473544,
          239.49085682085806,
          241.1353607043613,
          236.0614708228737,
          238.24897378583233,
          239.66655517985802,
          239.31104978967457,
          233.99236170809905,
          237.29360394453082,
          236.76562567575422,
          240.65647412435908,
          252.37644441421537,
          259.3210029096798,
          260.47065935591274,
          260.34259461506275,
          259.8110618177236,
          257.8191770104544,
          258.1264115297252,
          258.8427123217202,
          259.63002557169875,
          260.1622671241207,
          260.9600980766347,
          256.26716564622325,
          256.3611302736268,
          262.33007192349777,
          262.49621430238267,
          264.5021228733434,
          265.2559539235324,
          266.4938150418422,
          265.1724498755931,
          257.9840667135489,
          253.28875669936232,
          265.75267426861006,
          265.3629036796828,
          261.35371247658895,
          256.8126917223082,
          255.70625704213796,
          255.25664020683888,
          263.5439724588876,
          262.5183471660684,
          266.52175196035097,
          270.60397679454366,
          273.83092921781855,
          263.76307872024347,
          257.828409449038,
          261.8268983655768,
          279.6704014891908,
          278.91529140517,
          276.6328346038377,
          281.8681109987154,
          283.1670532102345,
          289.20795034538617,
          291.27096705784993,
          299.4775265547049,
          293.4829847315016,
          296.43982580267146,
          299.1151952305406,
          311.24182377051216,
          311.0602660399793,
          319.59627817389304,
          321.0344614256756,
          319.09220293405025,
          325.34641011155765,
          327.9735823732085,
          320.29900054567537,
          320.5392247960847,
          325.38470941091884,
          322.5375932035733,
          330.907443747206,
          336.71762694700067,
          344.3919828611678,
          337.53571246904005,
          324.75717295448294,
          321.6205599256976,
          330.092581508407,
          333.1899502315609,
          329.2388727383983,
          311.86329122962934,
          312.67933350834693,
          307.3194885997914,
          309.4604810370365,
          304.1068147150958,
          295.9542178572753,
          293.73456122334574,
          288.5085338215804,
          286.51061222486595,
          284.7748354324703,
          286.2437989585762,
          294.2800936143382,
          286.7685942248322,
          287.59544367151386,
          290.2604947828699,
          293.1340736812203,
          293.8693516493707,
          295.02033994333067,
          293.32888999432225,
          293.06000517849407,
          295.57543688960953,
          304.8170955496913,
          301.02974313748643,
          303.41289527065817,
          302.0146675116192,
          301.3905038654285,
          300.94595287222705,
          298.6161296204079,
          309.82916528765423,
          305.3366423909833,
          305.4536881038998,
          303.40460294620283,
          309.0024178705603,
          306.7312500992117,
          301.30210697245565,
          308.0286765106157,
          303.84975413262174,
          305.1715005630987,
          306.62360828661144,
          305.90048761040214,
          312.3269470209507,
          310.3514790810432,
          311.5101246029458,
          307.7979622847607,
          304.30445541282864,
          307.14463033001505,
          307.1087967898099,
          310.55609303377554,
          313.3718016106151,
          317.38034923955087,
          316.19810743863354,
          317.5682992375343,
          313.23349195294196,
          310.7746090411113,
          301.4151137321565,
          312.7106451192691,
          301.78661991643594,
          300.54929477477856,
          298.7209532315653,
          303.1544700457771,
          304.5767697677419,
          316.2039879536255,
          304.91814061691394,
          304.27781252161844,
          305.89809187329604,
          313.5251509640035,
          322.57818969808915,
          314.218038281747,
          307.01720823779027,
          312.22626687089144,
          309.5255096624582,
          337.7232835927081,
          320.04909841802305,
          311.8585085743418,
          306.897359467105,
          310.6670820325593,
          311.8061946666787,
          315.1399225646566,
          318.87958297909057,
          322.3686570828535,
          328.12831042273376,
          329.2001962367814,
          333.811742319065,
          337.2500032649513,
          333.11306989156793,
          337.71280619832066,
          338.0645903714439,
          330.79901888337764,
          324.79777334913507,
          328.375962152198,
          327.45650945817187,
          331.8526130976478,
          337.9061578482374,
          337.3457968031391,
          339.5610342021463,
          339.6317760842718,
          348.0659651903645,
          350.7054654263914,
          350.6850076075749,
          357.061629995906,
          372.64439463164393,
          355.2015315929278,
          352.1379183829388,
          351.1460632461601,
          338.9700735030995,
          334.60300905613457,
          333.5824698785134,
          333.9605300110424,
          340.74271177468336,
          333.92785753918963,
          335.6448032733703,
          337.44330001091015,
          339.36110276597213,
          342.47756889303963,
          341.0334551442073,
          337.99825739342384,
          344.8370888013508,
          343.4778559429921,
          348.286545926194,
          361.3530961741917,
          359.0765716682943,
          362.19754553704456,
          363.9119472525866,
          359.202320135226,
          359.40586811663593,
          359.67242746882243,
          355.3503637987387,
          352.8747562642742,
          353.16587793819224,
          347.7153512220128,
          344.2613787332075,
          342.442531115567,
          343.6153967846379,
          343.58676216823915,
          343.3090295354865,
          343.4063004271882,
          343.4435027764012,
          344.25631906630537,
          351.4914393720156,
          359.927233916944,
          366.11798234031545,
          366.6122416164749,
          374.39358644478455,
          367.98209627691756,
          359.8650245365426,
          366.696462252328,
          376.6950524564082,
          369.3055511773883,
          373.2355777511677,
          357.4943672595122,
          358.2361680715757,
          361.3946169534068,
          371.25165513081095,
          361.79711298014627,
          357.8565394246042,
          353.0612617963145,
          353.41432305811077,
          358.7096476652648,
          355.73235758964313,
          361.6019414898722,
          363.72635289612526,
          362.12898799632313,
          360.52958496600604,
          356.506675680427,
          355.3391163175736,
          345.67093119443297,
          344.3516204737075,
          343.7490051378785,
          341.98729148654684,
          344.1047627996677,
          344.3972518480474,
          342.9909630696679,
          345.6262769692531,
          345.64355828310164,
          341.51023739863285,
          338.2573523874109
         ]
        }
       ],
       "layout": {
        "yaxis": {
         "autorange": true,
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div id=\"3dfa9865-2eba-4374-b46f-9771eb52a5b7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3dfa9865-2eba-4374-b46f-9771eb52a5b7\", [{\"y\": [98.68916666666667, 100.5404109513889, 100.24633024935609, 101.68152354409273, 103.13133260062557, 102.52629544936855, 103.77967941123707, 103.39655942807727, 106.71903553769948, 103.73623849442077, 103.88579157158355, 104.40695195930098, 106.20536170679995, 107.40017202600146, 108.52518882797382, 107.69858863973407, 108.58979446072787, 108.88841639549487, 109.78583842728776, 108.20400747194792, 109.44384505756399, 109.26052661709258, 110.54615881362038, 108.37208435695251, 109.37723543936325, 111.18104834715072, 110.51952110948517, 110.53425704563311, 111.15785447913224, 109.76097077451115, 109.40973566803271, 110.339718421211, 109.1278205138847, 108.93320923396827, 108.2977655134368, 108.07395013137568, 106.32855583675396, 108.8972765315102, 110.92276587499627, 109.19884122202237, 110.17981081233354, 112.676301692323, 108.56925049563782, 108.59820229576998, 110.43532188460676, 108.20544867688673, 110.62925072724899, 113.29264993850752, 115.47258934440762, 114.88367913875115, 113.91482677801434, 114.06101747237946, 112.81204933105691, 114.08494528767565, 114.387270392688, 115.69033204791137, 116.07114605756908, 115.65232267221135, 115.20320615250093, 114.06269441159115, 111.4592134116466, 111.74529205940316, 111.14745474688534, 111.55870032944884, 110.88841847163606, 109.28977710533665, 109.61855718479521, 110.02505933435549, 109.4813521661449, 109.73498396532982, 110.52873368267903, 109.98622181485322, 109.69200867149848, 110.77521725712953, 110.62105507978002, 112.00105274190027, 113.2358643483797, 112.88860769771134, 113.08428128438737, 115.62113866120045, 117.04424217622207, 117.1710401052463, 117.36437232141996, 117.42109843470865, 119.61002341136233, 119.19936233098332, 118.25272072847144, 118.6971538705426, 119.49440308737309, 120.68735554486203, 123.19966399612092, 125.11336544352733, 124.75470712925589, 124.64970525075545, 123.91011699960097, 125.17296760868857, 125.60168502274833, 124.99879693463913, 126.71440542256704, 127.74818378013948, 129.6995372873811, 131.36725717100134, 129.6485355563474, 131.77693234839748, 130.00672889051734, 132.6588661598839, 132.28963231573886, 132.36459644071778, 132.13516447355386, 132.31354694559317, 132.55171133009523, 132.61688258816585, 131.17909455277248, 132.64392777527846, 133.47184695780916, 133.93010029903098, 132.7950426989967, 131.5722216808101, 132.30354394631925, 135.1535827888295, 137.0006817536102, 138.56248952560136, 136.59490217433782, 135.89371500984288, 135.87446340021648, 136.24585360017707, 136.54332371387082, 136.63093901325388, 137.7182935695677, 138.75347607623226, 139.35589741819655, 137.1482677432633, 136.6442478593068, 136.7660889803147, 138.46312820107875, 138.12389353698612, 139.44988291494118, 139.308108867311, 141.81913752964428, 140.8535855682966, 138.5846690607673, 140.37241129165122, 139.0061198217458, 139.38722826692376, 140.23168255817419, 142.84116045111088, 147.13115663665926, 148.3008493319207, 149.84070648415047, 150.87585603144515, 153.73118160684027, 152.79342139903855, 151.63855778896416, 152.67349094587385, 156.83766041142255, 152.22794017583016, 153.99885854654232, 150.15145373052118, 150.19900169086918, 151.50072637219006, 155.0407933450869, 155.22555029048982, 152.77686723465735, 154.1658635859324, 151.63882813865303, 154.93823630756987, 150.74328355954242, 149.3061975896081, 153.67962496067037, 157.26932486704337, 157.3387854855263, 156.06303016654783, 157.01501465056378, 155.61758102017376, 158.0439184709133, 154.73026431363982, 154.01334742231992, 153.67451805799084, 154.71438229684992, 157.79061993151893, 157.6144204059287, 156.026455120339, 160.30027977017696, 161.55863696637283, 159.04774648352046, 159.2160720152155, 156.07420819411524, 156.18606137665435, 156.37478620081782, 157.98414337546788, 160.13536079443048, 161.25097047463169, 162.6740102890703, 161.74947966392745, 162.9221633914909, 163.27380372747754, 164.15276103754377, 166.4837302442769, 170.0894903684842, 168.7925580044245, 172.2260799551645, 169.04850877999172, 171.97445671945871, 172.20948847697534, 171.0355937971906, 173.94034829851287, 172.51403744246508, 173.7906413195393, 174.37139171261543, 174.7942423375185, 174.41260824174825, 171.7237471980213, 173.0016580834199, 178.2493750452836, 175.30677494557773, 177.44405671012257, 179.6975962303411, 177.73140503158746, 178.69559790388382, 180.69698860040734, 181.53271217268426, 181.04106107721657, 179.51277278662306, 180.84116730524408, 182.53956726818583, 180.93169791316524, 181.48806288424822, 180.96023510135984, 179.41000908732488, 180.50889539298473, 179.67705023338206, 178.15728185015806, 180.231329539697, 182.57734067920538, 186.44037291240954, 181.49348835113364, 182.1317404518351, 185.68179162614214, 187.99352993188762, 188.147057981332, 188.9482508699025, 190.3149765511948, 191.17139394567516, 189.8825801314914, 189.9316331313587, 190.3779724692174, 191.50278898988972, 196.56005847579775, 205.49207513303546, 210.8485685581699, 215.3115299259845, 218.21644131723593, 221.57515604317703, 224.04571903305848, 226.9471110945366, 223.5807289466343, 220.9238446176518, 221.84067857281508, 222.11982809335257, 223.15638729112152, 219.41479853087372, 219.1844129924163, 220.37896804322497, 218.2541474930082, 223.21397299478684, 228.12840063355537, 228.0219407132597, 225.8196288025375, 227.5019850371164, 231.0813496017004, 233.07250056410172, 238.05248299282135, 235.72750374225814, 237.51117518724124, 243.78542873177088, 242.97281063599834, 241.92600277684156, 245.3331273159487, 246.8562371480352, 246.81920871246302, 245.45553258432668, 243.21779631226624, 244.4987433728442, 241.79091978998994, 243.84614260820484, 246.10578352970757, 246.6451653719435, 242.98248466617017, 246.09468532393603, 247.98961440093038, 248.92164203505388, 251.87551218720316, 251.74537650590642, 248.2356263817866, 250.62075702527164, 248.86850023240328, 250.9631434426927, 246.42907598449472, 249.0165812823319, 247.9748619173008, 246.97056372653577, 247.63326807253532, 246.10000542105288, 245.34529873776165, 244.57859467920616, 243.7898287113657, 245.74624208677437, 244.6403839973839, 239.73534429823636, 244.6379320891353, 243.05390147885814, 238.51284441956147, 239.71733428388026, 239.60346855009544, 243.27539170562568, 243.27133711576388, 247.01974296848928, 241.99289119908053, 243.90463503955323, 242.1525867445191, 243.51065916851132, 242.74765910311666, 243.23720021564122, 238.45556225473544, 239.49085682085806, 241.1353607043613, 236.0614708228737, 238.24897378583233, 239.66655517985802, 239.31104978967457, 233.99236170809905, 237.29360394453082, 236.76562567575422, 240.65647412435908, 252.37644441421537, 259.3210029096798, 260.47065935591274, 260.34259461506275, 259.8110618177236, 257.8191770104544, 258.1264115297252, 258.8427123217202, 259.63002557169875, 260.1622671241207, 260.9600980766347, 256.26716564622325, 256.3611302736268, 262.33007192349777, 262.49621430238267, 264.5021228733434, 265.2559539235324, 266.4938150418422, 265.1724498755931, 257.9840667135489, 253.28875669936232, 265.75267426861006, 265.3629036796828, 261.35371247658895, 256.8126917223082, 255.70625704213796, 255.25664020683888, 263.5439724588876, 262.5183471660684, 266.52175196035097, 270.60397679454366, 273.83092921781855, 263.76307872024347, 257.828409449038, 261.8268983655768, 279.6704014891908, 278.91529140517, 276.6328346038377, 281.8681109987154, 283.1670532102345, 289.20795034538617, 291.27096705784993, 299.4775265547049, 293.4829847315016, 296.43982580267146, 299.1151952305406, 311.24182377051216, 311.0602660399793, 319.59627817389304, 321.0344614256756, 319.09220293405025, 325.34641011155765, 327.9735823732085, 320.29900054567537, 320.5392247960847, 325.38470941091884, 322.5375932035733, 330.907443747206, 336.71762694700067, 344.3919828611678, 337.53571246904005, 324.75717295448294, 321.6205599256976, 330.092581508407, 333.1899502315609, 329.2388727383983, 311.86329122962934, 312.67933350834693, 307.3194885997914, 309.4604810370365, 304.1068147150958, 295.9542178572753, 293.73456122334574, 288.5085338215804, 286.51061222486595, 284.7748354324703, 286.2437989585762, 294.2800936143382, 286.7685942248322, 287.59544367151386, 290.2604947828699, 293.1340736812203, 293.8693516493707, 295.02033994333067, 293.32888999432225, 293.06000517849407, 295.57543688960953, 304.8170955496913, 301.02974313748643, 303.41289527065817, 302.0146675116192, 301.3905038654285, 300.94595287222705, 298.6161296204079, 309.82916528765423, 305.3366423909833, 305.4536881038998, 303.40460294620283, 309.0024178705603, 306.7312500992117, 301.30210697245565, 308.0286765106157, 303.84975413262174, 305.1715005630987, 306.62360828661144, 305.90048761040214, 312.3269470209507, 310.3514790810432, 311.5101246029458, 307.7979622847607, 304.30445541282864, 307.14463033001505, 307.1087967898099, 310.55609303377554, 313.3718016106151, 317.38034923955087, 316.19810743863354, 317.5682992375343, 313.23349195294196, 310.7746090411113, 301.4151137321565, 312.7106451192691, 301.78661991643594, 300.54929477477856, 298.7209532315653, 303.1544700457771, 304.5767697677419, 316.2039879536255, 304.91814061691394, 304.27781252161844, 305.89809187329604, 313.5251509640035, 322.57818969808915, 314.218038281747, 307.01720823779027, 312.22626687089144, 309.5255096624582, 337.7232835927081, 320.04909841802305, 311.8585085743418, 306.897359467105, 310.6670820325593, 311.8061946666787, 315.1399225646566, 318.87958297909057, 322.3686570828535, 328.12831042273376, 329.2001962367814, 333.811742319065, 337.2500032649513, 333.11306989156793, 337.71280619832066, 338.0645903714439, 330.79901888337764, 324.79777334913507, 328.375962152198, 327.45650945817187, 331.8526130976478, 337.9061578482374, 337.3457968031391, 339.5610342021463, 339.6317760842718, 348.0659651903645, 350.7054654263914, 350.6850076075749, 357.061629995906, 372.64439463164393, 355.2015315929278, 352.1379183829388, 351.1460632461601, 338.9700735030995, 334.60300905613457, 333.5824698785134, 333.9605300110424, 340.74271177468336, 333.92785753918963, 335.6448032733703, 337.44330001091015, 339.36110276597213, 342.47756889303963, 341.0334551442073, 337.99825739342384, 344.8370888013508, 343.4778559429921, 348.286545926194, 361.3530961741917, 359.0765716682943, 362.19754553704456, 363.9119472525866, 359.202320135226, 359.40586811663593, 359.67242746882243, 355.3503637987387, 352.8747562642742, 353.16587793819224, 347.7153512220128, 344.2613787332075, 342.442531115567, 343.6153967846379, 343.58676216823915, 343.3090295354865, 343.4063004271882, 343.4435027764012, 344.25631906630537, 351.4914393720156, 359.927233916944, 366.11798234031545, 366.6122416164749, 374.39358644478455, 367.98209627691756, 359.8650245365426, 366.696462252328, 376.6950524564082, 369.3055511773883, 373.2355777511677, 357.4943672595122, 358.2361680715757, 361.3946169534068, 371.25165513081095, 361.79711298014627, 357.8565394246042, 353.0612617963145, 353.41432305811077, 358.7096476652648, 355.73235758964313, 361.6019414898722, 363.72635289612526, 362.12898799632313, 360.52958496600604, 356.506675680427, 355.3391163175736, 345.67093119443297, 344.3516204737075, 343.7490051378785, 341.98729148654684, 344.1047627996677, 344.3972518480474, 342.9909630696679, 345.6262769692531, 345.64355828310164, 341.51023739863285, 338.2573523874109], \"x\": [1970.0, 1970.08, 1970.16, 1970.24, 1970.32, 1970.4, 1970.48, 1970.56, 1970.64, 1970.72, 1970.8, 1970.88, 1970.96, 1971.04, 1971.12, 1971.2, 1971.28, 1971.36, 1971.44, 1971.52, 1971.6, 1971.68, 1971.76, 1971.84, 1971.92, 1972.0, 1972.08, 1972.16, 1972.24, 1972.32, 1972.4, 1972.48, 1972.56, 1972.64, 1972.72, 1972.8, 1972.88, 1972.96, 1973.04, 1973.12, 1973.2, 1973.28, 1973.36, 1973.44, 1973.52, 1973.6, 1973.68, 1973.76, 1973.84, 1973.92, 1974.0, 1974.08, 1974.16, 1974.24, 1974.32, 1974.4, 1974.48, 1974.56, 1974.64, 1974.72, 1974.8, 1974.88, 1974.96, 1975.04, 1975.12, 1975.2, 1975.28, 1975.36, 1975.44, 1975.52, 1975.6, 1975.68, 1975.76, 1975.84, 1975.92, 1976.0, 1976.08, 1976.16, 1976.24, 1976.32, 1976.4, 1976.48, 1976.56, 1976.64, 1976.72, 1976.8, 1976.88, 1976.96, 1977.04, 1977.12, 1977.2, 1977.28, 1977.36, 1977.44, 1977.52, 1977.6, 1977.68, 1977.76, 1977.84, 1977.92, 1978.0, 1978.08, 1978.16, 1978.24, 1978.32, 1978.4, 1978.48, 1978.56, 1978.64, 1978.72, 1978.8, 1978.88, 1978.96, 1979.04, 1979.12, 1979.2, 1979.28, 1979.36, 1979.44, 1979.52, 1979.6, 1979.68, 1979.76, 1979.84, 1979.92, 1980.0, 1980.08, 1980.16, 1980.24, 1980.32, 1980.4, 1980.48, 1980.56, 1980.64, 1980.72, 1980.8, 1980.88, 1980.96, 1981.04, 1981.12, 1981.2, 1981.28, 1981.36, 1981.44, 1981.52, 1981.6, 1981.68, 1981.76, 1981.84, 1981.92, 1982.0, 1982.08, 1982.16, 1982.24, 1982.32, 1982.4, 1982.48, 1982.56, 1982.64, 1982.72, 1982.8, 1982.88, 1982.96, 1983.04, 1983.12, 1983.2, 1983.28, 1983.36, 1983.44, 1983.52, 1983.6, 1983.68, 1983.76, 1983.84, 1983.92, 1984.0, 1984.08, 1984.16, 1984.24, 1984.32, 1984.4, 1984.48, 1984.56, 1984.64, 1984.72, 1984.8, 1984.88, 1984.96, 1985.04, 1985.12, 1985.2, 1985.28, 1985.36, 1985.44, 1985.52, 1985.6, 1985.68, 1985.76, 1985.84, 1985.92, 1986.0, 1986.08, 1986.16, 1986.24, 1986.32, 1986.4, 1986.48, 1986.56, 1986.64, 1986.72, 1986.8, 1986.88, 1986.96, 1987.04, 1987.12, 1987.2, 1987.28, 1987.36, 1987.44, 1987.52, 1987.6, 1987.68, 1987.76, 1987.84, 1987.92, 1988.0, 1988.08, 1988.16, 1988.24, 1988.32, 1988.4, 1988.48, 1988.56, 1988.64, 1988.72, 1988.8, 1988.88, 1988.96, 1989.04, 1989.12, 1989.2, 1989.28, 1989.36, 1989.44, 1989.52, 1989.6, 1989.68, 1989.76, 1989.84, 1989.92, 1990.0, 1990.08, 1990.16, 1990.24, 1990.32, 1990.4, 1990.48, 1990.56, 1990.64, 1990.72, 1990.8, 1990.88, 1990.96, 1991.04, 1991.12, 1991.2, 1991.28, 1991.36, 1991.44, 1991.52, 1991.6, 1991.68, 1991.76, 1991.84, 1991.92, 1992.0, 1992.08, 1992.16, 1992.24, 1992.32, 1992.4, 1992.48, 1992.56, 1992.64, 1992.72, 1992.8, 1992.88, 1992.96, 1993.04, 1993.12, 1993.2, 1993.28, 1993.36, 1993.44, 1993.52, 1993.6, 1993.68, 1993.76, 1993.84, 1993.92, 1994.0, 1994.08, 1994.16, 1994.24, 1994.32, 1994.4, 1994.48, 1994.56, 1994.64, 1994.72, 1994.8, 1994.88, 1994.96, 1995.04, 1995.12, 1995.2, 1995.28, 1995.36, 1995.44, 1995.52, 1995.6, 1995.68, 1995.76, 1995.84, 1995.92, 1996.0, 1996.08, 1996.16, 1996.24, 1996.32, 1996.4, 1996.48, 1996.56, 1996.64, 1996.72, 1996.8, 1996.88, 1996.96, 1997.04, 1997.12, 1997.2, 1997.28, 1997.36, 1997.44, 1997.52, 1997.6, 1997.68, 1997.76, 1997.84, 1997.92, 1998.0, 1998.08, 1998.16, 1998.24, 1998.32, 1998.4, 1998.48, 1998.56, 1998.64, 1998.72, 1998.8, 1998.88, 1998.96, 1999.04, 1999.12, 1999.2, 1999.28, 1999.36, 1999.44, 1999.52, 1999.6, 1999.68, 1999.76, 1999.84, 1999.92, 2000.0, 2000.08, 2000.16, 2000.24, 2000.32, 2000.4, 2000.48, 2000.56, 2000.64, 2000.72, 2000.8, 2000.88, 2000.96, 2001.04, 2001.12, 2001.2, 2001.28, 2001.36, 2001.44, 2001.52, 2001.6, 2001.68, 2001.76, 2001.84, 2001.92, 2002.0, 2002.08, 2002.16, 2002.24, 2002.32, 2002.4, 2002.48, 2002.56, 2002.64, 2002.72, 2002.8, 2002.88, 2002.96, 2003.04, 2003.12, 2003.2, 2003.28, 2003.36, 2003.44, 2003.52, 2003.6, 2003.68, 2003.76, 2003.84, 2003.92, 2004.0, 2004.08, 2004.16, 2004.24, 2004.32, 2004.4, 2004.48, 2004.56, 2004.64, 2004.72, 2004.8, 2004.88, 2004.96, 2005.04, 2005.12, 2005.2, 2005.28, 2005.36, 2005.44, 2005.52, 2005.6, 2005.68, 2005.76, 2005.84, 2005.92, 2006.0, 2006.08, 2006.16, 2006.24, 2006.32, 2006.4, 2006.48, 2006.56, 2006.64, 2006.72, 2006.8, 2006.88, 2006.96, 2007.04, 2007.12, 2007.2, 2007.28, 2007.36, 2007.44, 2007.52, 2007.6, 2007.68, 2007.76, 2007.84, 2007.92, 2008.0, 2008.08, 2008.16, 2008.24, 2008.32, 2008.4, 2008.48, 2008.56, 2008.64, 2008.72, 2008.8, 2008.88, 2008.96, 2009.04, 2009.12, 2009.2, 2009.28, 2009.36, 2009.44, 2009.52, 2009.6, 2009.68, 2009.76, 2009.84, 2009.92, 2010.0, 2010.08, 2010.16, 2010.24, 2010.32, 2010.4, 2010.48, 2010.56, 2010.64, 2010.72, 2010.8, 2010.88, 2010.96, 2011.04, 2011.12, 2011.2, 2011.28, 2011.36, 2011.44, 2011.52, 2011.6, 2011.68, 2011.76, 2011.84, 2011.92, 2012.0, 2012.08, 2012.16, 2012.24, 2012.32, 2012.4, 2012.48, 2012.56, 2012.64, 2012.72, 2012.8, 2012.88, 2012.96, 2013.04, 2013.12, 2013.2, 2013.28, 2013.36, 2013.44, 2013.52, 2013.6, 2013.68, 2013.76, 2013.84, 2013.92, 2014.0, 2014.08, 2014.16, 2014.24, 2014.32, 2014.4, 2014.48, 2014.56, 2014.64, 2014.72, 2014.8, 2014.88, 2014.96, 2015.04, 2015.12, 2015.2, 2015.28, 2015.36, 2015.44, 2015.52, 2015.6, 2015.68, 2015.76, 2015.84, 2015.92, 2016.0], \"type\": \"scatter\", \"name\": \"Growth of $1\"}], {\"yaxis\": {\"type\": \"log\", \"autorange\": true}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"3dfa9865-2eba-4374-b46f-9771eb52a5b7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3dfa9865-2eba-4374-b46f-9771eb52a5b7\", [{\"y\": [98.68916666666667, 100.5404109513889, 100.24633024935609, 101.68152354409273, 103.13133260062557, 102.52629544936855, 103.77967941123707, 103.39655942807727, 106.71903553769948, 103.73623849442077, 103.88579157158355, 104.40695195930098, 106.20536170679995, 107.40017202600146, 108.52518882797382, 107.69858863973407, 108.58979446072787, 108.88841639549487, 109.78583842728776, 108.20400747194792, 109.44384505756399, 109.26052661709258, 110.54615881362038, 108.37208435695251, 109.37723543936325, 111.18104834715072, 110.51952110948517, 110.53425704563311, 111.15785447913224, 109.76097077451115, 109.40973566803271, 110.339718421211, 109.1278205138847, 108.93320923396827, 108.2977655134368, 108.07395013137568, 106.32855583675396, 108.8972765315102, 110.92276587499627, 109.19884122202237, 110.17981081233354, 112.676301692323, 108.56925049563782, 108.59820229576998, 110.43532188460676, 108.20544867688673, 110.62925072724899, 113.29264993850752, 115.47258934440762, 114.88367913875115, 113.91482677801434, 114.06101747237946, 112.81204933105691, 114.08494528767565, 114.387270392688, 115.69033204791137, 116.07114605756908, 115.65232267221135, 115.20320615250093, 114.06269441159115, 111.4592134116466, 111.74529205940316, 111.14745474688534, 111.55870032944884, 110.88841847163606, 109.28977710533665, 109.61855718479521, 110.02505933435549, 109.4813521661449, 109.73498396532982, 110.52873368267903, 109.98622181485322, 109.69200867149848, 110.77521725712953, 110.62105507978002, 112.00105274190027, 113.2358643483797, 112.88860769771134, 113.08428128438737, 115.62113866120045, 117.04424217622207, 117.1710401052463, 117.36437232141996, 117.42109843470865, 119.61002341136233, 119.19936233098332, 118.25272072847144, 118.6971538705426, 119.49440308737309, 120.68735554486203, 123.19966399612092, 125.11336544352733, 124.75470712925589, 124.64970525075545, 123.91011699960097, 125.17296760868857, 125.60168502274833, 124.99879693463913, 126.71440542256704, 127.74818378013948, 129.6995372873811, 131.36725717100134, 129.6485355563474, 131.77693234839748, 130.00672889051734, 132.6588661598839, 132.28963231573886, 132.36459644071778, 132.13516447355386, 132.31354694559317, 132.55171133009523, 132.61688258816585, 131.17909455277248, 132.64392777527846, 133.47184695780916, 133.93010029903098, 132.7950426989967, 131.5722216808101, 132.30354394631925, 135.1535827888295, 137.0006817536102, 138.56248952560136, 136.59490217433782, 135.89371500984288, 135.87446340021648, 136.24585360017707, 136.54332371387082, 136.63093901325388, 137.7182935695677, 138.75347607623226, 139.35589741819655, 137.1482677432633, 136.6442478593068, 136.7660889803147, 138.46312820107875, 138.12389353698612, 139.44988291494118, 139.308108867311, 141.81913752964428, 140.8535855682966, 138.5846690607673, 140.37241129165122, 139.0061198217458, 139.38722826692376, 140.23168255817419, 142.84116045111088, 147.13115663665926, 148.3008493319207, 149.84070648415047, 150.87585603144515, 153.73118160684027, 152.79342139903855, 151.63855778896416, 152.67349094587385, 156.83766041142255, 152.22794017583016, 153.99885854654232, 150.15145373052118, 150.19900169086918, 151.50072637219006, 155.0407933450869, 155.22555029048982, 152.77686723465735, 154.1658635859324, 151.63882813865303, 154.93823630756987, 150.74328355954242, 149.3061975896081, 153.67962496067037, 157.26932486704337, 157.3387854855263, 156.06303016654783, 157.01501465056378, 155.61758102017376, 158.0439184709133, 154.73026431363982, 154.01334742231992, 153.67451805799084, 154.71438229684992, 157.79061993151893, 157.6144204059287, 156.026455120339, 160.30027977017696, 161.55863696637283, 159.04774648352046, 159.2160720152155, 156.07420819411524, 156.18606137665435, 156.37478620081782, 157.98414337546788, 160.13536079443048, 161.25097047463169, 162.6740102890703, 161.74947966392745, 162.9221633914909, 163.27380372747754, 164.15276103754377, 166.4837302442769, 170.0894903684842, 168.7925580044245, 172.2260799551645, 169.04850877999172, 171.97445671945871, 172.20948847697534, 171.0355937971906, 173.94034829851287, 172.51403744246508, 173.7906413195393, 174.37139171261543, 174.7942423375185, 174.41260824174825, 171.7237471980213, 173.0016580834199, 178.2493750452836, 175.30677494557773, 177.44405671012257, 179.6975962303411, 177.73140503158746, 178.69559790388382, 180.69698860040734, 181.53271217268426, 181.04106107721657, 179.51277278662306, 180.84116730524408, 182.53956726818583, 180.93169791316524, 181.48806288424822, 180.96023510135984, 179.41000908732488, 180.50889539298473, 179.67705023338206, 178.15728185015806, 180.231329539697, 182.57734067920538, 186.44037291240954, 181.49348835113364, 182.1317404518351, 185.68179162614214, 187.99352993188762, 188.147057981332, 188.9482508699025, 190.3149765511948, 191.17139394567516, 189.8825801314914, 189.9316331313587, 190.3779724692174, 191.50278898988972, 196.56005847579775, 205.49207513303546, 210.8485685581699, 215.3115299259845, 218.21644131723593, 221.57515604317703, 224.04571903305848, 226.9471110945366, 223.5807289466343, 220.9238446176518, 221.84067857281508, 222.11982809335257, 223.15638729112152, 219.41479853087372, 219.1844129924163, 220.37896804322497, 218.2541474930082, 223.21397299478684, 228.12840063355537, 228.0219407132597, 225.8196288025375, 227.5019850371164, 231.0813496017004, 233.07250056410172, 238.05248299282135, 235.72750374225814, 237.51117518724124, 243.78542873177088, 242.97281063599834, 241.92600277684156, 245.3331273159487, 246.8562371480352, 246.81920871246302, 245.45553258432668, 243.21779631226624, 244.4987433728442, 241.79091978998994, 243.84614260820484, 246.10578352970757, 246.6451653719435, 242.98248466617017, 246.09468532393603, 247.98961440093038, 248.92164203505388, 251.87551218720316, 251.74537650590642, 248.2356263817866, 250.62075702527164, 248.86850023240328, 250.9631434426927, 246.42907598449472, 249.0165812823319, 247.9748619173008, 246.97056372653577, 247.63326807253532, 246.10000542105288, 245.34529873776165, 244.57859467920616, 243.7898287113657, 245.74624208677437, 244.6403839973839, 239.73534429823636, 244.6379320891353, 243.05390147885814, 238.51284441956147, 239.71733428388026, 239.60346855009544, 243.27539170562568, 243.27133711576388, 247.01974296848928, 241.99289119908053, 243.90463503955323, 242.1525867445191, 243.51065916851132, 242.74765910311666, 243.23720021564122, 238.45556225473544, 239.49085682085806, 241.1353607043613, 236.0614708228737, 238.24897378583233, 239.66655517985802, 239.31104978967457, 233.99236170809905, 237.29360394453082, 236.76562567575422, 240.65647412435908, 252.37644441421537, 259.3210029096798, 260.47065935591274, 260.34259461506275, 259.8110618177236, 257.8191770104544, 258.1264115297252, 258.8427123217202, 259.63002557169875, 260.1622671241207, 260.9600980766347, 256.26716564622325, 256.3611302736268, 262.33007192349777, 262.49621430238267, 264.5021228733434, 265.2559539235324, 266.4938150418422, 265.1724498755931, 257.9840667135489, 253.28875669936232, 265.75267426861006, 265.3629036796828, 261.35371247658895, 256.8126917223082, 255.70625704213796, 255.25664020683888, 263.5439724588876, 262.5183471660684, 266.52175196035097, 270.60397679454366, 273.83092921781855, 263.76307872024347, 257.828409449038, 261.8268983655768, 279.6704014891908, 278.91529140517, 276.6328346038377, 281.8681109987154, 283.1670532102345, 289.20795034538617, 291.27096705784993, 299.4775265547049, 293.4829847315016, 296.43982580267146, 299.1151952305406, 311.24182377051216, 311.0602660399793, 319.59627817389304, 321.0344614256756, 319.09220293405025, 325.34641011155765, 327.9735823732085, 320.29900054567537, 320.5392247960847, 325.38470941091884, 322.5375932035733, 330.907443747206, 336.71762694700067, 344.3919828611678, 337.53571246904005, 324.75717295448294, 321.6205599256976, 330.092581508407, 333.1899502315609, 329.2388727383983, 311.86329122962934, 312.67933350834693, 307.3194885997914, 309.4604810370365, 304.1068147150958, 295.9542178572753, 293.73456122334574, 288.5085338215804, 286.51061222486595, 284.7748354324703, 286.2437989585762, 294.2800936143382, 286.7685942248322, 287.59544367151386, 290.2604947828699, 293.1340736812203, 293.8693516493707, 295.02033994333067, 293.32888999432225, 293.06000517849407, 295.57543688960953, 304.8170955496913, 301.02974313748643, 303.41289527065817, 302.0146675116192, 301.3905038654285, 300.94595287222705, 298.6161296204079, 309.82916528765423, 305.3366423909833, 305.4536881038998, 303.40460294620283, 309.0024178705603, 306.7312500992117, 301.30210697245565, 308.0286765106157, 303.84975413262174, 305.1715005630987, 306.62360828661144, 305.90048761040214, 312.3269470209507, 310.3514790810432, 311.5101246029458, 307.7979622847607, 304.30445541282864, 307.14463033001505, 307.1087967898099, 310.55609303377554, 313.3718016106151, 317.38034923955087, 316.19810743863354, 317.5682992375343, 313.23349195294196, 310.7746090411113, 301.4151137321565, 312.7106451192691, 301.78661991643594, 300.54929477477856, 298.7209532315653, 303.1544700457771, 304.5767697677419, 316.2039879536255, 304.91814061691394, 304.27781252161844, 305.89809187329604, 313.5251509640035, 322.57818969808915, 314.218038281747, 307.01720823779027, 312.22626687089144, 309.5255096624582, 337.7232835927081, 320.04909841802305, 311.8585085743418, 306.897359467105, 310.6670820325593, 311.8061946666787, 315.1399225646566, 318.87958297909057, 322.3686570828535, 328.12831042273376, 329.2001962367814, 333.811742319065, 337.2500032649513, 333.11306989156793, 337.71280619832066, 338.0645903714439, 330.79901888337764, 324.79777334913507, 328.375962152198, 327.45650945817187, 331.8526130976478, 337.9061578482374, 337.3457968031391, 339.5610342021463, 339.6317760842718, 348.0659651903645, 350.7054654263914, 350.6850076075749, 357.061629995906, 372.64439463164393, 355.2015315929278, 352.1379183829388, 351.1460632461601, 338.9700735030995, 334.60300905613457, 333.5824698785134, 333.9605300110424, 340.74271177468336, 333.92785753918963, 335.6448032733703, 337.44330001091015, 339.36110276597213, 342.47756889303963, 341.0334551442073, 337.99825739342384, 344.8370888013508, 343.4778559429921, 348.286545926194, 361.3530961741917, 359.0765716682943, 362.19754553704456, 363.9119472525866, 359.202320135226, 359.40586811663593, 359.67242746882243, 355.3503637987387, 352.8747562642742, 353.16587793819224, 347.7153512220128, 344.2613787332075, 342.442531115567, 343.6153967846379, 343.58676216823915, 343.3090295354865, 343.4063004271882, 343.4435027764012, 344.25631906630537, 351.4914393720156, 359.927233916944, 366.11798234031545, 366.6122416164749, 374.39358644478455, 367.98209627691756, 359.8650245365426, 366.696462252328, 376.6950524564082, 369.3055511773883, 373.2355777511677, 357.4943672595122, 358.2361680715757, 361.3946169534068, 371.25165513081095, 361.79711298014627, 357.8565394246042, 353.0612617963145, 353.41432305811077, 358.7096476652648, 355.73235758964313, 361.6019414898722, 363.72635289612526, 362.12898799632313, 360.52958496600604, 356.506675680427, 355.3391163175736, 345.67093119443297, 344.3516204737075, 343.7490051378785, 341.98729148654684, 344.1047627996677, 344.3972518480474, 342.9909630696679, 345.6262769692531, 345.64355828310164, 341.51023739863285, 338.2573523874109], \"x\": [1970.0, 1970.08, 1970.16, 1970.24, 1970.32, 1970.4, 1970.48, 1970.56, 1970.64, 1970.72, 1970.8, 1970.88, 1970.96, 1971.04, 1971.12, 1971.2, 1971.28, 1971.36, 1971.44, 1971.52, 1971.6, 1971.68, 1971.76, 1971.84, 1971.92, 1972.0, 1972.08, 1972.16, 1972.24, 1972.32, 1972.4, 1972.48, 1972.56, 1972.64, 1972.72, 1972.8, 1972.88, 1972.96, 1973.04, 1973.12, 1973.2, 1973.28, 1973.36, 1973.44, 1973.52, 1973.6, 1973.68, 1973.76, 1973.84, 1973.92, 1974.0, 1974.08, 1974.16, 1974.24, 1974.32, 1974.4, 1974.48, 1974.56, 1974.64, 1974.72, 1974.8, 1974.88, 1974.96, 1975.04, 1975.12, 1975.2, 1975.28, 1975.36, 1975.44, 1975.52, 1975.6, 1975.68, 1975.76, 1975.84, 1975.92, 1976.0, 1976.08, 1976.16, 1976.24, 1976.32, 1976.4, 1976.48, 1976.56, 1976.64, 1976.72, 1976.8, 1976.88, 1976.96, 1977.04, 1977.12, 1977.2, 1977.28, 1977.36, 1977.44, 1977.52, 1977.6, 1977.68, 1977.76, 1977.84, 1977.92, 1978.0, 1978.08, 1978.16, 1978.24, 1978.32, 1978.4, 1978.48, 1978.56, 1978.64, 1978.72, 1978.8, 1978.88, 1978.96, 1979.04, 1979.12, 1979.2, 1979.28, 1979.36, 1979.44, 1979.52, 1979.6, 1979.68, 1979.76, 1979.84, 1979.92, 1980.0, 1980.08, 1980.16, 1980.24, 1980.32, 1980.4, 1980.48, 1980.56, 1980.64, 1980.72, 1980.8, 1980.88, 1980.96, 1981.04, 1981.12, 1981.2, 1981.28, 1981.36, 1981.44, 1981.52, 1981.6, 1981.68, 1981.76, 1981.84, 1981.92, 1982.0, 1982.08, 1982.16, 1982.24, 1982.32, 1982.4, 1982.48, 1982.56, 1982.64, 1982.72, 1982.8, 1982.88, 1982.96, 1983.04, 1983.12, 1983.2, 1983.28, 1983.36, 1983.44, 1983.52, 1983.6, 1983.68, 1983.76, 1983.84, 1983.92, 1984.0, 1984.08, 1984.16, 1984.24, 1984.32, 1984.4, 1984.48, 1984.56, 1984.64, 1984.72, 1984.8, 1984.88, 1984.96, 1985.04, 1985.12, 1985.2, 1985.28, 1985.36, 1985.44, 1985.52, 1985.6, 1985.68, 1985.76, 1985.84, 1985.92, 1986.0, 1986.08, 1986.16, 1986.24, 1986.32, 1986.4, 1986.48, 1986.56, 1986.64, 1986.72, 1986.8, 1986.88, 1986.96, 1987.04, 1987.12, 1987.2, 1987.28, 1987.36, 1987.44, 1987.52, 1987.6, 1987.68, 1987.76, 1987.84, 1987.92, 1988.0, 1988.08, 1988.16, 1988.24, 1988.32, 1988.4, 1988.48, 1988.56, 1988.64, 1988.72, 1988.8, 1988.88, 1988.96, 1989.04, 1989.12, 1989.2, 1989.28, 1989.36, 1989.44, 1989.52, 1989.6, 1989.68, 1989.76, 1989.84, 1989.92, 1990.0, 1990.08, 1990.16, 1990.24, 1990.32, 1990.4, 1990.48, 1990.56, 1990.64, 1990.72, 1990.8, 1990.88, 1990.96, 1991.04, 1991.12, 1991.2, 1991.28, 1991.36, 1991.44, 1991.52, 1991.6, 1991.68, 1991.76, 1991.84, 1991.92, 1992.0, 1992.08, 1992.16, 1992.24, 1992.32, 1992.4, 1992.48, 1992.56, 1992.64, 1992.72, 1992.8, 1992.88, 1992.96, 1993.04, 1993.12, 1993.2, 1993.28, 1993.36, 1993.44, 1993.52, 1993.6, 1993.68, 1993.76, 1993.84, 1993.92, 1994.0, 1994.08, 1994.16, 1994.24, 1994.32, 1994.4, 1994.48, 1994.56, 1994.64, 1994.72, 1994.8, 1994.88, 1994.96, 1995.04, 1995.12, 1995.2, 1995.28, 1995.36, 1995.44, 1995.52, 1995.6, 1995.68, 1995.76, 1995.84, 1995.92, 1996.0, 1996.08, 1996.16, 1996.24, 1996.32, 1996.4, 1996.48, 1996.56, 1996.64, 1996.72, 1996.8, 1996.88, 1996.96, 1997.04, 1997.12, 1997.2, 1997.28, 1997.36, 1997.44, 1997.52, 1997.6, 1997.68, 1997.76, 1997.84, 1997.92, 1998.0, 1998.08, 1998.16, 1998.24, 1998.32, 1998.4, 1998.48, 1998.56, 1998.64, 1998.72, 1998.8, 1998.88, 1998.96, 1999.04, 1999.12, 1999.2, 1999.28, 1999.36, 1999.44, 1999.52, 1999.6, 1999.68, 1999.76, 1999.84, 1999.92, 2000.0, 2000.08, 2000.16, 2000.24, 2000.32, 2000.4, 2000.48, 2000.56, 2000.64, 2000.72, 2000.8, 2000.88, 2000.96, 2001.04, 2001.12, 2001.2, 2001.28, 2001.36, 2001.44, 2001.52, 2001.6, 2001.68, 2001.76, 2001.84, 2001.92, 2002.0, 2002.08, 2002.16, 2002.24, 2002.32, 2002.4, 2002.48, 2002.56, 2002.64, 2002.72, 2002.8, 2002.88, 2002.96, 2003.04, 2003.12, 2003.2, 2003.28, 2003.36, 2003.44, 2003.52, 2003.6, 2003.68, 2003.76, 2003.84, 2003.92, 2004.0, 2004.08, 2004.16, 2004.24, 2004.32, 2004.4, 2004.48, 2004.56, 2004.64, 2004.72, 2004.8, 2004.88, 2004.96, 2005.04, 2005.12, 2005.2, 2005.28, 2005.36, 2005.44, 2005.52, 2005.6, 2005.68, 2005.76, 2005.84, 2005.92, 2006.0, 2006.08, 2006.16, 2006.24, 2006.32, 2006.4, 2006.48, 2006.56, 2006.64, 2006.72, 2006.8, 2006.88, 2006.96, 2007.04, 2007.12, 2007.2, 2007.28, 2007.36, 2007.44, 2007.52, 2007.6, 2007.68, 2007.76, 2007.84, 2007.92, 2008.0, 2008.08, 2008.16, 2008.24, 2008.32, 2008.4, 2008.48, 2008.56, 2008.64, 2008.72, 2008.8, 2008.88, 2008.96, 2009.04, 2009.12, 2009.2, 2009.28, 2009.36, 2009.44, 2009.52, 2009.6, 2009.68, 2009.76, 2009.84, 2009.92, 2010.0, 2010.08, 2010.16, 2010.24, 2010.32, 2010.4, 2010.48, 2010.56, 2010.64, 2010.72, 2010.8, 2010.88, 2010.96, 2011.04, 2011.12, 2011.2, 2011.28, 2011.36, 2011.44, 2011.52, 2011.6, 2011.68, 2011.76, 2011.84, 2011.92, 2012.0, 2012.08, 2012.16, 2012.24, 2012.32, 2012.4, 2012.48, 2012.56, 2012.64, 2012.72, 2012.8, 2012.88, 2012.96, 2013.04, 2013.12, 2013.2, 2013.28, 2013.36, 2013.44, 2013.52, 2013.6, 2013.68, 2013.76, 2013.84, 2013.92, 2014.0, 2014.08, 2014.16, 2014.24, 2014.32, 2014.4, 2014.48, 2014.56, 2014.64, 2014.72, 2014.8, 2014.88, 2014.96, 2015.04, 2015.12, 2015.2, 2015.28, 2015.36, 2015.44, 2015.52, 2015.6, 2015.68, 2015.76, 2015.84, 2015.92, 2016.0], \"type\": \"scatter\", \"name\": \"Growth of $1\"}], {\"yaxis\": {\"type\": \"log\", \"autorange\": true}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = R[STARTMONTH:]\n",
    "perf = 100 * np.cumprod(1 + results / 100)\n",
    "mychart(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walkforward_xval (X, Y, model, coef_dict=None, minmaxscale=False):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    Xscale = X.copy()\n",
    "    Yscale = Y.copy()\n",
    "\n",
    "    if minmaxscale:\n",
    "        # minmaxscale each row\n",
    "        for i in range(Xscale.shape[0]):\n",
    "            Xscale[i] = Xscale[i] - np.min(Xscale[i])\n",
    "            Xscale[i] = Xscale[i]/np.max(Xscale[i])\n",
    "            \n",
    "        for i in range(Yscale.shape[0]):\n",
    "            Yscale[i] = Yscale[i] - np.min(Yscale[i])\n",
    "            Yscale[i] = Yscale[i]/np.max(Yscale[i])\n",
    "            \n",
    "    # generate k-folds\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    kf.get_n_splits(Xscale)\n",
    "    last_indexes = []\n",
    "    for train_index, test_index in kf.split(Xscale):\n",
    "        # use test_index as last index to train\n",
    "        last_index = test_index[-1] + 1\n",
    "        last_indexes.append(last_index)\n",
    "    print(\"%s Generate splits %s\" % (time.strftime(\"%H:%M:%S\"), str([i for i in last_indexes])))\n",
    "\n",
    "    print(\"%s Starting training\" % (time.strftime(\"%H:%M:%S\")))\n",
    "    \n",
    "    avg_bests = []\n",
    "    for i in range(1, n_splits-1):\n",
    "\n",
    "        models = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        count = 0        \n",
    "        # skip kfold 0 so you start with train 2x size of eval set\n",
    "        last_train_index = last_indexes[i]\n",
    "        last_xval_index = last_indexes[i+1]\n",
    "\n",
    "        # set up train, xval\n",
    "        # train from beginning to last_train_index        \n",
    "        print(\"Training indexes 0 to %d\" % (last_train_index-1))\n",
    "        X_fit = Xscale[:last_train_index]\n",
    "        Y_fit = Yscale[:last_train_index]\n",
    "        # xval from last_train_index to last_xval_index\n",
    "        print(\"Cross-validating indexes %d to %d\" % (last_train_index, last_xval_index -1 ))\n",
    "        X_xval = Xscale[last_train_index:last_xval_index]\n",
    "        Y_xval = Yscale[last_train_index:last_xval_index]\n",
    "\n",
    "        if coef_dict is None:\n",
    "            print(\"Performing LASSO subset selection on training set\")\n",
    "            coef_dict = subset_selection(X_fit, Y_fit, LassoLarsIC(criterion='aic'), verbose=False)\n",
    "        \n",
    "        mse_list = []\n",
    "        \n",
    "        for response in responses:\n",
    "            predcols = [predictor_reverse_dict[indstr] for indstr in coef_dict[response]]\n",
    "            if len(predcols) == 0:\n",
    "                continue\n",
    "            responsecol = response_reverse_dict[response]\n",
    "            \n",
    "            fit = model.fit(X_fit[:,predcols], Y_fit[:,responsecol])\n",
    "            # evaluate ... run prediction, calc MSE by industry, and average\n",
    "            y_xval_pred = fit.predict(X_xval[:,predcols])\n",
    "            mse_list.append(mean_squared_error(Y_xval[:,i], y_xval_pred))\n",
    "            sys.stdout.write('.')\n",
    "            count += 1\n",
    "            if count % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training\" % (time.strftime(\"%H:%M:%S\")))\n",
    "            sys.stdout.flush()             \n",
    "        # mean mse over industry ys for this fold\n",
    "        xval_score = np.mean(np.array(mse_list))            \n",
    "\n",
    "        # choose model with lowest xval loss\n",
    "        print (\"\\n%s Xval MSE %f\" % (time.strftime(\"%H:%M:%S\"), xval_score))\n",
    "        avg_bests.append(xval_score)\n",
    "    \n",
    "    print (\"Last Xval loss %f\" % (xval_score))\n",
    "    # mean over folds\n",
    "    avg_loss = np.mean(np.array(avg_bests))\n",
    "    print (\"Avg Xval loss %f\" % avg_loss)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    return (avg_loss, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:04:46 Generate splits [140, 280, 419, 558, 697]\n",
      "19:04:46 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:04:46 Xval MSE 0.083287\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:04:46 Xval MSE 0.115757\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:04:46 Xval MSE 0.081577\n",
      "Last Xval loss 0.081577\n",
      "Avg Xval loss 0.093541\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09354052111484512,\n",
       " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# walk forward with LinearRegression to get a baseline MSE\n",
    "model = LinearRegression()\n",
    "walkforward_xval (X, Y, model, coef_dict=coef_dict, minmaxscale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:04:46 Generate splits [140, 280, 419, 558, 697]\n",
      "19:04:46 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:04:47 Xval MSE 0.077867\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:04:48 Xval MSE 0.110198\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:04:48 Xval MSE 0.079193\n",
      "Last Xval loss 0.079193\n",
      "Avg Xval loss 0.089086\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08908620755601267,\n",
       " MLPRegressor(activation='tanh', alpha=1.0, batch_size='auto', beta_1=0.9,\n",
       "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_sizes=(2, 2, 2), learning_rate='constant',\n",
       "        learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "        nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "        shuffle=True, solver='lbfgs', tol=1e-10, validation_fraction=0.1,\n",
       "        verbose=False, warm_start=False))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(2,2,2),\n",
    "                     alpha=1.0,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "walkforward_xval (X, Y, model, coef_dict=coef_dict, minmaxscale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:04:48 Running 60 experiments\n",
      "19:04:48 Running experiment 1 of 60\n",
      "19:04:48 n_hidden_layers = 1, hidden_layer_size = 1, reg_penalty = 0.000000\n",
      "19:04:49 Generate splits [140, 280, 419, 558, 697]\n",
      "19:04:49 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:04:49 Xval MSE 0.172606\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:04:50 Xval MSE 0.818340\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:04:51 Xval MSE 0.083405\n",
      "Last Xval loss 0.083405\n",
      "Avg Xval loss 0.358117\n",
      "--------------------------------------------------------------------------------\n",
      "19:04:51 Running experiment 2 of 60\n",
      "19:04:51 n_hidden_layers = 1, hidden_layer_size = 1, reg_penalty = 0.001000\n",
      "19:04:51 Generate splits [140, 280, 419, 558, 697]\n",
      "19:04:51 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:04:52 Xval MSE 0.092908\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:04:54 Xval MSE 0.177421\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:04:55 Xval MSE 0.086076\n",
      "Last Xval loss 0.086076\n",
      "Avg Xval loss 0.118802\n",
      "--------------------------------------------------------------------------------\n",
      "19:04:55 Running experiment 3 of 60\n",
      "19:04:55 n_hidden_layers = 1, hidden_layer_size = 1, reg_penalty = 0.010000\n",
      "19:04:55 Generate splits [140, 280, 419, 558, 697]\n",
      "19:04:55 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:04:56 Xval MSE 0.085275\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:04:57 Xval MSE 0.122645\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:04:58 Xval MSE 0.083651\n",
      "Last Xval loss 0.083651\n",
      "Avg Xval loss 0.097190\n",
      "--------------------------------------------------------------------------------\n",
      "19:04:58 Running experiment 4 of 60\n",
      "19:04:58 n_hidden_layers = 1, hidden_layer_size = 1, reg_penalty = 0.100000\n",
      "19:04:58 Generate splits [140, 280, 419, 558, 697]\n",
      "19:04:58 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:04:59 Xval MSE 0.083190\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:04:59 Xval MSE 0.115918\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:00 Xval MSE 0.081763\n",
      "Last Xval loss 0.081763\n",
      "Avg Xval loss 0.093624\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:00 Running experiment 5 of 60\n",
      "19:05:00 n_hidden_layers = 1, hidden_layer_size = 1, reg_penalty = 1.000000\n",
      "19:05:00 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:00 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:00 Xval MSE 0.080567\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:05:01 Xval MSE 0.113663\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:01 Xval MSE 0.080259\n",
      "Last Xval loss 0.080259\n",
      "Avg Xval loss 0.091497\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:01 Running experiment 6 of 60\n",
      "19:05:01 n_hidden_layers = 1, hidden_layer_size = 2, reg_penalty = 0.000000\n",
      "19:05:01 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:01 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:04 Xval MSE 6.688536\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:05:07 Xval MSE 12.703930\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:10 Xval MSE 78.215648\n",
      "Last Xval loss 78.215648\n",
      "Avg Xval loss 32.536038\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:10 Running experiment 7 of 60\n",
      "19:05:10 n_hidden_layers = 1, hidden_layer_size = 2, reg_penalty = 0.001000\n",
      "19:05:10 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:10 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:15 Xval MSE 0.192088\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:05:19 Xval MSE 0.170303\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:23 Xval MSE 0.186293\n",
      "Last Xval loss 0.186293\n",
      "Avg Xval loss 0.182895\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:23 Running experiment 8 of 60\n",
      "19:05:23 n_hidden_layers = 1, hidden_layer_size = 2, reg_penalty = 0.010000\n",
      "19:05:23 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:23 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:26 Xval MSE 0.098205\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:05:30 Xval MSE 0.126830\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:34 Xval MSE 0.094526\n",
      "Last Xval loss 0.094526\n",
      "Avg Xval loss 0.106520\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:34 Running experiment 9 of 60\n",
      "19:05:34 n_hidden_layers = 1, hidden_layer_size = 2, reg_penalty = 0.100000\n",
      "19:05:34 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:34 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:36 Xval MSE 0.088188\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:05:38 Xval MSE 0.118255\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:40 Xval MSE 0.083790\n",
      "Last Xval loss 0.083790\n",
      "Avg Xval loss 0.096745\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:40 Running experiment 10 of 60\n",
      "19:05:40 n_hidden_layers = 1, hidden_layer_size = 2, reg_penalty = 1.000000\n",
      "19:05:40 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:40 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:41 Xval MSE 0.080607\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:05:42 Xval MSE 0.113964\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:05:42 Xval MSE 0.080641\n",
      "Last Xval loss 0.080641\n",
      "Avg Xval loss 0.091737\n",
      "--------------------------------------------------------------------------------\n",
      "19:05:42 Running experiment 11 of 60\n",
      "19:05:42 n_hidden_layers = 1, hidden_layer_size = 4, reg_penalty = 0.000000\n",
      "19:05:43 Generate splits [140, 280, 419, 558, 697]\n",
      "19:05:43 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:05:53 Xval MSE 20363.247577\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:06:07 Xval MSE 67.672943\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:06:22 Xval MSE 7.492740\n",
      "Last Xval loss 7.492740\n",
      "Avg Xval loss 6812.804420\n",
      "--------------------------------------------------------------------------------\n",
      "19:06:22 Running experiment 12 of 60\n",
      "19:06:22 n_hidden_layers = 1, hidden_layer_size = 4, reg_penalty = 0.001000\n",
      "19:06:22 Generate splits [140, 280, 419, 558, 697]\n",
      "19:06:22 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:06:32 Xval MSE 0.199507\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:06:44 Xval MSE 0.230204\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:06:57 Xval MSE 0.122354\n",
      "Last Xval loss 0.122354\n",
      "Avg Xval loss 0.184022\n",
      "--------------------------------------------------------------------------------\n",
      "19:06:57 Running experiment 13 of 60\n",
      "19:06:57 n_hidden_layers = 1, hidden_layer_size = 4, reg_penalty = 0.010000\n",
      "19:06:57 Generate splits [140, 280, 419, 558, 697]\n",
      "19:06:57 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "19:07:05 Xval MSE 0.142937\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:07:14 Xval MSE 0.164141\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:07:24 Xval MSE 0.104014\n",
      "Last Xval loss 0.104014\n",
      "Avg Xval loss 0.137031\n",
      "--------------------------------------------------------------------------------\n",
      "19:07:24 Running experiment 14 of 60\n",
      "19:07:24 n_hidden_layers = 1, hidden_layer_size = 4, reg_penalty = 0.100000\n",
      "19:07:24 Generate splits [140, 280, 419, 558, 697]\n",
      "19:07:24 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:07:28 Xval MSE 0.099754\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:07:32 Xval MSE 0.130136\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:07:38 Xval MSE 0.089504\n",
      "Last Xval loss 0.089504\n",
      "Avg Xval loss 0.106465\n",
      "--------------------------------------------------------------------------------\n",
      "19:07:38 Running experiment 15 of 60\n",
      "19:07:38 n_hidden_layers = 1, hidden_layer_size = 4, reg_penalty = 1.000000\n",
      "19:07:38 Generate splits [140, 280, 419, 558, 697]\n",
      "19:07:38 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:07:39 Xval MSE 0.080619\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:07:40 Xval MSE 0.114328\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:07:41 Xval MSE 0.081071\n",
      "Last Xval loss 0.081071\n",
      "Avg Xval loss 0.092006\n",
      "--------------------------------------------------------------------------------\n",
      "19:07:41 Running experiment 16 of 60\n",
      "19:07:41 n_hidden_layers = 1, hidden_layer_size = 8, reg_penalty = 0.000000\n",
      "19:07:41 Generate splits [140, 280, 419, 558, 697]\n",
      "19:07:41 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:08:23 Xval MSE 1.231571\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:09:14 Xval MSE 0.832147\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:10:10 Xval MSE 0.936856\n",
      "Last Xval loss 0.936856\n",
      "Avg Xval loss 1.000191\n",
      "--------------------------------------------------------------------------------\n",
      "19:10:10 Running experiment 17 of 60\n",
      "19:10:10 n_hidden_layers = 1, hidden_layer_size = 8, reg_penalty = 0.001000\n",
      "19:10:10 Generate splits [140, 280, 419, 558, 697]\n",
      "19:10:10 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:10:35 Xval MSE 0.310925\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:11:06 Xval MSE 0.276138\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:11:42 Xval MSE 0.205239\n",
      "Last Xval loss 0.205239\n",
      "Avg Xval loss 0.264101\n",
      "--------------------------------------------------------------------------------\n",
      "19:11:42 Running experiment 18 of 60\n",
      "19:11:42 n_hidden_layers = 1, hidden_layer_size = 8, reg_penalty = 0.010000\n",
      "19:11:42 Generate splits [140, 280, 419, 558, 697]\n",
      "19:11:42 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:12:04 Xval MSE 0.186834\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:12:29 Xval MSE 0.188768\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:12:58 Xval MSE 0.131886\n",
      "Last Xval loss 0.131886\n",
      "Avg Xval loss 0.169163\n",
      "--------------------------------------------------------------------------------\n",
      "19:12:58 Running experiment 19 of 60\n",
      "19:12:58 n_hidden_layers = 1, hidden_layer_size = 8, reg_penalty = 0.100000\n",
      "19:12:58 Generate splits [140, 280, 419, 558, 697]\n",
      "19:12:58 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:13:07 Xval MSE 0.113540\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:13:22 Xval MSE 0.139644\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:13:36 Xval MSE 0.104458\n",
      "Last Xval loss 0.104458\n",
      "Avg Xval loss 0.119214\n",
      "--------------------------------------------------------------------------------\n",
      "19:13:36 Running experiment 20 of 60\n",
      "19:13:36 n_hidden_layers = 1, hidden_layer_size = 8, reg_penalty = 1.000000\n",
      "19:13:36 Generate splits [140, 280, 419, 558, 697]\n",
      "19:13:36 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:13:38 Xval MSE 0.080620\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:13:39 Xval MSE 0.114325\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:13:41 Xval MSE 0.081149\n",
      "Last Xval loss 0.081149\n",
      "Avg Xval loss 0.092032\n",
      "--------------------------------------------------------------------------------\n",
      "19:13:41 Running experiment 21 of 60\n",
      "19:13:41 n_hidden_layers = 2, hidden_layer_size = 1, reg_penalty = 0.000000\n",
      "19:13:41 Generate splits [140, 280, 419, 558, 697]\n",
      "19:13:41 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:13:42 Xval MSE 0.392379\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:13:43 Xval MSE 0.129274\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:13:45 Xval MSE 0.082744\n",
      "Last Xval loss 0.082744\n",
      "Avg Xval loss 0.201465\n",
      "--------------------------------------------------------------------------------\n",
      "19:13:45 Running experiment 22 of 60\n",
      "19:13:45 n_hidden_layers = 2, hidden_layer_size = 1, reg_penalty = 0.001000\n",
      "19:13:45 Generate splits [140, 280, 419, 558, 697]\n",
      "19:13:45 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:13:47 Xval MSE 0.089156\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:13:49 Xval MSE 0.117283\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:13:52 Xval MSE 0.082767\n",
      "Last Xval loss 0.082767\n",
      "Avg Xval loss 0.096402\n",
      "--------------------------------------------------------------------------------\n",
      "19:13:52 Running experiment 23 of 60\n",
      "19:13:52 n_hidden_layers = 2, hidden_layer_size = 1, reg_penalty = 0.010000\n",
      "19:13:52 Generate splits [140, 280, 419, 558, 697]\n",
      "19:13:52 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:13:53 Xval MSE 0.087844\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:13:55 Xval MSE 0.116621\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:13:57 Xval MSE 0.084119\n",
      "Last Xval loss 0.084119\n",
      "Avg Xval loss 0.096195\n",
      "--------------------------------------------------------------------------------\n",
      "19:13:57 Running experiment 24 of 60\n",
      "19:13:57 n_hidden_layers = 2, hidden_layer_size = 1, reg_penalty = 0.100000\n",
      "19:13:57 Generate splits [140, 280, 419, 558, 697]\n",
      "19:13:57 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:13:58 Xval MSE 0.083034\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:13:59 Xval MSE 0.118027\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:13:59 Xval MSE 0.081643\n",
      "Last Xval loss 0.081643\n",
      "Avg Xval loss 0.094235\n",
      "--------------------------------------------------------------------------------\n",
      "19:13:59 Running experiment 25 of 60\n",
      "19:13:59 n_hidden_layers = 2, hidden_layer_size = 1, reg_penalty = 1.000000\n",
      "19:13:59 Generate splits [140, 280, 419, 558, 697]\n",
      "19:13:59 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:14:00 Xval MSE 0.077933\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "19:14:00 Xval MSE 0.111734\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:14:01 Xval MSE 0.079560\n",
      "Last Xval loss 0.079560\n",
      "Avg Xval loss 0.089742\n",
      "--------------------------------------------------------------------------------\n",
      "19:14:01 Running experiment 26 of 60\n",
      "19:14:01 n_hidden_layers = 2, hidden_layer_size = 2, reg_penalty = 0.000000\n",
      "19:14:01 Generate splits [140, 280, 419, 558, 697]\n",
      "19:14:01 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:14:12 Xval MSE 0.397108\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:14:24 Xval MSE 0.234972\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:14:33 Xval MSE 0.088819\n",
      "Last Xval loss 0.088819\n",
      "Avg Xval loss 0.240300\n",
      "--------------------------------------------------------------------------------\n",
      "19:14:33 Running experiment 27 of 60\n",
      "19:14:33 n_hidden_layers = 2, hidden_layer_size = 2, reg_penalty = 0.001000\n",
      "19:14:33 Generate splits [140, 280, 419, 558, 697]\n",
      "19:14:33 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:14:41 Xval MSE 0.139988\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:14:51 Xval MSE 0.162966\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:15:01 Xval MSE 0.092006\n",
      "Last Xval loss 0.092006\n",
      "Avg Xval loss 0.131653\n",
      "--------------------------------------------------------------------------------\n",
      "19:15:01 Running experiment 28 of 60\n",
      "19:15:01 n_hidden_layers = 2, hidden_layer_size = 2, reg_penalty = 0.010000\n",
      "19:15:01 Generate splits [140, 280, 419, 558, 697]\n",
      "19:15:01 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:15:07 Xval MSE 0.104945\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:15:14 Xval MSE 0.129256\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:15:21 Xval MSE 0.099557\n",
      "Last Xval loss 0.099557\n",
      "Avg Xval loss 0.111253\n",
      "--------------------------------------------------------------------------------\n",
      "19:15:21 Running experiment 29 of 60\n",
      "19:15:21 n_hidden_layers = 2, hidden_layer_size = 2, reg_penalty = 0.100000\n",
      "19:15:21 Generate splits [140, 280, 419, 558, 697]\n",
      "19:15:21 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:15:23 Xval MSE 0.090852\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:15:26 Xval MSE 0.120273\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:15:30 Xval MSE 0.086347\n",
      "Last Xval loss 0.086347\n",
      "Avg Xval loss 0.099157\n",
      "--------------------------------------------------------------------------------\n",
      "19:15:30 Running experiment 30 of 60\n",
      "19:15:30 n_hidden_layers = 2, hidden_layer_size = 2, reg_penalty = 1.000000\n",
      "19:15:30 Generate splits [140, 280, 419, 558, 697]\n",
      "19:15:30 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:15:31 Xval MSE 0.079046\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:15:32 Xval MSE 0.112179\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:15:32 Xval MSE 0.079808\n",
      "Last Xval loss 0.079808\n",
      "Avg Xval loss 0.090344\n",
      "--------------------------------------------------------------------------------\n",
      "19:15:32 Running experiment 31 of 60\n",
      "19:15:32 n_hidden_layers = 2, hidden_layer_size = 4, reg_penalty = 0.000000\n",
      "19:15:32 Generate splits [140, 280, 419, 558, 697]\n",
      "19:15:32 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:16:04 Xval MSE 0.700485\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:16:42 Xval MSE 44.697622\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:17:27 Xval MSE 0.174180\n",
      "Last Xval loss 0.174180\n",
      "Avg Xval loss 15.190762\n",
      "--------------------------------------------------------------------------------\n",
      "19:17:27 Running experiment 32 of 60\n",
      "19:17:27 n_hidden_layers = 2, hidden_layer_size = 4, reg_penalty = 0.001000\n",
      "19:17:27 Generate splits [140, 280, 419, 558, 697]\n",
      "19:17:27 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:17:54 Xval MSE 0.185441\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:18:25 Xval MSE 0.182810\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:18:57 Xval MSE 0.128121\n",
      "Last Xval loss 0.128121\n",
      "Avg Xval loss 0.165457\n",
      "--------------------------------------------------------------------------------\n",
      "19:18:57 Running experiment 33 of 60\n",
      "19:18:57 n_hidden_layers = 2, hidden_layer_size = 4, reg_penalty = 0.010000\n",
      "19:18:57 Generate splits [140, 280, 419, 558, 697]\n",
      "19:18:57 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:19:19 Xval MSE 0.161094\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:19:44 Xval MSE 0.154978\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:20:21 Xval MSE 0.107552\n",
      "Last Xval loss 0.107552\n",
      "Avg Xval loss 0.141208\n",
      "--------------------------------------------------------------------------------\n",
      "19:20:21 Running experiment 34 of 60\n",
      "19:20:21 n_hidden_layers = 2, hidden_layer_size = 4, reg_penalty = 0.100000\n",
      "19:20:21 Generate splits [140, 280, 419, 558, 697]\n",
      "19:20:21 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:20:31 Xval MSE 0.108753\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:20:44 Xval MSE 0.135457\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:20:58 Xval MSE 0.093532\n",
      "Last Xval loss 0.093532\n",
      "Avg Xval loss 0.112581\n",
      "--------------------------------------------------------------------------------\n",
      "19:20:58 Running experiment 35 of 60\n",
      "19:20:58 n_hidden_layers = 2, hidden_layer_size = 4, reg_penalty = 1.000000\n",
      "19:20:58 Generate splits [140, 280, 419, 558, 697]\n",
      "19:20:58 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:20:59 Xval MSE 0.079057\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:21:01 Xval MSE 0.113538\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:21:03 Xval MSE 0.080688\n",
      "Last Xval loss 0.080688\n",
      "Avg Xval loss 0.091094\n",
      "--------------------------------------------------------------------------------\n",
      "19:21:03 Running experiment 36 of 60\n",
      "19:21:03 n_hidden_layers = 2, hidden_layer_size = 8, reg_penalty = 0.000000\n",
      "19:21:03 Generate splits [140, 280, 419, 558, 697]\n",
      "19:21:03 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:22:13 Xval MSE 0.708105\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:23:56 Xval MSE 0.482935\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:25:56 Xval MSE 0.236018\n",
      "Last Xval loss 0.236018\n",
      "Avg Xval loss 0.475686\n",
      "--------------------------------------------------------------------------------\n",
      "19:25:56 Running experiment 37 of 60\n",
      "19:25:56 n_hidden_layers = 2, hidden_layer_size = 8, reg_penalty = 0.001000\n",
      "19:25:56 Generate splits [140, 280, 419, 558, 697]\n",
      "19:25:56 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:26:58 Xval MSE 0.378871\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:28:18 Xval MSE 0.310082\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "19:29:44 Xval MSE 0.246529\n",
      "Last Xval loss 0.246529\n",
      "Avg Xval loss 0.311827\n",
      "--------------------------------------------------------------------------------\n",
      "19:29:44 Running experiment 38 of 60\n",
      "19:29:44 n_hidden_layers = 2, hidden_layer_size = 8, reg_penalty = 0.010000\n",
      "19:29:44 Generate splits [140, 280, 419, 558, 697]\n",
      "19:29:44 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:30:34 Xval MSE 0.284638\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:31:36 Xval MSE 0.268429\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:32:50 Xval MSE 0.177225\n",
      "Last Xval loss 0.177225\n",
      "Avg Xval loss 0.243431\n",
      "--------------------------------------------------------------------------------\n",
      "19:32:50 Running experiment 39 of 60\n",
      "19:32:50 n_hidden_layers = 2, hidden_layer_size = 8, reg_penalty = 0.100000\n",
      "19:32:50 Generate splits [140, 280, 419, 558, 697]\n",
      "19:32:50 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:33:20 Xval MSE 0.145553\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:34:04 Xval MSE 0.167772\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:11 Xval MSE 0.121832\n",
      "Last Xval loss 0.121832\n",
      "Avg Xval loss 0.145053\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:11 Running experiment 40 of 60\n",
      "19:35:11 n_hidden_layers = 2, hidden_layer_size = 8, reg_penalty = 1.000000\n",
      "19:35:11 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:11 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:12 Xval MSE 0.079152\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:35:15 Xval MSE 0.114873\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:18 Xval MSE 0.082569\n",
      "Last Xval loss 0.082569\n",
      "Avg Xval loss 0.092198\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:18 Running experiment 41 of 60\n",
      "19:35:18 n_hidden_layers = 3, hidden_layer_size = 1, reg_penalty = 0.000000\n",
      "19:35:18 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:18 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:20 Xval MSE 0.150592\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:35:21 Xval MSE 0.114825\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:22 Xval MSE 0.082909\n",
      "Last Xval loss 0.082909\n",
      "Avg Xval loss 0.116109\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:22 Running experiment 42 of 60\n",
      "19:35:22 n_hidden_layers = 3, hidden_layer_size = 1, reg_penalty = 0.001000\n",
      "19:35:22 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:22 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:26 Xval MSE 0.086921\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:35:30 Xval MSE 0.118471\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:33 Xval MSE 0.082881\n",
      "Last Xval loss 0.082881\n",
      "Avg Xval loss 0.096091\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:33 Running experiment 43 of 60\n",
      "19:35:33 n_hidden_layers = 3, hidden_layer_size = 1, reg_penalty = 0.010000\n",
      "19:35:33 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:33 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:35 Xval MSE 0.087999\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:35:37 Xval MSE 0.114853\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:40 Xval MSE 0.083617\n",
      "Last Xval loss 0.083617\n",
      "Avg Xval loss 0.095489\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:40 Running experiment 44 of 60\n",
      "19:35:40 n_hidden_layers = 3, hidden_layer_size = 1, reg_penalty = 0.100000\n",
      "19:35:40 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:40 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:40 Xval MSE 0.080160\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:35:41 Xval MSE 0.112099\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:42 Xval MSE 0.080607\n",
      "Last Xval loss 0.080607\n",
      "Avg Xval loss 0.090955\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:42 Running experiment 45 of 60\n",
      "19:35:42 n_hidden_layers = 3, hidden_layer_size = 1, reg_penalty = 1.000000\n",
      "19:35:42 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:42 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:42 Xval MSE 0.077370\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:35:43 Xval MSE 0.110179\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:35:43 Xval MSE 0.079193\n",
      "Last Xval loss 0.079193\n",
      "Avg Xval loss 0.088914\n",
      "--------------------------------------------------------------------------------\n",
      "19:35:43 Running experiment 46 of 60\n",
      "19:35:43 n_hidden_layers = 3, hidden_layer_size = 2, reg_penalty = 0.000000\n",
      "19:35:43 Generate splits [140, 280, 419, 558, 697]\n",
      "19:35:43 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:35:54 Xval MSE 0.285284\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:36:12 Xval MSE 0.500240\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:36:31 Xval MSE 0.224358\n",
      "Last Xval loss 0.224358\n",
      "Avg Xval loss 0.336627\n",
      "--------------------------------------------------------------------------------\n",
      "19:36:31 Running experiment 47 of 60\n",
      "19:36:31 n_hidden_layers = 3, hidden_layer_size = 2, reg_penalty = 0.001000\n",
      "19:36:31 Generate splits [140, 280, 419, 558, 697]\n",
      "19:36:31 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:36:47 Xval MSE 0.138172\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:37:05 Xval MSE 0.131475\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:37:25 Xval MSE 0.090934\n",
      "Last Xval loss 0.090934\n",
      "Avg Xval loss 0.120194\n",
      "--------------------------------------------------------------------------------\n",
      "19:37:25 Running experiment 48 of 60\n",
      "19:37:25 n_hidden_layers = 3, hidden_layer_size = 2, reg_penalty = 0.010000\n",
      "19:37:25 Generate splits [140, 280, 419, 558, 697]\n",
      "19:37:25 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:37:36 Xval MSE 0.104146\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:37:52 Xval MSE 0.142527\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:38:09 Xval MSE 0.093598\n",
      "Last Xval loss 0.093598\n",
      "Avg Xval loss 0.113424\n",
      "--------------------------------------------------------------------------------\n",
      "19:38:09 Running experiment 49 of 60\n",
      "19:38:09 n_hidden_layers = 3, hidden_layer_size = 2, reg_penalty = 0.100000\n",
      "19:38:09 Generate splits [140, 280, 419, 558, 697]\n",
      "19:38:09 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:38:12 Xval MSE 0.088863\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:38:17 Xval MSE 0.120505\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:38:22 Xval MSE 0.086347\n",
      "Last Xval loss 0.086347\n",
      "Avg Xval loss 0.098572\n",
      "--------------------------------------------------------------------------------\n",
      "19:38:22 Running experiment 50 of 60\n",
      "19:38:22 n_hidden_layers = 3, hidden_layer_size = 2, reg_penalty = 1.000000\n",
      "19:38:22 Generate splits [140, 280, 419, 558, 697]\n",
      "19:38:22 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "19:38:23 Xval MSE 0.077550\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:38:23 Xval MSE 0.110285\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:38:24 Xval MSE 0.079459\n",
      "Last Xval loss 0.079459\n",
      "Avg Xval loss 0.089098\n",
      "--------------------------------------------------------------------------------\n",
      "19:38:24 Running experiment 51 of 60\n",
      "19:38:24 n_hidden_layers = 3, hidden_layer_size = 4, reg_penalty = 0.000000\n",
      "19:38:24 Generate splits [140, 280, 419, 558, 697]\n",
      "19:38:24 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:39:24 Xval MSE 0.276038\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:40:36 Xval MSE 0.267102\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:41:50 Xval MSE 0.196261\n",
      "Last Xval loss 0.196261\n",
      "Avg Xval loss 0.246467\n",
      "--------------------------------------------------------------------------------\n",
      "19:41:50 Running experiment 52 of 60\n",
      "19:41:50 n_hidden_layers = 3, hidden_layer_size = 4, reg_penalty = 0.001000\n",
      "19:41:50 Generate splits [140, 280, 419, 558, 697]\n",
      "19:41:50 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:42:52 Xval MSE 0.233835\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:44:00 Xval MSE 0.220763\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:45:14 Xval MSE 0.171627\n",
      "Last Xval loss 0.171627\n",
      "Avg Xval loss 0.208742\n",
      "--------------------------------------------------------------------------------\n",
      "19:45:14 Running experiment 53 of 60\n",
      "19:45:14 n_hidden_layers = 3, hidden_layer_size = 4, reg_penalty = 0.010000\n",
      "19:45:14 Generate splits [140, 280, 419, 558, 697]\n",
      "19:45:14 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:45:51 Xval MSE 0.155403\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:46:41 Xval MSE 0.176179\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:47:35 Xval MSE 0.131306\n",
      "Last Xval loss 0.131306\n",
      "Avg Xval loss 0.154296\n",
      "--------------------------------------------------------------------------------\n",
      "19:47:35 Running experiment 54 of 60\n",
      "19:47:35 n_hidden_layers = 3, hidden_layer_size = 4, reg_penalty = 0.100000\n",
      "19:47:35 Generate splits [140, 280, 419, 558, 697]\n",
      "19:47:35 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:47:50 Xval MSE 0.113667\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:48:14 Xval MSE 0.140251\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:48:35 Xval MSE 0.093752\n",
      "Last Xval loss 0.093752\n",
      "Avg Xval loss 0.115890\n",
      "--------------------------------------------------------------------------------\n",
      "19:48:35 Running experiment 55 of 60\n",
      "19:48:35 n_hidden_layers = 3, hidden_layer_size = 4, reg_penalty = 1.000000\n",
      "19:48:35 Generate splits [140, 280, 419, 558, 697]\n",
      "19:48:35 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:48:35 Xval MSE 0.077927\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:48:36 Xval MSE 0.110685\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:48:37 Xval MSE 0.079464\n",
      "Last Xval loss 0.079464\n",
      "Avg Xval loss 0.089359\n",
      "--------------------------------------------------------------------------------\n",
      "19:48:37 Running experiment 56 of 60\n",
      "19:48:37 n_hidden_layers = 3, hidden_layer_size = 8, reg_penalty = 0.000000\n",
      "19:48:37 Generate splits [140, 280, 419, 558, 697]\n",
      "19:48:37 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:50:16 Xval MSE 0.737371\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:52:34 Xval MSE 0.534889\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "19:55:26 Xval MSE 0.270366\n",
      "Last Xval loss 0.270366\n",
      "Avg Xval loss 0.514209\n",
      "--------------------------------------------------------------------------------\n",
      "19:55:26 Running experiment 57 of 60\n",
      "19:55:26 n_hidden_layers = 3, hidden_layer_size = 8, reg_penalty = 0.001000\n",
      "19:55:26 Generate splits [140, 280, 419, 558, 697]\n",
      "19:55:26 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "19:57:06 Xval MSE 0.510514\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "19:59:16 Xval MSE 0.426412\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "20:01:47 Xval MSE 0.278345\n",
      "Last Xval loss 0.278345\n",
      "Avg Xval loss 0.405090\n",
      "--------------------------------------------------------------------------------\n",
      "20:01:47 Running experiment 58 of 60\n",
      "20:01:47 n_hidden_layers = 3, hidden_layer_size = 8, reg_penalty = 0.010000\n",
      "20:01:47 Generate splits [140, 280, 419, 558, 697]\n",
      "20:01:47 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "20:03:24 Xval MSE 0.295183\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "20:05:33 Xval MSE 0.295983\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "20:07:42 Xval MSE 0.207131\n",
      "Last Xval loss 0.207131\n",
      "Avg Xval loss 0.266099\n",
      "--------------------------------------------------------------------------------\n",
      "20:07:42 Running experiment 59 of 60\n",
      "20:07:42 n_hidden_layers = 3, hidden_layer_size = 8, reg_penalty = 0.100000\n",
      "20:07:42 Generate splits [140, 280, 419, 558, 697]\n",
      "20:07:42 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "20:08:34 Xval MSE 0.169535\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "20:09:47 Xval MSE 0.182110\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "20:11:24 Xval MSE 0.124954\n",
      "Last Xval loss 0.124954\n",
      "Avg Xval loss 0.158866\n",
      "--------------------------------------------------------------------------------\n",
      "20:11:24 Running experiment 60 of 60\n",
      "20:11:24 n_hidden_layers = 3, hidden_layer_size = 8, reg_penalty = 1.000000\n",
      "20:11:24 Generate splits [140, 280, 419, 558, 697]\n",
      "20:11:24 Starting training\n",
      "Training indexes 0 to 279\n",
      "Cross-validating indexes 280 to 418\n",
      "..............................\n",
      "20:11:26 Xval MSE 0.078443\n",
      "Training indexes 0 to 418\n",
      "Cross-validating indexes 419 to 557\n",
      "..............................\n",
      "20:11:27 Xval MSE 0.111086\n",
      "Training indexes 0 to 557\n",
      "Cross-validating indexes 558 to 696\n",
      "..............................\n",
      "20:11:29 Xval MSE 0.079468\n",
      "Last Xval loss 0.079468\n",
      "Avg Xval loss 0.089665\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MODELPREFIX = \"MLP\"\n",
    "\n",
    "n_hiddens = [1, 2, 3]\n",
    "layer_sizes = [1, 2, 4, 8]\n",
    "reg_penalties = [0.0, 0.001, 0.01, 0.1, 1]\n",
    "hyperparameter_combos = list(product(n_hiddens, layer_sizes, reg_penalties))\n",
    "\n",
    "print(\"%s Running %d experiments\" % (time.strftime(\"%H:%M:%S\"), len(hyperparameter_combos)))\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "for counter, param_list in enumerate(hyperparameter_combos):\n",
    "    n_hidden_layers, layer_size, reg_penalty = param_list\n",
    "    print(\"%s Running experiment %d of %d\" % (time.strftime(\"%H:%M:%S\"), counter+1, len(hyperparameter_combos)))\n",
    "    key = (n_hidden_layers, layer_size, reg_penalty)\n",
    "    print(\"%s n_hidden_layers = %d, hidden_layer_size = %d, reg_penalty = %.6f\" % \n",
    "          (time.strftime(\"%H:%M:%S\"), n_hidden_layers, layer_size, reg_penalty))\n",
    "    hls = tuple([layer_size]*n_hidden_layers)\n",
    "    model = MLPRegressor(hidden_layer_sizes=hls,\n",
    "                         alpha=reg_penalty,\n",
    "                         activation='tanh',\n",
    "                         max_iter=10000, \n",
    "                         tol=1e-10,\n",
    "                         solver='lbfgs')\n",
    "    \n",
    "    score, model = walkforward_xval (X, Y, model, coef_dict=coef_dict, minmaxscale=True)\n",
    "\n",
    "    experiments[key] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_hidden_layers</th>\n",
       "      <th>layer_size</th>\n",
       "      <th>reg_penalty</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.088914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.089098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.089359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.089665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.089742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.090344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.090955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.091094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.091497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.091737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.092006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.092032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.092198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.093624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.094235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.095489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.096091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.096195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.096402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.096745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.097190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.098572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.099157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.106465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.106520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.111253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.112581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.113424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.115890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.118802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.119214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.120194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.131653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.137031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.141208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.145053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.154296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.158866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.165457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.169163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.182895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.184022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.201465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.208742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.243431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.246467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.264101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.266099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.311827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.336627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.405090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.475686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.514209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.190762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.536038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6812.804420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_hidden_layers  layer_size  reg_penalty         loss\n",
       "35                3           1        1.000     0.088914\n",
       "36                3           2        1.000     0.089098\n",
       "9                 3           4        1.000     0.089359\n",
       "21                3           8        1.000     0.089665\n",
       "1                 2           1        1.000     0.089742\n",
       "51                2           2        1.000     0.090344\n",
       "0                 3           1        0.100     0.090955\n",
       "7                 2           4        1.000     0.091094\n",
       "44                1           1        1.000     0.091497\n",
       "33                1           2        1.000     0.091737\n",
       "23                1           4        1.000     0.092006\n",
       "42                1           8        1.000     0.092032\n",
       "58                2           8        1.000     0.092198\n",
       "52                1           1        0.100     0.093624\n",
       "15                2           1        0.100     0.094235\n",
       "43                3           1        0.010     0.095489\n",
       "31                3           1        0.001     0.096091\n",
       "56                2           1        0.010     0.096195\n",
       "50                2           1        0.001     0.096402\n",
       "2                 1           2        0.100     0.096745\n",
       "34                1           1        0.010     0.097190\n",
       "53                3           2        0.100     0.098572\n",
       "45                2           2        0.100     0.099157\n",
       "12                1           4        0.100     0.106465\n",
       "14                1           2        0.010     0.106520\n",
       "29                2           2        0.010     0.111253\n",
       "46                2           4        0.100     0.112581\n",
       "10                3           2        0.010     0.113424\n",
       "17                3           4        0.100     0.115890\n",
       "38                3           1        0.000     0.116109\n",
       "55                1           1        0.001     0.118802\n",
       "59                1           8        0.100     0.119214\n",
       "27                3           2        0.001     0.120194\n",
       "48                2           2        0.001     0.131653\n",
       "28                1           4        0.010     0.137031\n",
       "8                 2           4        0.010     0.141208\n",
       "25                2           8        0.100     0.145053\n",
       "37                3           4        0.010     0.154296\n",
       "57                3           8        0.100     0.158866\n",
       "26                2           4        0.001     0.165457\n",
       "4                 1           8        0.010     0.169163\n",
       "6                 1           2        0.001     0.182895\n",
       "20                1           4        0.001     0.184022\n",
       "49                2           1        0.000     0.201465\n",
       "13                3           4        0.001     0.208742\n",
       "54                2           2        0.000     0.240300\n",
       "16                2           8        0.010     0.243431\n",
       "40                3           4        0.000     0.246467\n",
       "5                 1           8        0.001     0.264101\n",
       "24                3           8        0.010     0.266099\n",
       "11                2           8        0.001     0.311827\n",
       "32                3           2        0.000     0.336627\n",
       "47                1           1        0.000     0.358117\n",
       "41                3           8        0.001     0.405090\n",
       "3                 2           8        0.000     0.475686\n",
       "22                3           8        0.000     0.514209\n",
       "39                1           8        0.000     1.000191\n",
       "18                2           4        0.000    15.190762\n",
       "30                1           2        0.000    32.536038\n",
       "19                1           4        0.000  6812.804420"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list and chart experiments\n",
    "flatlist = [list(l[0]) + [l[1]] for l in experiments.items()]\n",
    " \n",
    "lossframe = pd.DataFrame(flatlist, columns=[\"n_hidden_layers\", \"layer_size\", \"reg_penalty\", \"loss\"])\n",
    "lossframe.sort_values(['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_hidden_layers</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>342.437090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.911002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.174708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       loss\n",
       "n_hidden_layers            \n",
       "1                342.437090\n",
       "2                  0.911002\n",
       "3                  0.174708"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can pick lowest loss , but first we look at patterns by hyperparameter\n",
    "pd.DataFrame(lossframe.groupby(['n_hidden_layers'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_size</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.121655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.296304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>455.322653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.289788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  loss\n",
       "layer_size            \n",
       "1             0.121655\n",
       "2             2.296304\n",
       "4           455.322653\n",
       "8             0.289788"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lossframe.groupby(['layer_size'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_penalty</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.000</th>\n",
       "      <td>572.001699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.190440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.144275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.110946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.090641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   loss\n",
       "reg_penalty            \n",
       "0.000        572.001699\n",
       "0.001          0.190440\n",
       "0.010          0.144275\n",
       "0.100          0.110946\n",
       "1.000          0.090641"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lossframe.groupby(['reg_penalty'])['loss'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['1  units', '2  units', '4  units', '8  units'], ['1  layers', '2  layers', '3  layers'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "1  units",
          "2  units",
          "4  units",
          "8  units"
         ],
         "y": [
          "1  layers",
          "2  layers",
          "3  layers"
         ],
         "z": [
          [
           0.15184586470773512,
           6.602786918265053,
           1362.66478865355,
           0.32894001948825136
          ],
          [
           0.11560781841150325,
           0.13454145030223402,
           3.1402203427950193,
           0.25363884479439486
          ],
          [
           0.09751173730196204,
           0.15158289232355643,
           0.16295080591528915,
           0.2867859431672258
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "n_hidden_layers v. layer_size",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "n_hidden_layers"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "layer_size"
        }
       }
      },
      "text/html": [
       "<div id=\"fb9728c9-98d4-4d2d-b412-f8cb7f94cf30\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fb9728c9-98d4-4d2d-b412-f8cb7f94cf30\", [{\"y\": [\"1  layers\", \"2  layers\", \"3  layers\"], \"x\": [\"1  units\", \"2  units\", \"4  units\", \"8  units\"], \"z\": [[0.15184586470773512, 6.602786918265053, 1362.66478865355, 0.32894001948825136], [0.11560781841150325, 0.13454145030223402, 3.1402203427950193, 0.25363884479439486], [0.09751173730196204, 0.15158289232355643, 0.16295080591528915, 0.2867859431672258]], \"type\": \"heatmap\", \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. layer_size\", \"yaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"layer_size\"}, \"height\": 480, \"width\": 640, \"xaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"n_hidden_layers\"}, \"margin\": {\"r\": 30, \"b\": 120, \"t\": 100, \"l\": 150}}, {\"linkText\": \"\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"fb9728c9-98d4-4d2d-b412-f8cb7f94cf30\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fb9728c9-98d4-4d2d-b412-f8cb7f94cf30\", [{\"y\": [\"1  layers\", \"2  layers\", \"3  layers\"], \"x\": [\"1  units\", \"2  units\", \"4  units\", \"8  units\"], \"z\": [[0.15184586470773512, 6.602786918265053, 1362.66478865355, 0.32894001948825136], [0.11560781841150325, 0.13454145030223402, 3.1402203427950193, 0.25363884479439486], [0.09751173730196204, 0.15158289232355643, 0.16295080591528915, 0.2867859431672258]], \"type\": \"heatmap\", \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. layer_size\", \"yaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"layer_size\"}, \"height\": 480, \"width\": 640, \"xaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"n_hidden_layers\"}, \"margin\": {\"r\": 30, \"b\": 120, \"t\": 100, \"l\": 150}}, {\"linkText\": \"\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_matrix(lossframe, x_labels, y_labels, x_suffix=\"\", y_suffix=\"\"):\n",
    "\n",
    "    pivot = lossframe.pivot_table(index=[x_labels], columns=[y_labels], values=['loss'])\n",
    "    # specify labels as strings, to force it to use a discrete axis\n",
    "    if lossframe[x_labels].dtype == np.float64 or lossframe[x_labels].dtype == np.float32:\n",
    "        xaxis = [\"%f %s\" % (i, x_suffix) for i in pivot.columns.levels[1].values]\n",
    "    else:\n",
    "        xaxis = [\"%d %s\" % (i, x_suffix) for i in pivot.columns.levels[1].values]\n",
    "    if lossframe[y_labels].dtype == np.float64 or lossframe[y_labels].dtype == np.float32:\n",
    "        yaxis = [\"%f %s\" % (i, y_suffix) for i in pivot.index.values]\n",
    "    else:\n",
    "        yaxis = [\"%d %s\" % (i, y_suffix) for i in pivot.index.values]\n",
    "        \n",
    "    print(xaxis, yaxis)\n",
    "    \"\"\"plot a heat map of a matrix\"\"\"\n",
    "    chart_width=640\n",
    "    chart_height=480\n",
    "    \n",
    "    layout = Layout(\n",
    "        title=\"%s v. %s\" % (x_labels, y_labels),\n",
    "        height=chart_height,\n",
    "        width=chart_width,     \n",
    "        margin=dict(\n",
    "            l=150,\n",
    "            r=30,\n",
    "            b=120,\n",
    "            t=100,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=x_labels,\n",
    "            tickfont=dict(\n",
    "                family='Arial, sans-serif',\n",
    "                size=10,\n",
    "                color='black'\n",
    "            ),\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=y_labels,\n",
    "            tickfont=dict(\n",
    "                family='Arial, sans-serif',\n",
    "                size=10,\n",
    "                color='black'\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [Heatmap(z=pivot.values,\n",
    "                    x=xaxis,\n",
    "                    y=yaxis,\n",
    "                    colorscale=[[0, 'rgb(0,0,255)', [1, 'rgb(255,0,0)']]],\n",
    "                   )\n",
    "           ]\n",
    "\n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    return iplot(fig, link_text=\"\")\n",
    "\n",
    "plot_matrix(lossframe, \"n_hidden_layers\", \"layer_size\", x_suffix=\" units\", y_suffix=\" layers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['0 p', '0 p', '0 p', '0 p', '1 p'], ['1.000000  layers', '2.000000  layers', '3.000000  layers'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "0 p",
          "0 p",
          "0 p",
          "0 p",
          "1 p"
         ],
         "y": [
          "1.000000  layers",
          "2.000000  layers",
          "3.000000  layers"
         ],
         "z": [
          [
           1711.674691450922,
           0.18745480526940927,
           0.12747594430735198,
           0.10401179146155842,
           0.09181782805385942
          ],
          [
           4.027053274067391,
           0.17633492432829243,
           0.14802140200224836,
           0.11275632920554593,
           0.0908446407754609
          ],
          [
           0.3033528253621503,
           0.20752912176615804,
           0.15732715258620686,
           0.11607091528044186,
           0.08925920839008467
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "n_hidden_layers v. reg_penalty",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "n_hidden_layers"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "reg_penalty"
        }
       }
      },
      "text/html": [
       "<div id=\"48d3e84e-80b0-4966-8bdc-53cef39b4e5a\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"48d3e84e-80b0-4966-8bdc-53cef39b4e5a\", [{\"y\": [\"1.000000  layers\", \"2.000000  layers\", \"3.000000  layers\"], \"x\": [\"0 p\", \"0 p\", \"0 p\", \"0 p\", \"1 p\"], \"z\": [[1711.674691450922, 0.18745480526940927, 0.12747594430735198, 0.10401179146155842, 0.09181782805385942], [4.027053274067391, 0.17633492432829243, 0.14802140200224836, 0.11275632920554593, 0.0908446407754609], [0.3033528253621503, 0.20752912176615804, 0.15732715258620686, 0.11607091528044186, 0.08925920839008467]], \"type\": \"heatmap\", \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. reg_penalty\", \"yaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"reg_penalty\"}, \"height\": 480, \"width\": 640, \"xaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"n_hidden_layers\"}, \"margin\": {\"r\": 30, \"b\": 120, \"t\": 100, \"l\": 150}}, {\"linkText\": \"\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"48d3e84e-80b0-4966-8bdc-53cef39b4e5a\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"48d3e84e-80b0-4966-8bdc-53cef39b4e5a\", [{\"y\": [\"1.000000  layers\", \"2.000000  layers\", \"3.000000  layers\"], \"x\": [\"0 p\", \"0 p\", \"0 p\", \"0 p\", \"1 p\"], \"z\": [[1711.674691450922, 0.18745480526940927, 0.12747594430735198, 0.10401179146155842, 0.09181782805385942], [4.027053274067391, 0.17633492432829243, 0.14802140200224836, 0.11275632920554593, 0.0908446407754609], [0.3033528253621503, 0.20752912176615804, 0.15732715258620686, 0.11607091528044186, 0.08925920839008467]], \"type\": \"heatmap\", \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"n_hidden_layers v. reg_penalty\", \"yaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"reg_penalty\"}, \"height\": 480, \"width\": 640, \"xaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"n_hidden_layers\"}, \"margin\": {\"r\": 30, \"b\": 120, \"t\": 100, \"l\": 150}}, {\"linkText\": \"\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(lossframe, \"n_hidden_layers\", \"reg_penalty\", x_suffix=\"p\", y_suffix=\" layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['1.000000  units', '2.000000  units', '4.000000  units', '8.000000  units'], ['0 p', '0 p', '0 p', '0 p', '1 p'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(0,0,255)",
           [
            1,
            "rgb(255,0,0)"
           ]
          ]
         ],
         "type": "heatmap",
         "x": [
          "1.000000  units",
          "2.000000  units",
          "4.000000  units",
          "8.000000  units"
         ],
         "y": [
          "0 p",
          "0 p",
          "0 p",
          "0 p",
          "1 p"
         ],
         "z": [
          [
           0.22523033799086858,
           11.037654904616987,
           2276.0805496224816,
           0.6633618687124895
          ],
          [
           0.1037647660932917,
           0.14491390337745377,
           0.18607368958017398,
           0.3270061094342269
          ],
          [
           0.0962914945675285,
           0.11039873906344978,
           0.14417822330378885,
           0.2262308749263091
          ],
          [
           0.09293805163126566,
           0.09815791067509898,
           0.11164526180513223,
           0.1410441571518981
          ],
          [
           0.09005105041904625,
           0.0903933104184161,
           0.09081953993021445,
           0.09129833552486316
          ]
         ]
        }
       ],
       "layout": {
        "height": 480,
        "margin": {
         "b": 120,
         "l": 150,
         "r": 30,
         "t": 100
        },
        "title": "reg_penalty v. layer_size",
        "width": 640,
        "xaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "reg_penalty"
        },
        "yaxis": {
         "tickfont": {
          "color": "black",
          "family": "Arial, sans-serif",
          "size": 10
         },
         "title": "layer_size"
        }
       }
      },
      "text/html": [
       "<div id=\"fc6281dc-5481-4b5a-a2ce-65873e4745d7\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fc6281dc-5481-4b5a-a2ce-65873e4745d7\", [{\"y\": [\"0 p\", \"0 p\", \"0 p\", \"0 p\", \"1 p\"], \"x\": [\"1.000000  units\", \"2.000000  units\", \"4.000000  units\", \"8.000000  units\"], \"z\": [[0.22523033799086858, 11.037654904616987, 2276.0805496224816, 0.6633618687124895], [0.1037647660932917, 0.14491390337745377, 0.18607368958017398, 0.3270061094342269], [0.0962914945675285, 0.11039873906344978, 0.14417822330378885, 0.2262308749263091], [0.09293805163126566, 0.09815791067509898, 0.11164526180513223, 0.1410441571518981], [0.09005105041904625, 0.0903933104184161, 0.09081953993021445, 0.09129833552486316]], \"type\": \"heatmap\", \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"reg_penalty v. layer_size\", \"yaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"layer_size\"}, \"height\": 480, \"width\": 640, \"xaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"reg_penalty\"}, \"margin\": {\"r\": 30, \"b\": 120, \"t\": 100, \"l\": 150}}, {\"linkText\": \"\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"fc6281dc-5481-4b5a-a2ce-65873e4745d7\" style=\"height: 480px; width: 640px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fc6281dc-5481-4b5a-a2ce-65873e4745d7\", [{\"y\": [\"0 p\", \"0 p\", \"0 p\", \"0 p\", \"1 p\"], \"x\": [\"1.000000  units\", \"2.000000  units\", \"4.000000  units\", \"8.000000  units\"], \"z\": [[0.22523033799086858, 11.037654904616987, 2276.0805496224816, 0.6633618687124895], [0.1037647660932917, 0.14491390337745377, 0.18607368958017398, 0.3270061094342269], [0.0962914945675285, 0.11039873906344978, 0.14417822330378885, 0.2262308749263091], [0.09293805163126566, 0.09815791067509898, 0.11164526180513223, 0.1410441571518981], [0.09005105041904625, 0.0903933104184161, 0.09081953993021445, 0.09129833552486316]], \"type\": \"heatmap\", \"colorscale\": [[0, \"rgb(0,0,255)\", [1, \"rgb(255,0,0)\"]]]}], {\"title\": \"reg_penalty v. layer_size\", \"yaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"layer_size\"}, \"height\": 480, \"width\": 640, \"xaxis\": {\"tickfont\": {\"color\": \"black\", \"family\": \"Arial, sans-serif\", \"size\": 10}, \"title\": \"reg_penalty\"}, \"margin\": {\"r\": 30, \"b\": 120, \"t\": 100, \"l\": 150}}, {\"linkText\": \"\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(lossframe, \"reg_penalty\", \"layer_size\", x_suffix=\" units\", y_suffix=\"p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:41:59 Starting\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................\n",
      "last prediction not stored\n",
      "MSE across all predictions: 0.0508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>-0.00537598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0721702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>-0.0440373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.385612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>-0.024083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-12-31 00:00:00\n",
       "cagr                   -0.00537598\n",
       "yearly_vol               0.0721702\n",
       "yearly_sharpe           -0.0440373\n",
       "max_drawdown             -0.385612\n",
       "sortino                  -0.024083"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(1,1,1),\n",
    "                     alpha=1.0,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "run_backtest(X, Y, model, startmonth=STARTMONTH, minmaxscale=True)\n",
    "# lower MSE, worse performance\n",
    "# linear is lucky? worse forecast accuracy gives better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:47:02 Starting\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................\n",
      "last prediction not stored\n",
      "MSE across all predictions: 14.1375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>0.0162446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0508186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>0.322345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.368019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>0.154886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-12-31 00:00:00\n",
       "cagr                     0.0162446\n",
       "yearly_vol               0.0508186\n",
       "yearly_sharpe             0.322345\n",
       "max_drawdown             -0.368019\n",
       "sortino                   0.154886"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(1,1,1),\n",
    "                     alpha=1.0,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "run_backtest(X, Y, model, startmonth=STARTMONTH, minmaxscale=False)\n",
    "# runs slower without minmaxscale, optimization takes longer to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:46:27 Starting\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................\n",
      "last prediction not stored\n",
      "MSE across all predictions: 0.0508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>1970-01-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2017-12-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cagr</th>\n",
       "      <td>0.00135136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_vol</th>\n",
       "      <td>0.0531628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearly_sharpe</th>\n",
       "      <td>0.0439699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>-0.242301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sortino</th>\n",
       "      <td>0.0246606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "start          1970-01-31 00:00:00\n",
       "end            2017-12-31 00:00:00\n",
       "cagr                    0.00135136\n",
       "yearly_vol               0.0531628\n",
       "yearly_sharpe            0.0439699\n",
       "max_drawdown             -0.242301\n",
       "sortino                  0.0246606"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "model = MLPRegressor(hidden_layer_sizes=(2,2,2),\n",
    "                     alpha=1.0,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "run_backtest(X, Y, model, startmonth=STARTMONTH, minmaxscale=True)\n",
    "# lower MSE, worse performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:16:53 Starting\n",
      "..............................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-295639605c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      solver='lbfgs')\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun_backtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartmonth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTARTMONTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmaxscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# lower MSE, worse performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# linear is lucky? worse forecast accuracy gives better performance?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-c14cf7544dba>\u001b[0m in \u001b[0;36mrun_backtest\u001b[0;34m(X, Y, model, coef_dict, startmonth, minmaxscale)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                   \u001b[0mYscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmonth_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                   coef_dict)\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonth_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ddf150564949>\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(X, Y, model, coef_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpredcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredictor_reverse_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpredstr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoef_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mresponsecol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_reverse_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresponsecol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[0;32m--> 383\u001b[0;31m                             intercept_grads, layer_units)\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0miprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mpgtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mderivative\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malltrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36malltrue\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"%s Starting\" % (time.strftime(\"%H:%M:%S\")))\n",
    "model = MLPRegressor(hidden_layer_sizes=(2,2,2),\n",
    "                     alpha=1.0,\n",
    "                     activation='tanh',\n",
    "                     max_iter=10000, \n",
    "                     tol=1e-10,\n",
    "                     solver='lbfgs')\n",
    "run_backtest(X, Y, model, startmonth=STARTMONTH, minmaxscale=False)\n",
    "# lower MSE, worse performance\n",
    "# linear is lucky? worse forecast accuracy gives better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
